{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **딥러닝 알고리즘의 발전(2010~)**\n",
    "\n",
    "[![Open in Colab](http://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thekimk/All-About-Deep-Learning/blob/main/Lecture4-2_DeepLearning_AdvancedAI_KK.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위기극복의 계기\n",
    "\n",
    "- **NN의 두번째위기:** (1) `Vanishing Gradients`, (2) `Local Minimum`, (3) Low Learning Time, (4) Curse of Dimensionality\n",
    "\n",
    "> **\"RBM 이외에도, `Supervised Learning 문제에서의 아이디어로` 해법 개발 노력\"**\n",
    "\n",
    "---\n",
    "\n",
    "**1) Vanishing Gradients의 해결:** `활성화 함수 ReLU`\n",
    "\n",
    "<center><img src='Image/Expert/DL_ActivationFunction_Type.png' width='700'></center>\n",
    "\n",
    "- `Sigmoid`는 신경망이 깊어지면 여전히 `비용함수 최적화가 어려운 이슈` 발견\n",
    "- `다양한 미분가능한 비선형 활성화 함수` 필요성이 증가했고 `ReLU`가 Vanishing Gradient의 문제를 해결(Hinton et al. 2010)\n",
    "- 기존 활성화 함수는 양끝의 기울기가 0이 되는 이슈가 있었으나, ReLU는 `기울기가 0으로 감소하는 현상이 없고 일반적으로 학습성능도 향상`\n",
    "\n",
    "<center><img src='Image/Expert/Logistic_ReLU.png' width='400'></center>\n",
    "\n",
    "- 미세변화(미분) 중첩의 문제가 해결되면서 `선학습(Pre Training)도 불필요` (Bordes and Bengio. 2011)\n",
    "- 많은 종류의 비선형적 `활성화 함수가 지금도 개발중`\n",
    "\n",
    "<center><img src='Image/Expert/DL_ActivationFunction_Type_All.png' width='1100'>(https://miro.medium.com/max/814/1*F9-nc6ez5GOJ1mdB3TLlow.png)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) Local Minimum의 해결:** `Global Minimum` $\\approx$ `Local Minimum`\n",
    "\n",
    "> - `고차원 데이터나 Non-convex 형태의 비용함수에서의 최적화`를 하더라도 Local Minimum들은 서로 유사할 것이며 Global Minimum과 큰 차이가 없을 것(Bengio et al. 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 추정 최적화\n",
    "\n",
    "> **\"`비용함수가 가능한 낮은 가중치`를 찾아가는 과정인 `최적화가 중요해짐!`\"**  \n",
    "> - **Prerequisite:** Derivatives, Partial Derivatives, Chain Rule\n",
    "\n",
    "---\n",
    "\n",
    "**1) 추정 프로세스:**\n",
    "\n",
    "<center><img src='Image/Expert/DNN_Process.png' width='700'>(https://www.inf.ufpr.br/todt/IAaplicada/CNN_Presentation.pdf)</center>\n",
    "\n",
    "> **(1) 네트워크 초기화:** `가중치의 초기값`이 필요하며 일반적으로 `무작위로 초기화` 됨   \n",
    ">\n",
    "> - 초기값이 모두 같으면 모든 노드들의 입력값에 동일 가중치가 반영되어 `같은 값이 출력되고` 역전파로 인해 `비용함수 변화량도 모두 동일`해지므로 다른값 필요   \n",
    ">\n",
    "> **(2) 순전파:** 초기화된 가중치들의 출력이 퍼셉트론을 거쳐 Output Layer에 도달\n",
    ">\n",
    "> - $\\hat{Y}_{init} = f(\\sum_{i}^{k} w_i x_i - \\theta)$  \n",
    ">\n",
    "> **(3) 비용함수 평가:**\n",
    ">\n",
    "> - **회귀문제:** $MSE = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$    \n",
    ">\n",
    "> - **분류문제:** $Cross Entropy = \\sum_{i=1}^{k} \\left[ - \\color{red}{\\hat{Y}_{init} log (Pr(\\hat{Y}_{init}))} - \\color{blue}{(1-\\hat{Y}_{init}) log (1-Pr(\\hat{Y}_{init}))} \\right]$    \n",
    ">\n",
    "> **(4) 역전파:** `각 가중치 별 현재 비용함수에 미치는 영향` 계산 \n",
    ">\n",
    "> - $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    ">\n",
    "> **(5) 가중치 업데이트:** 비용함수를 줄이는 방향으로 각 가중치 업데이트\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_GD.PNG' width='400'></center>\n",
    ">\n",
    ">$$\n",
    "\\begin{aligned}\n",
    "W_1 &:= W_0 - \\alpha \\frac{\\partial}{\\partial w} \\left[ \\text{Cost Function} \\right] \\\\ &= W_0 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_0} \\\\\n",
    "W_2 &:= W_1 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_1} \\\\\n",
    "W_3 &:= W_2 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_2} \\\\\n",
    "& \\vdots \\\\\n",
    "W &:= W - \\alpha \\frac{\\partial C(W)}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> - **$\\alpha$:** `학습률(Learing Rate)`이 낮으면 추정이 느리고, 높으면 최적점을 벗어나 오차가 증가될 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "**2) 예시:**\n",
    "\n",
    "> <center>$총비용(Y) = w_{정장}X_{정장} + w_{셔츠}X_{셔츠} + w_{타이}X_{타이}$</center>\n",
    "> <center><img src='Image/Expert/DL_Optimization_Example.png' width='400'></center>\n",
    ">\n",
    "> **(1+2) 네트워크 초기화 및 순전파:**  초기가중치($w^{initial}$)가 모두 150원이라면 `총비용은 1500원 출력`    \n",
    ">\n",
    "> **(3) 비용함수 평가:** \n",
    ">\n",
    "> - **회귀문제:** $MSE = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2 = 423200$    \n",
    ">\n",
    "> **(4) 역전파:** $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    ">\n",
    "> - $\\frac{\\partial MSE}{\\partial w_i} = \\frac{\\partial Y}{\\partial w_i} \\frac{d MSE}{dY} = X_i (\\hat{Y} - Y)$\n",
    ">\n",
    "> **(5) 가중치 업데이트:** $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> $$\\rightarrow W_i^{new} = W_i^{initial} - \\alpha \\frac{\\delta MSE}{\\delta W_i} = W_i^{initial} - \\alpha X_i(\\hat{Y} - Y)$$ \n",
    ">\n",
    "> - **정장:** $$W_{정장}^{new} = W_{정장}^{initial} - \\alpha X_{정장} (\\hat{Y} - Y) \\\\ = 150 - (1/500)2(920) = 146.32$$\n",
    ">\n",
    "> - **셔츠:** $$W_{셔츠}^{new} = W_{셔츠}^{initial} - \\alpha X_{셔츠} (\\hat{Y} - Y) \\\\ = 150 - (1/500)5(920) = 140.80$$\n",
    ">\n",
    "> - **타이:** $$W_{타이}^{new} = W_{타이}^{initial} - \\alpha X_{타이} (\\hat{Y} - Y) \\\\ = 150 - (1/500)3(920) = 144.48$$\n",
    ">\n",
    "> **(2') 순전파:** `총비용은 1430.08원`(69.92 감소)\n",
    ">\n",
    "> **(3') 비용함수 평가:** $MSE = \\frac{1}{2}(580 - 1430.08)^2 = 361318$ (14.6% 감소)\n",
    ">\n",
    "> **(4') 역전파:** $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$ \n",
    ">\n",
    "> - $\\frac{\\partial MSE}{\\partial w_i} = \\frac{\\partial Y}{\\partial w_i} \\frac{d MSE}{dY} = X_i (\\hat{Y} - Y)$\n",
    ">\n",
    "> **(5') 가중치 업데이트:** $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> $$\\rightarrow W_i^{new} = W_i^{initial} - \\alpha \\frac{\\delta MSE}{\\delta W_i} = W_i^{initial} - \\alpha X_i(\\hat{Y} - Y)$$ \n",
    ">\n",
    "> - **정장:** $$W_{정장}^{new} = W_{정장}^{old} - \\alpha X_{정장} (\\hat{Y} - Y) \\\\ = 146.32 - (1/500)2(850.08) = 142.92$$\n",
    ">\n",
    "> - **셔츠:** $$W_{셔츠}^{new} = W_{셔츠}^{old} - \\alpha X_{셔츠} (\\hat{Y} - Y) \\\\ = 140.80 - (1/500)5(850.08) = 132.30$$\n",
    ">\n",
    "> - **타이:** $$W_{타이}^{new} = W_{타이}^{old} - \\alpha X_{타이} (\\hat{Y} - Y) \\\\ = 144.48 - (1/500)3(850.08) = 139.38$$\n",
    ">\n",
    "> ---\n",
    ">\n",
    "> | **수량데이터** | **`epoch=0` <br> (초기가중치)** | **`epoch=1`** | **`epoch=2`** |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| 2 | 150 |    146.32  |    142.92  |\n",
    "| 5 | 150 |    140.80  |    132.30  |\n",
    "| 3 | 150 |    144.48  |    139.38  |\n",
    ">\n",
    "> (...) **`비용함수가 더이상 변하지 않는` 최소값이 될때까지 반복하여 `목표값(580원)에 가까운 추정치 확보`**\n",
    "\n",
    "---\n",
    "\n",
    "**3) 결론:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_MLP_Learning.PNG' width='600'></center>\n",
    "\n",
    "- **이슈:** 딥러닝은 추정해야할 가중치($W$)가 너무 많아 선형/로지시틱회귀분석에서의 `모든 가중치를 수학적 및 통계적으로 하나씩 규명 또는 추정하기 어려움`\n",
    "\n",
    "- **대응:** `비용함수를 최소`로 하는 위치의 가중치(W)를 추정하는 `최적화 알고리즘` 활용\n",
    "\n",
    "> - **Gradient Descent Algorithm:** 예시처럼 비용함수의 변화에 따라 `가중치 업데이트하는 대표적 알고리즘 `\n",
    ">\n",
    "> $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    ">\n",
    "> - **변화량크기:** 기울기 변화 크기로 `크면 가중치가 크게` 변경되고 `작으면 가중치가 작게` 변경\n",
    "> - **학습율(Learning Rate):** 이동속도로 `크면 가중치가 크게` 변경되고 `작으면 가중치가 작게` 변경\n",
    ">\n",
    "> <center><img src='Image/Expert/DL_Optimization_Flow.png' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**4) 최적화 알고리즘 종류 및 방향:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_Optimization_Direction.png' width='700'></center>\n",
    "\n",
    "<!-- <center><img src='Image/Expert/DL_Optimization_Direction_KR.png' width='700'>(https://www.slideshare.net/yongho/ss-79607172)</center> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과적합 개선 아이디어 3가지\n",
    "\n",
    "> **\"한계의 개선과 은닉층 증가로 성능은 대폭 향상되지만 `연산이 기하급수적으로 늘어나 학습시간이 오래걸리고 과적합(Overfitting) 가능성 높아짐`\"**\n",
    "\n",
    "---\n",
    "\n",
    "**0) 과적합?(Overfitting?)**\n",
    "\n",
    "<center><img src='Image/Expert/Bias_Variance2.jpeg' width='400'>(붉은색: Target, 파란색: Predicted)</center>\n",
    "\n",
    "> - **Underfitting:** Train 패턴 `적게 학습`하여, `주로 Bias 때문에 Train/Test 성능 낮음`\n",
    "> - **Overfitting:** Train 패턴 `과하게 학습`하여, `주로 Variance 때문에 Test 성능 낮음`\n",
    ">\n",
    "> <center><img src='Image/Expert/Bias_Variance4.png' width='400'></center>\n",
    ">\n",
    "> **\"알고리즘이 복잡해지면 `Bias와 Variance는 모두 감소하는 경항`이지만, `어느 시점(Train 패턴과 다른 패턴이 Test에 나타나는 시점)`부터는 Variance가 증가하여 `Test의 비용함수/에러가 증가`\"**\n",
    "\n",
    "---\n",
    "\n",
    "**1) Regularization:** `비용함수를 개선`하여 과적합 낮춤\n",
    "\n",
    "> **\"`추정 가중치가 커지면` 활성함수를 통해 `기울기가 급변`하게되어 비용함수 최소화가 어렵고 `과적합 높아짐`\"**   \n",
    ">\n",
    "> **(0) Linear Regression:** `MSE`를 비용함수로 사용\n",
    ">\n",
    "> <center> $\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2\\Biggr]$ </center>\n",
    ">\n",
    "> **(1) L1 Panelty:** `LASSO Regression`에 사용한 비용함수 반영    \n",
    ">\n",
    "> \\begin{align*}\n",
    "\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k \\left|w_i \\right|\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)\n",
    "\\end{align*}\n",
    ">\n",
    "> - `중요도가 낮은 변수`의 가중치는 0으로 출력하여 과적합 방지\n",
    "> - `변수선택 효과`가 있어 모델 복잡도를 효과적으로 제약\n",
    "> - 샘플 수보다 변수가 많더라도 변수선택 효과 때문에 `고차원의 데이터도 적용가능`\n",
    "> - `패널티의 정도`는 Hyperparameter로 사전 결정되며 교차검증이나 유사 방법으로 결정\n",
    "> - `모델에 제약`을 주며 정확도를 상승시킴   \n",
    ">\n",
    "> **(2) L2 Panelty:** `Ridge Regression`에 사용한 비용함수 반영   \n",
    ">\n",
    "> \\begin{align*}\n",
    "\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k w_i^2\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)\n",
    "\\end{align*}\n",
    ">\n",
    "> - `모든 가중치`를 일률적으로 `작게 만드는 경향`\n",
    "> - `중요도 낮은 변수`라도 0이 아닌 가중치를 출력하므로 `일반화 및 변수비교 효과`\n",
    "> - 일반화 및 패널티 효과를 높이기 위해 `L1 보다 L2를 많이 사용하는 경향`\n",
    "\n",
    "<center><img src='Image/Expert/DL_CF_L1L2.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_CF_L1L2_Compare.png' width='600'></center>\n",
    "<!-- (https://kevinthegrey.tistory.com/110) -->\n",
    "\n",
    "---\n",
    "\n",
    "**2) Drop Out:** 각 `batch마다` 은닉층의 `일부 뉴런을 무작위 확률로 제외(출력을 0으로 변환)`하면서 학습 \n",
    "\n",
    "> **\"`모든 직원`이 함께 일하는 것 < `소그룹`의 결과를 통합하는 것 $\\rightarrow$ 더욱 효율적일수도\"**    \n",
    ">\n",
    "> **\"각 단계의 줄어든 뉴런은 약한 학습이지만 `약한 모델들이 합쳐져 강력한 예측력`\"**   \n",
    ">\n",
    "> - `Labeled 데이터의 부족`과 `Overfitting 문제 해결`을 위해 Drop Out 제안 (Hinton et al. 2012)\n",
    "> - 학습할 때마다 `일부 유닛만을 사용하고 이를 반복해서 합치는 방식`으로 `Ensemble과 유사`\n",
    "> - 빠진 뉴런들로 예측 하기에 `여러개의 국소적 독립적 내부패턴 학습가능`\n",
    "> - 네트워크가 `뉴런의 특정 가중치에 덜 민감해짐`\n",
    "> - `더욱 일반화에 기여가 가능`하고 훈련 데이터에만 과적합 가능성 적어짐\n",
    "> - 너무 낮은 비율은 효과가 적고 너무 높은 비율은 과소적합 하기에 `20~50% 권장`\n",
    "> - 일반적으로 `Learning Rate(LR, 10->100)과 Momentum(0.9 or 0.99)을 높여 사용`\n",
    "> - LR을 높여 가중치의 크기를 줄이면 Ridge와 유사하게 `과적합이 줄어 높은 성능`\n",
    "\n",
    "<center><img src='Image/Expert/DL_DropOut.png' width='500'></center>\n",
    "<!-- (https://t1.daumcdn.net/cfile/tistory/99324B335D383CBD1B) -->\n",
    "\n",
    "> - 랜덤한 뉴런을 제거하는 것 대신, `연결선을 랜덤하게 제거하는 DropConnect 방법(별도 외장함수)`도 존재 (Wan et al. 2013)\n",
    "> - Dropout을 Dropconnect로 표현할 수 있지만, `Dropconnect를 Dropout으로 표현하기 어려움`\n",
    "> - `특정 노드를 비활성화` = `특정 노드에 연결된 모든 가중치를 비활성화`\n",
    "\n",
    "<center><img src='Image/Expert/Dropout_Dropconnect.png' width='500'>(https://stats.stackexchange.com/questions/201569/what-is-the-difference-between-dropout-and-drop-connect)</center>\n",
    "\n",
    "---\n",
    "\n",
    "> - 성능변화 예시:\n",
    "\n",
    "<center><img src='Image/Expert/DL_Dropout_Dropconnect.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**3) Early Stopping:** Test 에러가 빠르게 증가하는 경우 `계속 학습하지 않고 일찍 종료`하는 방법   \n",
    "\n",
    "> **\"데이터가 Train/Validation/Test로 구분되어 있을때, `Validation/Test의 비용함수가 낮으면 멈추므로 Train을 계속 학습하는 과적합 가능성 낮춤`\"**   \n",
    "\n",
    "<center><img src='Image/Expert/DL_Overfitting_Epoch.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_Overfitting_EarlyStopping.png' width='600'></center>\n",
    "<!-- (https://kevinthegrey.tistory.com/110) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 최적화\n",
    "\n",
    "> **\"최종적으로 `미래 데이터 예측에 적합한 최적 가이드를 하이퍼파라미터로 표현`\"**\n",
    "\n",
    "<center><img src='Image/Expert/Hyperparameter_Tuning.png' width='900'>(Hyperparameter tuning for big data using Bayesian optimisation)</center>\n",
    "\n",
    "---\n",
    "\n",
    "**0) 에러분석:** `에러를 수동으로 분석`하여 `모델링에 도움될 패턴을 찾아내어 반영`\n",
    "\n",
    "**1) 학습성능변화:** 하이퍼파라미터의 변화에 따른 `성능변화를 추적하여 최적 하이퍼파라미터 선택`    \n",
    "\n",
    "> - `데이터 분리 비율`\n",
    "> - `가중치 초기값`\n",
    "> - `은닉층의 수`\n",
    "> - `Batch 크기`\n",
    "> - `Epoch 횟수`\n",
    "> - `Cost Function 선정` <span style=\"color:red\">$\\Rightarrow$ **\"`실제문제를 해결하려면` 비용함수보다 `현실을 반영한 평가함수`를 만들 수 있는 능력이 훨씬 중요!\"**</span>\n",
    ">> - 재고만 줄이면 되는가? Shortage는? MSE를 쓰면 진짜 해결되는가?\n",
    "> - `Learning Rate 크기`\n",
    "> - `Regularized 비중`\n",
    "> - `DropOut 비율`\n",
    "> - `Early Stopping 고려기간`\n",
    "\n",
    "---\n",
    "\n",
    "- **예시:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_hyperparameter_epoch.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_hyperparameter_search_simple.jpg' width='800'></center>\n",
    "\n",
    "<center><img src='Image/Expert/DL_hyperparameter_table_simple.png' width='350'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공지능 발전방향 요약\n",
    "\n",
    "> **1) 1950년대 `퍼셉트론(Perceptron)`에서 시작된 인공신경망 연구는 1980년대 `오류의 역전파알고리즘(Error Backpropagation Algorithm)`으로 `다층퍼셉트론(Multilayer perceptron)`을 학습할 수 있게 되면서 발전을 이루었다.**\n",
    ">\n",
    "> **2) 하지만 `Gradient Vanishing, Labeled 데이터의 부족, Overfitting, Local Minimum 등`이 잘 해결되지 못해 2000년대 초까지 인공신경망 연구는 잠시 지지부진 하였고,** \n",
    ">\n",
    "> **3) 2006년부터 `볼츠만머신을 이용한 Unsupervised Learning`인 Restricted Boltzmann Machine(RBM), Deep Belief Network(DBN), Deep Boltzmann Machine(DBM), Convolutional Deep Belief Network 등이 개발되면서 Unlabeled data를 이용하여 `Pre-training을 통해 다층퍼셉트론의 한계점이 극복될 수 있는 가능성`을 확인하였다.**\n",
    ">\n",
    "> **4) `Rectified Linear Unit (ReLU), DropOut, DropConnect 등의 발견`으로 Vanishing Gradient문제와 Overfitting 이슈를 해결할 수 있게 되었으며, `Local Minimum 문제도 High Dimension Non-convex Optimization에서 얼마나 Global Minimum과 큰 차이가 없다`라는 연구도 있다.**\n",
    ">\n",
    "> **5) 이후 디지털 시대에 맞게 `(단순히 몸집만 큰게 아닌 실속있는)빅데이터를 적극적으로 이용`함으로서 Labeled 데이터가 많아지고 `많은 분야에서 성능이 빠르게 증가되어 상용화에 기여`하고 있다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **기초 알고리즘과 딥러닝 알고리즘 비교**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계추론에서 기계학습/딥러닝학습으로\n",
    "\n",
    "> **\"데이터 과학은 크게 `2가지 관점`으로 발전\"**\n",
    "> - **통계학(Inferential Statistics):**\n",
    ">> - 데이터 과학의 근간\n",
    ">> - `통계학 기반의 다양한 기능들`은 딥러닝, 패턴인식, 기계학습 등에서 사용중\n",
    "> - **컴퓨터공학(Computer Science):**\n",
    ">> - `부품 가격 절하와 성능 향상`은 분석 성능에 영향\n",
    ">> - `통계학에 컴퓨터 공학적인 접근`을 받아들이게 하고, `통계와 기계학습 영역이 겹합되어 시너지`를 이루게 됨\n",
    "\n",
    "---\n",
    "\n",
    "- **통계학습(Statistical Learning) vs 기계학습(Machine Learning):** 알고리즘 생성 방식\n",
    "\n",
    "<center><img src='Image/Advanced/StatisticsMachinelearning.png' width='800'>(루이, 다빈치랩스)</center>\n",
    "\n",
    "> **\"데이터를 통해 문제 해결한다는 점은 일맥상통하나, 해결하는 `목표/전략/방식에 대한 출발점이 다르며` 점차 `경계가 모호`해지고 있음\"**\n",
    "> - **통계학습:**\n",
    ">> - 인공지능 및 기계학습에 대한 `통계의 대응`\n",
    ">> - 기술통계, 추론통계에서 나아간 개념이지만 엄밀히 말하면 결국 `추론 통계`\n",
    ">> - `실패`의 위험을 줄여 신뢰성을 높이는 것\n",
    ">> - `모델기반` 사고방식과 `데이터기반` 사고방식을 모두 활용\n",
    ">> - `정확도` 자체에 매몰되지 않고 `모델설명력`과 다양한 `가정` 고려\n",
    ">> - 모델들이 대부분 내부 구조 파악이 쉬운 `화이트박스(White-box)` 모형/알고리즘\n",
    ">\n",
    "> - **기계학습:**\n",
    ">> - `인공지능, 패턴인식 등`의 발전 역사와 결이 같음\n",
    ">> - 에이전트라는 `인간과 비슷한 녀석`이 사물을 보고, 듣고, 인식하게 하는 것을 목적으로 발전\n",
    ">> - `성공`의 확률을 높이는 것\n",
    ">> - 인과성보다 `정확도`에 굉장히 의존적이고 모델이나 가정에 크게 관심 없음\n",
    ">> - 모델들의 내부 구조를 속속들이 알아야 할 이유가 없고 `맞추면 장땡`\n",
    ">> - 모델 내부 구조를 알 수가 없는 `블랙박스(Black-box)` 모형/알고리즘\n",
    "\n",
    "|  | **통계학습(Statistical Learning)** | **기계학습(Machine Learning)** |\n",
    "|:---:|:---:|:---:|\n",
    "| **이론적 배경** | `통계학` | `컴퓨터과학` |\n",
    "| **발전 기반** | 통계학, 수치해석 등 | 패턴인식, 인공지능 등 |\n",
    "| **모형 구조** | 대부분 `화이트박스` | 대부분 `블랙박스` |\n",
    "| **관심 목적** | `설명력`, `실패위험` 줄이기 | `정확성`, `성공확률` 높이기 |\n",
    "| **주 사용 데이터** | 관측치 및 변수가 적은 경우 | 관측치 및 변수가 많은 경우 |\n",
    "| **상황/가정 반영** | 의존적 <br> (독립성, 정규성, 등분산성 등) | 독립적 <br> (대부분 무시) |\n",
    "| **학습 방법** | 데이터에 맞게 `최적화 중점` | `반복학습으로 모델 구축` 중점 |\n",
    "| **성능 평가** | 데이터의 해석과 가정 적합성 등 | 분할 데이터 반복 평가 |\n",
    "| **특징** | 가설(Hypothesis), 모집단(Population), 표분(Sample)에 기반하여  데이터를 기술(Descriptive)하거나 추론(Inference)하는데 이용 | 예측력(Prediction) 중심의 다양한 문제 해결을 위한  지도(Supervised), 비지도(Unsupervised), 강화학습(Reinforcement) 등의 방법론 구축에 이용 |\n",
    "| **문제 예시** | 대기오염과 호흡기 질환의 관계 <br> 배너위치에 따른 컨텐츠 클릭 빈도 변화 <br> 신규 장비의 불량률 감소 효과 분석      임상을 통한 신약의 효능 분석 | 이미지 데이터의 객체 구분 <br> 상황이나 사물인식 성능 향상 <br> 음성인식을 통한 AI스피커 성능 향상 <br> MRI데이터 사용 암 환자 조기 진단 |\n",
    "\n",
    "<center><img src='Image/Expert/DL_AutoFE.PNG' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀분석의 신경망 표현\n",
    "\n",
    "**1) 알고리즘 함수세팅:** \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) = w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \n",
    "= [w_0~w_1~w_2~\\cdots~w_k]\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix} \\\\\n",
    "&= [1~X_1~X_2~\\cdots~X_k]\\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_k \\end{bmatrix} \n",
    "= \\begin{bmatrix} 1~X_{11}~X_{21}~\\cdots~X_{k1} \\\\ 1~X_{12}~X_{22}~\\cdots~X_{k2} \\\\ \\vdots \\\\ 1~X_{1t}~X_{2t}~\\cdots~X_{kt} \\end{bmatrix}\n",
    "\\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_k \\end{bmatrix} = XW = WX\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Expert/Example_LinearRegression.png' width='900'></center>\n",
    "\n",
    "**2) 함수 추정을 위한 비용함수:** 나의 주장 기반 알고리즘의 `예측값`($\\hat{Y}$)과 `실제 데이터`($Y$)의 차이를 평가하는 함수\n",
    "> - **손실함수(Loss Function):** `하나의 데이터(Single Row)`에서 예측값과 정답의 차이를 평가\n",
    "> - **비용함수(Cost Function):** `모든 데이터`에서 예측값과 정답의 차이를 평가\n",
    "> \n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Y - \\hat{Y} &= Y - WX = \\text{residual} = \\text{cost} \\\\\n",
    "&= \\sum_{i=1}^{m} \\left[ \\sum_{j=1}^{k} (Y_{i} - w_{j}X_{j}) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    ">\n",
    "> - `회귀분석`은 여러가지의 비용함수 중 `최소제곱법/최소자승법`을 사용 \n",
    "> - `최소제곱법/최소자승법`를 최소로 하는 `직선`을 추정하여 `계수(coefficient)`를 결정\n",
    ">\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{W} = \\underset{W}{\\arg\\min} \\sum_{i=1}^{m} \\left[\\sum_{j=1}^{k} (Y_{i} - w_{j}X_{j})^2 \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**3) Linear Regression의 신경망 표현:** `입력값` $X$변수에 `가중치` $w$가 반영되어 `출력값` $Y$에 영향\n",
    "\n",
    "\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(X_0, X_1, X_2, \\cdots, X_k) \\\\\n",
    "&= w_0X_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= \\sum_{i=0}^{k} w_i X_i \\rightarrow \\sigma(\\sum_{i=0}^{k} w_i X_i) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<center><img src='Image/Expert/DL_Comparing1.PNG' width='250'></center>\n",
    "\n",
    "\\begin{align*}\n",
    "&(1)~X: 입력층 \\\\\n",
    "&(2)~w_i: 가중치 \\\\\n",
    "&(3)~\\sigma: 활성함수(Linear) \\\\\n",
    "&(4)~Y: 출력층 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱회귀분석의 신경망 표현\n",
    "\n",
    "**1) 알고리즘 함수세팅:** 분류문제를 푸는 대표적인 알고리즘 `Logistic Regression`\n",
    "\n",
    "- `범주형 종속변수`의 적합/추정하기 위한 `변환과정` 필요\n",
    "- `Logistic/Sigmoid Function`를 사용하여 `곡선(S-curve) 형태로 변환`\n",
    "\n",
    "<center><img src='Image/Expert/Linear_Logistic.png' width='600'></center>\n",
    "\n",
    "> **(1) 회귀분석 추정:**\n",
    ">\n",
    ">\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) \\\\\n",
    "&= w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= XW\n",
    "\\end{align*}\n",
    ">\n",
    "> **(2) 시그모이드 변환(Logistic/Sigmoid Transformation):** `Binary Classification` 반영하는 `곡선 형태`로 변경\n",
    ">\n",
    ">\\begin{align*}\n",
    "Pr(\\hat{Y}) &= \\dfrac{1}{1+exp(-\\hat{Y})} \\\\ \n",
    "&= \\dfrac{1}{1+exp(-XW)} \\\\\n",
    "&= \\dfrac{exp(XW)}{1+exp(XW)}\n",
    "\\end{align*}\n",
    ">\n",
    "> **(3) 로짓 변환(Logit Transformation):** `X`의 선형관계 형태로 변환하여 `변수들`로 `Y=1`인 확률 추정\n",
    ">\n",
    ">\\begin{align*}\n",
    "Pr(\\hat{Y}) \\left( 1 + exp(XW) \\right) &= exp(XW) \\\\\n",
    "Pr(\\hat{Y}) &= \\left( 1 - Pr(\\hat{Y}) \\right) exp(XW) \\\\\n",
    "\\text{Odds(ratio):} \\left( \\dfrac{Pr(\\hat{Y})}{1 - Pr(\\hat{Y})} \\right) &= exp(XW) \\\\\n",
    "\\text{Logit(log-odds): } log \\left( \\dfrac{Pr(\\hat{Y})}{1 - Pr(\\hat{Y})} \\right) &= XW = w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**2) 함수 추정을 위한 비용함수:** 나의 주장 기반 알고리즘의 `분류값`($Pr(\\hat{Y})$)과 `실제 데이터`($Y$)의 차이를 평가하는 함수\n",
    "\n",
    "- **이슈: `잔차`를 사용하는 Linear Regression 비용함수 적용 어려움**    \n",
    "\n",
    "> (1) 분류문제에서는 $\\hat{Y}$를 사용한 `잔차(에러)계산이 무의미`   \n",
    ">\n",
    "> (2) 잔차($Y - \\hat{Y}$)를 `시그모이드 및 로짓 변환`을 하면 Non-convex 형태가 되서 `최소값(Global Minimum) 추정 어려움`    \n",
    ">\n",
    "> (3) 정확한 `수학적 방정식 기반` 계수추정 어렵기에 `확률론적 접근 필요`    \n",
    ">\n",
    "> <center><img src='Image/Expert/Cost_Comparison.png' width='600'></center>\n",
    "\n",
    "- **방향:** 회귀문제와 달리 `새로운 비용함수`가 필요\n",
    "\n",
    "> - Y를 `잘` 분류하면 `cost=0`으로 그렇지 않으면 cost=$\\infty$가 되는 방향\n",
    ">> - (빨간선) 실제값이 `1`일때 예측값이 `1`이면 Cost는 `0`\n",
    ">> - (빨간선) 실제값이 `1`일때 예측값이 `0`이면 Cost는 `무한대`\n",
    ">\n",
    "> \\begin{align*}\n",
    "\\text{Cost} = \\begin{cases} -log(Pr(\\hat{Y})) ~~~~ & \\text{in the case of } ~~~ Y = 1 \\\\ -log(1-Pr(\\hat{Y})) ~~~~ & \\text{in the case of } ~~~ Y = 0 \\end{cases}\n",
    "\\end{align*}\n",
    "> <center><img src='Image/Expert/Cost_Logistic.png' width='600'></center>\n",
    "\n",
    "- **Cross Entropy 등장:** Y가 0과 1인 경우의 `Cost를 결합`하여 하나의 식으로 표현\n",
    "\n",
    "> - 로지스틱 알고리즘은 `비용함수`로 `Cross Entropy`를 사용하고 `최소로 하는 계수/가중치 추정`\n",
    "> - `Y=0`인 경우 `파란부분`만 남고 `Y=1`인 경우 `빨간부분`만 남아 `Class별`로 독립적으로 작동\n",
    "> - 분류문제의 `Cost 함수는 다양`하고 많지만 통계학적으로 `Cross Entropy`는 계수 추정에 `효율적`인 편\n",
    "> - `Convex 형태`이기 때문에 `Global Minimum`을 찾기가 용이함\n",
    "> - 추정된 계수/가중치($\\hat{w}$)로 방정식을 만들어 Y=1인 `분류확률` 계산 가능\n",
    ">\n",
    ">\\begin{align*}\n",
    "\\text{Cost} &= \\sum_{i=1}^{m} \\left[ - \\color{red}{\\hat{Y}_{i} log (Pr(\\hat{Y}_{i}))} - \\color{blue}{(1-\\hat{Y}_{i}) log (1-Pr(\\hat{Y}_{i}))} \\right] \\\\\n",
    "\\hat{W} &= \\underset{W}{\\arg\\min} \\sum_{i=1}^{m} \\left[\\text{Cost} \\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**3) Logistic Regression의 신경망 표현:** Linear Regression를 실행한 후 `활성함수를 Logistic/Sigmoid로 반영`\n",
    "\n",
    "\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(X_0, X_1, X_2, \\cdots, X_k) \\\\\n",
    "&= w_0X_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= \\sum_{i=0}^{k} w_i X_i \\rightarrow \\sigma(\\sum_{i=0}^{k} w_i X_i) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<center><img src='Image/Expert/DL_Classification.png' width='450'></center>\n",
    "\n",
    "\\begin{align*}\n",
    "&(1)~X: 입력층 \\\\\n",
    "&(2)~w_i: 가중치 \\\\\n",
    "&(3)~\\sigma: 활성함수(Logistic/Sigmoid) \\\\\n",
    "&(4)~Y: 출력층 \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다층신경망 구조로의 일반화\n",
    "\n",
    "**1) 은닉층이 반영된 신경망 구조:** \n",
    "\n",
    "<center><img src='Image/Expert/DL_Comparing2.PNG' width='400'></center>\n",
    "\n",
    "> **(1)** `입력값` $X$변수에 `가중치` $w$가 반영되어 `은닉층` $H$에 영향\n",
    ">\n",
    "> **(2)** `은닉층` $H$변수에 `가중치` $v$가 반영되어 `출력층` $Y$에 영향  \n",
    "\n",
    "\\begin{align*}\n",
    "Y \\approx \\hat{Y} &= f(H_0, H_1, \\cdots, H_{k-2}) \\\\\n",
    "&= v_0H_0 + v_1H_1 + \\cdots + v_{k-2} H_{k-2} \\\\\n",
    "&= v_0h_0(X_{0}, X_{1}, \\cdots, X_{k}) + v_1h_1(X_{0}, X_{1}, \\cdots, X_{k}) + \\cdots + v_{k-2} h_{k-2}(X_{0}, X_{1}, \\cdots, X_{k}) \\\\\n",
    "&= v_0(w_{00}X_{0} + w_{10}X_{1} + \\cdots + w_{k0} X_{k}) + v_1(w_{01}X_{0} + w_{11}X_{1} + \\cdots + w_{k1} X_{k}) + \\\\\n",
    "&\\cdots + v_{k-2}(w_{0k-2}X_{0} + w_{1k-2}X_{1} + \\cdots + w_{kk-2} X_{k}) \\\\\n",
    "&= \\sum_{i=0}^{k-2} v_i ( \\sum_{j=0}^{k} w_j X_j ) \\rightarrow \\sigma_{v}(\\sum_{i=0}^{k-2} v_i \\sigma_{w} ( \\sum_{j=0}^{k} w_j X_j )) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "&(1)~X: 입력 노드 \\\\\n",
    "&(2)~w_i: 가중치(입력 노드) \\\\\n",
    "&(3)~\\sigma_{w}: 활성함수 \\\\\n",
    "&(4)~H: 히든 노드 \\\\\n",
    "&(5)~v_i: 가중치(히든 노드) \\\\\n",
    "&(6)~\\sigma_{v}: 활성함수 \\\\\n",
    "&(7)~Y: 출력 노드 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정확성 vs. 설명력 기반 활용방향\n",
    "\n",
    "|  | **통계학습(Statistical Learning)** | **기계학습(Machine Learning)** |\n",
    "|:---:|:---:|:---:|\n",
    "| **이론적 배경** | `통계학` | `컴퓨터과학` |\n",
    "| **발전 기반** | 통계학, 수치해석 등 | 패턴인식, 인공지능 등 |\n",
    "| **모형 구조** | 대부분 `화이트박스` | 대부분 `블랙박스` |\n",
    "| **관심 목적** | `설명력`, `실패위험` 줄이기 | `정확성`, `성공확률` 높이기 |\n",
    "| **주 사용 데이터** | 관측치 및 변수가 적은 경우 | 관측치 및 변수가 많은 경우 |\n",
    "| **상황/가정 반영** | 의존적 <br> (독립성, 정규성, 등분산성 등) | 독립적 <br> (대부분 무시) |\n",
    "| **학습 방법** | 데이터에 맞게 `최적화 중점` | `반복학습으로 모델 구축` 중점 |\n",
    "| **성능 평가** | 데이터의 해석과 가정 적합성 등 | 분할 데이터 반복 평가 |\n",
    "| **특징** | 가설(Hypothesis), 모집단(Population), 표분(Sample)에 기반하여  데이터를 기술(Descriptive)하거나 추론(Inference)하는데 이용 | 예측력(Prediction) 중심의 다양한 문제 해결을 위한  지도(Supervised), 비지도(Unsupervised), 강화학습(Reinforcement) 등의 방법론 구축에 이용 |\n",
    "| **문제 예시** | 대기오염과 호흡기 질환의 관계 <br> 배너위치에 따른 컨텐츠 클릭 빈도 변화 <br> 신규 장비의 불량률 감소 효과 분석      임상을 통한 신약의 효능 분석 | 이미지 데이터의 객체 구분 <br> 상황이나 사물인식 성능 향상 <br> 음성인식을 통한 AI스피커 성능 향상 <br> MRI데이터 사용 암 환자 조기 진단 |\n",
    "\n",
    "---\n",
    "\n",
    "**1) 기계학습 활용 데이터분석의 현실:**\n",
    "\n",
    "> **\"(이상적으로) 머신러닝 알고리즘에 데이터를 `학습/적합/모델링` 한다는 건..\"**     \n",
    ">\n",
    "> (1) 사람/사물/시스템이 `어떻게 동작하는지 이해`의 과정    \n",
    "> (2) 사람/사물/시스템이 만들어내는 `데이터를 체계적으로 요약`하는 과정    \n",
    "> (3) 미래의 예측값과 실제값의 비교로 `사람/사물/시스템을 일반화`하는 과정    \n",
    "> (4) 일반화된 사람/사물/시스템으로 더욱 `효과적이고 체계적인 의사결정`하는 방법    \n",
    "\n",
    "> **\"(현실적으로) 많은 시간과 비용이 투입되지만, `우리 사회를 이해((1),(2),(3)) <<< 빠른 의사결정(4)`에 집중되어 효과는 글쎄..\"**\n",
    ">\n",
    "> - 사회를 이해하기 위한 `사회학/교육학/철학/경제학 등`의 학문은 `((1),(2),(3))에 집중`\n",
    "> - `경영과학/컴퓨터과학/산업공학 등`의 학문은 기술적인 자동화나 인공지능을 반영하는 `(4)에 집중`\n",
    "> - 학문적 출신(?)에 따라 `분석에 대한 관점 차이` 또는 `비즈니스 방향 차이`가 존재할 수 있음\n",
    ">\n",
    "> <center><img src='Image/Expert/Programming_Multidisciplinary.png' width='500'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) `정확성 vs. 설명력`은 모두 욕심낼 수 없는 반비례 관계:**\n",
    "- 대부분의 `기계학습 및 딥러닝 모델`은 이론적 기반이 없기 때문에 `1회성 추정을 반복`하는 알고리즘  \n",
    "- `통계추론`은 이론적 기반의 `결과의 범위(신뢰구간)와 설명력`을 제공하기에 `반복추정이 필요없는` 알고리즘\n",
    "\n",
    "<center><img src='Image/Expert/Performance_Explanability.png' width='600'></center>\n",
    "\n",
    "- **설명력 최근 연구동향:**   \n",
    "> - [LIME](https://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html)   \n",
    "> - [DARPA](https://bdtechtalks.com/2019/01/10/darpa-xai-explainable-artificial-intelligence/)   \n",
    "\n",
    "---\n",
    "\n",
    "**3) 반비례 관계 원인:** `회귀분석(통계학습) vs 딥러닝(기계학습)`\n",
    "\n",
    "- **회귀분석(통계학습):** 함수의 `선형성`을 추정하는 정확성 보다 `설명력에 집중`하는 알고리즘\n",
    "- **딥러닝(기계학습):** 함수의 `비선형성`을 추정하는 설명력 보다 `정확성에 집중`하는 알고리즘\n",
    "\n",
    "| -                            | **회귀분석**                                            | **딥러닝**                                   |\n",
    "|----------------------------- |:----------------------------------- |:----------------------------------- |\n",
    "| **모델특징**                 | -                                                       | -                                                      |\n",
    "| 분석목적                     | 선형성파악(설명가능)                                    | 비선형성파악(설명불가)                                 |\n",
    "| 이론적(수학적) 근거          | 존재                                                    | 미존재                                                 |\n",
    "| **분석단계 특징(전처리)**    | -                                                       | -                                                      |\n",
    "| 데이터 로딩                  | <span style=\"color:blue\">Panel Data</span>              | <span style=\"color:red\">다양(운이좋으면 Panel)</span>  |\n",
    "| 데이터 빈칸 채우기/삭제      | <span style=\"color:red\">분석필요</span>                 | <span style=\"color:red\">분석필요</span>                |\n",
    "| 데이터 컬럼 추가/삭제        | <span style=\"color:red\">분석필요+민감</span>            | <span style=\"color:red\">분석필요+덜민감</span>         |\n",
    "| 데이터 분리                  | <span style=\"color:blue\">Train/Validate/Test</span>     | <span style=\"color:blue\">Train/Validate/Test</span>    |\n",
    "| 데이터 스케일링              | <span style=\"color:red\">분석필요/미필요</span>          | <span style=\"color:red\">분석필요</span>                |\n",
    "| **분석단계 특징(모델링)**    | -                                                       | -                                                      |\n",
    "| 입력 확인 및 변환            | <span style=\"color:blue\">Panel Data</span>              | <span style=\"color:red\">다양(정해지지 않음)</span>     |\n",
    "| 데이터 모델연결              | <span style=\"color:blue\">자동화</span>                  | <span style=\"color:red\">반자동화</span>                |\n",
    "| 비용함수(Cost)               | <span style=\"color:blue\">최소제곱에러(MSE)</span>       | <span style=\"color:red\">다양</span>                    |\n",
    "| 추정함수(Optimizer)          | <span style=\"color:blue\">고정(미분1회 대체가능)</span>  | <span style=\"color:red\">다양(미분지속)</span>          |\n",
    "| **분석단계 특징(검증)**      | -                                                       | -                                                      |\n",
    "| 정확성지표                   | <span style=\"color:red\">다양</span>                     | <span style=\"color:red\">다양</span>                    |\n",
    "| 잔차진단활용                 | <span style=\"color:red\">가능(분석필요)</span>           | <span style=\"color:blue\">불가</span>                   |\n",
    "| **분석단계 특징(결과해석)**  | -                                                       | -                                                      |\n",
    "| 관계성 시각화/영향력 해석    | <span style=\"color:red\">가능(분석필요)</span>           | <span style=\"color:blue\">불가</span>                   |    \n",
    "\n",
    "---\n",
    "\n",
    "**4) 활용 장단점 비교 요약:**\n",
    "\n",
    "<center><img src='Image/Expert/DL_Comparing2.PNG' width='400'></center>\n",
    "\n",
    "|  | **회귀분석** | **딥러닝** |\n",
    "|:-:|:-:|:-:|\n",
    "| **구조** | `은닉층이 없고` 입력층과 출력층이 다이렉트로 연결 | 입력층과 출력층이 `은닉층을 통해` 복잡한 비선형성으로 연결 |\n",
    "| **패턴 반영 횟수**<br>(모델수) | `1개`의 회귀분석 | `2개 이상(은닉층과 노드 갯수에 따라)`의 회귀분석 |\n",
    "| **설명성** | `가능`<br>(입력값과 출력값의 영향을 명확히 설명) | `불가능`<br>(알파고의 우승 이유를 모르듯 사람이 이해하기 어려움) |\n",
    "| **설명성 예시**<br>(부동산가격<br>영향요인) | **[결과]**<br>- 공원의 개수가 `2만큼 영향`<br>- 지하철역 유무가 `8만큼 영향`<br>- 유흥업소 개수가 `-3만큼 영향`<br><br>**[인싸이트]**<br>- 지하철 유무가 `큰 영향`을 주며 <br>공원과 유흥업소는 `반대의 영향을 주지만 크기는 비슷`<br>- `정확도는 70%`이기에 다른 케이스에선 <br>`30% 정도 오차` 발생 가능 | **[결과]**<br>- 공원의 개수가 1번 은닉층에서 `0.5만큼 영향`<br>- 2번 은닉층에선 `-1만큼 영향`<br>- 3번 은닉층에선 `0.1만큼 영향`<br>- 집값에는 1번 은닉층이 `1만큼 영향`<br>- 2번 은닉은 `3만큼 영향`<br>- `…`<br><br>**[인싸이트]**<br>- `그냥 모르겠음`<br>- `정확도는 95%`이기에 다른 케이스에선 <br>`동일 데이터로 정확하게 예측` 가능 |\n",
    "| **요약** | 단 `1개의 수학적 표현/가설/Layer`로<br>입력 데이터에 가장 적합한 `수학적 표현(Parameters) 추정` | `여러개의 수학적 표현/가설/Layer`로 입력데이터에<br>가장 적합한 수학적 표현을 `단계별 업데이트 추정`<br>$\\rightarrow$ Layer가 늘어날수록 더 복잡한 패턴(특징) 반영 가능<br>$\\rightarrow$ Layer 1개는 1차방정식이지만 Layer 2개는 2차방정식 |\n",
    "| **한계** | `설명은 되지만 미래에 나타나기 어려운 패턴만 모델링` | `설명은 안되지만 미래에 나타날 수 있는 패턴을 모델링`<br>- 많은 다층신경망도 `성능이 낮을 수 있기에(과적합)`,<br>모델을 간소화 하면서도 `성능을 높일 방법을 찾아야`<br>- `얼마나 많은 층을 사용해야하는지` 예측 불가<br>- `사람이 설정해줘야 하는 많은 Hyper-parameter` 존재 |\n",
    "\n",
    "- 데이터분석 프로세스는 동일하나 `딥러닝 알고리즘이 자유도가 높음`\n",
    "- 자유도는 높으나 그만큼 `분석가가 고려해야 할 것들이 많고 명확한 설명이 어려워 결과 신뢰성 어려움`\n",
    "- 단, 고려해야 할 것들을 `엄격하고 객관적으로 반영한다면 설명이 어려워도 신뢰성은 높일 수 있음`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어려움에도 딥러닝이 필요한 이유\n",
    "\n",
    "**1) 머신러닝 방향 및 발전**\n",
    "\n",
    "|  | **통계추론** | **기계학습** | **딥러닝** |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| **설명** | - 정보요약을 위한 `기술통계`<br>- `가설 기반 수학적 검정`<br>- `수치기반 예측통계` | - `데이터의 패턴을 찾기 위한 컴퓨터 알고리즘`<br>- `예측, 분류, 군집화, 차원변환, 텍스트, 시계열 분석 등` | - 인간 `뇌구조를 모방한 신경망`<br>- `인공지능(AI)의 출발점` |\n",
    "\n",
    "\n",
    "> **\"기존 분석은 `인간지능에 의해 분석된 고정 형태의 시스템`이었다면, 최근 분석은 `지속적이고 반복적이며 자동수행되는 학습을 기반으로 한 진화되는 성능개선 시스템` 방향\"**\n",
    ">\n",
    "> **(1) 통계추론과 기계학습 관계:** 통계적 분석기법 중 `회귀분석은 인공신경망과 지도학습 알고리즘으로 발전`\n",
    ">\n",
    "> **(2) 데이터마이닝과 기계학습 관계:** `데이터의 패턴과 지식 발견`을 위해 고객관계관리, 마케팅 영역의 군집화, 분류, 예측, 텍스트, 시계열 분석 등이 `지도학습/비지도학습으로 발전`  \n",
    ">\n",
    "> **(3) 기계학습과 인공신경망 관계:** 인간 뇌구조를 모방하여 `신경망의 층을 깊게 네트워크화 한 딥러닝 알고리즘으로 문제해결 성능향상`  \n",
    "\n",
    "---\n",
    "\n",
    "**2) 딥러닝 적용 필요성:**\n",
    "\n",
    "**(1) 딥러닝은 불완전하고 전처리없는 `Raw 데이터`에서 `자동으로 변수들을 추출`하고 학습 가능**\n",
    "\n",
    "- **기존:** 사람이 `이론과 경험을 바탕으로 직접 생성`\n",
    "\n",
    "> - **Hand-crafted Feature:** 통계추론이나 기계학습시 문제 해결을 위해 `수동으로 생성한 변수`로 일반적으로 `이론이나 사람의 경험을 기반`으로 추정\n",
    "\n",
    "- **한계:** 큰 정보를 압축하는 과정에서 `많은 정보 손실이나 오류반영 가능성`\n",
    "\n",
    "> - 이론이나 사람의 경험 기반 변수들은 적은 양의 데이터로 큰 정보를 압축하여 생성했기 때문에, 알고리즘이 `학습하기 힘든 데이터의 특징을 찾아내기도` 하지만 알고리즘이 `데이터의 특징을 학습하기 힘들게 할 수도` 있음\n",
    "> - `사람의 영향을 최소화` 하기 위해 `변수추출을 알고리즘에 맡기려고` 하지만 아직까진 완벽하진 않음\n",
    "\n",
    "- **방향:** `사람의 영향을 최소화` 및 Layer 적용을 통해 `새로운 Feature Extraction`\n",
    "\n",
    "> - `CNN`의 Layer, `Autoencoder`의 압축되어 있는 Representation Vector, `Embedding 등`\n",
    "\n",
    "<center><img src='Image/Expert/DL_AutoFE.PNG' width='600'></center>\n",
    "\n",
    "**(2) 데이터 형태와 갯수에 상관없이 약간의 처리를 통해 딥러닝은 `여러개의 입력을(Multiple Input) 받아 여러개의 출력을(Multivariate Output) 쉽게 가능`하게 함**\n",
    "\n",
    "- **기존 및 한계:** 기계학습 알고리즘도 다양한 입출력을 사용할 순 있지만 `알고리즘에 따라 제한적`\n",
    "\n",
    "- **방향:** 입출력에 따른 `알고리즘 구분 없이 딥러닝 구조의 처리를 통해 모든 경우 가능`\n",
    "\n",
    "<center><img src='Image/Expert/Regression-Algorithms-Tree2.png' width='900'></center> \n",
    "\n",
    "**(3) 비교적 `길이가 긴 시퀀스(Sequence)에 걸쳐있는 데이터 패턴` 추출도 가능**\n",
    "\n",
    "- **기존 및 한계:** 입력되는 데이터가 길수록(시간) `추정해야 할 파라미터 양과 연산시간이 급격히 늘어나고`, 가중치 파라미터가 `유일하지 않기 때문에 결과 신뢰성 이슈`\n",
    "\n",
    "- **방향:** 알고리즘 `내부에서 이전 시점들의 데이터 학습을 별도로 반영`\n",
    "\n",
    "> - RNN 계열은 Cell 내부에서 이전 시점의 데이터들을 기억하도록 반영하여 `계절성`과 같은 특유의 `긴 Sequence로 분포된 데이터 패턴` 학습 가능\n",
    "\n",
    "---\n",
    "\n",
    "**3) 일반적 알고리즘의 한계 인지: `Garbage in, garbage out`**\n",
    "\n",
    "- 대부분 연구들은 `알고리즘 성능에 집중`하고 `어떻게 분석 했는지 없이 그저 딥러닝 결과가 좋았다!`      \n",
    "\n",
    "- `실제 분석에 활용`해보면 오히려 `통계추론이나 기계학습 보다 성능 낮은 경우 다수`\n",
    "\n",
    "- 실제 실리콘밸리의 많은 회사들은 `딥러닝이 아닌 회귀분석으로 수익`을 내고 있다 (Andrew Ng 교수)      \n",
    "\n",
    "> **\"결국 핵심은 `알고리즘이 아니라 데이터!`, `데이터가 Garbage인데 알고리즘이 아무리 좋아봤자 Garbage를 출력`한다\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "428px",
    "left": "23px",
    "top": "110.229px",
    "width": "390.938px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
