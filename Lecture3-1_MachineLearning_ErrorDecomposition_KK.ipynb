{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알고리즘의 발전과정: 오차를 줄여 성능향상 아이디어\n",
    "\n",
    "**1) 회귀분석 vs 분류분석**\n",
    "\n",
    "| | | **Supervised Learning** |  |\n",
    "|:-:|:-:|:-|:-|\n",
    "| | | **Regression** | **Classification** |\n",
    "| **분석목적** |  | 수치 예측 | 클래스 예측 |\n",
    "| **분석단계** | 전처리 | 동일 | 동일 |\n",
    "| | 기초 알고리즘 | Linear Regression | Logistic Regression |\n",
    "| | 특징 | 선형 | 선형 |\n",
    "| | 비용함수 | $Y - \\hat{Y}$ | $-\\hat{Y}log(Pr(\\hat{Y}))$ $-$ $(1-\\hat{Y})log(1-Pr(\\hat{Y}))$ |\n",
    "| | 검증지표 | MSE<br>     MAE<br>     RMSE<br>     MAPE<br>     R^2<br>     t검정<br>     F검정<br>     Log-Likelihood<br>     AIC<br>     BIC | Log-Likelihood<br>     Confusion Matrix<br>     Accuracy<br>     Precision<br>     Recall<br>     F1-score<br>     Classification Report<br>     ROC<br>     AUC |\n",
    "| | 잔차진단 | 정규분포<br>     자기상관<br>     등분산성 | 미실행 |\n",
    "| **확장 알고리즘** | | - Linear regression<br>     - Polynomial regression<br>     - Stepwise regression<br>     - Ridge/Lasso/ElasticNet regression<br>     - Bayesian Linear regression<br>     - Quantile regression<br>     - Decision Tree regression<br>     - Random Forest regression<br>     - Support Vector regression | - Logistic Regression<br>     - Ordinal Regression<br>     - Cox Regression<br>     - Naïve Bayes<br>     - Stochastic Gradient Descent<br>     - K-Nearest Neighbours<br>     - Decision Tree<br>     - Random Forest<br>     - Support Vector Machine |\n",
    "\n",
    "**2) 확장 알고리즘 발전과정:** \n",
    "\n",
    "> 오차(에러)를 줄여 검증지표의 성능을 향상시키기 위한 방향/연구\n",
    "> - Loss Function\n",
    "> - Cost Function\n",
    "> - Object Function\n",
    "\n",
    "<center><img src='Image/Performance_Explanability.png' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차의 구조: 편향(Bias) + 분산(Variance) + 자연에러(Error)\n",
    "\n",
    "> **(수학적 이해)**\n",
    "\n",
    "<center><img src='Image/Bias-Variance_Structure.PNG' width='500'>($F$: 모집단 알고리즘, $\\hat{F}$: 학습한 알고리즘)</center>\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Real Y} && Y &= F(x) + \\epsilon, ~~~ \\epsilon \\sim N(0, \\sigma^2) \\\\\n",
    "\\text{Estimated Y} && &= \\hat{F}(x) \\\\\n",
    "\\text{Equation of Error} && MSE &= E\\Bigl[\\bigl(Y-\\hat{F}(x)\\bigr)^2 \\Bigr] \\\\\n",
    "&& &= E\\Bigl[\\bigl(F(x) + \\epsilon - \\hat{F}(x)\\bigr)^2 \\Bigr] \\\\\n",
    "&& &= E\\Bigl[\\bigl(F(x) - \\hat{F}(x)\\bigr)^2 \\Bigr] + 2 \\bigl(F(x) - \\hat{F}(x)\\bigr) E(\\epsilon) + E(\\epsilon^2) \\\\\n",
    "&& &= E\\Bigl[\\bigl(F(x) - \\hat{F}(x)\\bigr)^2 \\Bigr] + \\sigma^2 \\\\\n",
    "&& &= E\\Bigl[\\bigl(F(x) - E\\bigl[\\hat{F}(x)\\bigr] + E\\bigl[\\hat{F}(x)\\bigr] - \\hat{F}(x)\\bigr)^2 \\Bigr] + \\sigma^2 \\\\\n",
    "&& &= E\\Bigl[\\bigl(F(x) - E\\bigl[\\hat{F}(x)\\bigr] \\bigr)^2 \\Bigr] + E\\Bigl[\\bigl(E\\bigl[\\hat{F}(x)\\bigr] - \\hat{F}(x)\\bigr] \\bigr)^2 \\Bigr] + \\sigma^2 \\\\\n",
    "&& &= E\\Bigl[\\bigl(F(x) - \\bar{F}(x) \\bigr)^2 \\Bigr] + E\\Bigl[\\bigl(\\bar{F}(x) - \\hat{F}(x)\\bigr] \\bigr)^2 \\Bigr] + \\sigma^2 \\\\\n",
    "&& &= \\text{Bias}^2 \\bigl( \\hat{F}(x) \\bigr) + \\text{Variance} \\bigl( \\hat{F}(x) \\bigr) + \\text{Natural(Irreducible) Error}\n",
    "\\end{align*}\n",
    "\n",
    "- **(비수학적 이해)**\n",
    "\n",
    "> **편향(Bias):** 여러번 측정된 정확성의 중심 통계량(점추정)    \n",
    "> - 여러번 점추정을 반복했을 때, 예측값이 실제값에서 얼마나 떨어져있는지 측정\n",
    "> - 데이터 패턴을 잘 반영했는지 측정\n",
    "\n",
    "> **분산(Variance):** 여러번 측정된 정확성의 변동 통계량(구간추정)  \n",
    "> - 여러번 점추정을 반복했을 때, 실제값과 상관없이 예측값이 퍼져있는 정도\n",
    "> - <span style=\"color:red\">**실제값과 관계없이 예측값들의 퍼진 정도만 의미!**</span>\n",
    "> - 데이터 패턴이 과하게 학습되어 다른 데이터나 환경의 적은 변동에도 정확성 성능이 급하게/안정적으로 변하는지 측정\n",
    "    \n",
    "<center><img src='Image/Bias_Variance1.jpeg' width='400'>(붉은색: Target, 파란색: Predicted)</center>\n",
    "<center>\"직관적으로 Bias와 Variance가 모두 작은 경우가 Best\"</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 알고리즘 복잡도와 편향-분산 상충관계(Bias-variance Trade-off) \n",
    "\n",
    "**1) 회귀/분류 문제에서의 Under-fittng vs. Over-fitting:** 알고리즘의 복잡도가 증가할수록 Over-fitting 가능성 증가\n",
    "\n",
    "<center><img src='Image/Underfitting_Overfitting.png' width='600'></center>\n",
    "\n",
    "--- \n",
    "\n",
    "<center><img src='Image/Underfitting_Overfitting.jpg' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**2) 복잡도와 적합력의 관계:** Bias와 Variance가 최소화 되는 수준에서 모델의 복잡도 선택\n",
    "\n",
    "> - **Train(1,1):** Train 데이터가 **최적반영(Fitting)** 상태\n",
    "> - **Train(1,2):** Train 데이터가 **과대반영(Over-fitting)** 상태로 High Variance\n",
    "> - **Train(2,1):** Train 데이터가 **과소반영(Under-fitting)** 상태로 High Bias\n",
    "> - **Train(2,2):** Train 데이터 일부는 **과소반영(Under-fitting)** 이며  일부는 **과대반영(Over-fitting)** 상태로 High Bias + High Variance\n",
    "\n",
    "<center><img src='Image/Bias_Variance1.jpeg' width='400'>(붉은색: Target, 파란색: Predicted)</center>\n",
    "<center>\"직관적으로 Bias와 Variance가 모두 작은 경우가 Best\"</center>\n",
    "\n",
    "> - **Under-fitting: 알고리즘의 복잡도가 낮아 Bias가 증가하고 Variance가 감소**  \n",
    "> - **Over-fitting: 알고리즘의 복잡도가 높아 Bias가 감소하고 Variance가 증가**  \n",
    "> - **Bias-variance Trade-off: 알고리즘 복잡도가 증가할수록 Bias는 감소하나 Variance가 증가**\n",
    "\n",
    "<center><img src='Image/Bias-Variance-Tradeoff.png' width='400'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test와 편향-분산 상충관계(Bias-variance Trade-off) \n",
    "\n",
    "> - **Train(1,1) & Test(1,2):** Train 데이터로 Test 예측이 정확한 편인데 기복이 심함\n",
    ">> Test보다 Train이 **과대반영(Over-fitting)** 되어 Test예측의 Variance 발생\n",
    "> - **Train(1,1) & Test(2,1):** Train과 Test 차이가 너무 커서 Train 성능은 좋은데 Test 성능 낮음\n",
    ">> Train은  **최적반영(Fitting)** 되었으나 Test예측은 **과소반영(Under-fitting)** 되어 Bias 발생\n",
    "> - **Train(1,1) & Test(2,2):** Train 데이터로 Test 예측성능 기복이 심하고 Train과 Test 차이가 너무 커서 Test 추정 의미없음\n",
    ">> Test보다 Train이 **과대반영(Over-fitting)** 되어 Test예측의 Variance 발생하고 Train/Test 차이 때문에 Test **과소반영(Under-fitting)** 으로 Test예측의 Bias 도 발생\n",
    "\n",
    "<center><img src='Image/Bias_Variance1.jpeg' width='400'></center>\n",
    "<center>\"Train & Test 모두 최소 Bias & Variance가 Best\"</center>\n",
    "\n",
    "    \n",
    "**1) 적합력에 따른 편향과 분산의 관계:** Bias & Variance 모두 감소시키는건 어려움\n",
    "\n",
    "> - **Under-fitting:** 알고리즘의 복잡도가 낮아 Bias가 증가하고 Variance가 감소  \n",
    ">> **Training/Testing 모두 추정/예측 성능이 낮음**\n",
    ">> - Training: High Bias + Low Variance\n",
    ">> - Testing: <span style=\"color:red\">High Bias</span> + Low Variance => Cost 증가!\n",
    "> - **Over-fitting:** 알고리즘의 복잡도가 높아 Bias가 감소하고 Variance가 증가  \n",
    ">> **Training만 추정 성능이 높고 Testing 예측 성능은 낮음**\n",
    ">> - Training: Low Bias + Low Variance\n",
    ">> - Testing: Low Bias + <span style=\"color:red\">High Variance</span> => Cost 증가!\n",
    "> - **Bias-variance Trade-off:** 알고리즘 복잡도가 증가할수록 Bias는 감소하나 Variance가 증가\n",
    ">> - 최적의 모델 복잡도: Bias & Variance가 교차하는 부분에서 Cost가 가장 작은 곳\n",
    ">> - 이상적 방향: Train의 B-V Trade-off가 아니라 Train 학습 후 Test의 B-V Trade-off를 최소가 되도록 해야 함\n",
    ">> - 현실적 대안: Train/Test의 차이를 줄이는 것 -> n-fold Cross Validation 배경\n",
    "\n",
    "<center><img src='Image/Bias_Variance4.png' width='400'></center>\n",
    "\n",
    "> - [**실제 예시:**](https://datacadamia.com/data_mining/bias_trade-off)\n",
    ">> - **한문장 정리:** 알고리즘의 복잡도를 높이면 Bias와 Variance는 모두 감소하는데 어느 시점부터는 Test의 Cost가 증가(Test의 Cost 중 Variance가 다시 증가하기 때문)\n",
    ">> - **딥러닝(인공지능 알고리즘):** 딥러닝은 엄청나게 복잡한 모델이며 Bias-variance Trade-off를 피할 수 없음\n",
    ">>> - 딥러닝으로 성능을 내기 위해선 빅데이터가 반드시 필요!\n",
    ">>> - 빅데이터를 통해 Train 데이터와 Test 데이터의 차이를 줄일 수 있고, 스몰데이터의 딥러닝은 Over-fitting 발생 가능\n",
    "\n",
    "<center><img src='Image/Bias_Variance_Example1.jpg' width='700'></center>\n",
    "<center><img src='Image/Bias_Variance_Example2.jpg' width='700'></center>\n",
    "<center><img src='Image/Bias_Variance_Example3.jpg' width='700'></center>\n",
    "\n",
    "**2) 편향과 분산 모두를 최소화하는 방법**\n",
    "> Train 데이터의 정확도 성능이 잘 나오는지 보고, Bias가 적절해지면 Test 데이터에 적용하여 Variance가 적절한지 판단\n",
    ">> - Bias가 높다면 알고리즘을 복잡하게 하거나 최적화를 통해 해결\n",
    ">> - Variance가 높다면 알고리즘 복잡도를 낮추거나 데이터의 갯수(Row) 증가 또는 계수를 제한(Regularization)하면서 해결\n",
    "<center><img src='Image/Bias_Variance_Reduce.png' width='600'></center>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "413.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
