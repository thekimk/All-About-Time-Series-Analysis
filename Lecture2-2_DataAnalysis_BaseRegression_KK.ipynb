{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습종류와 알고리즘(Learning Style and Algorithms)\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/DataSplit_Concept1.png' width='700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기초 알고리즘(Baseline): 회귀분석\n",
    "\n",
    "> **\"$t$개의 값을 가지는 $k$차원 독립변수 $X_i$와 이에 대응하는 \"연속형인 종속변수 $Y$\"와의 관계를 정량적으로 찾는 알고리즘\"**\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/ML_Type_Application_Circle.jpg' width='600'></center>  \n",
    "\n",
    "---\n",
    "\n",
    "| Regression Algorithms | Instance-based Algorithms | Regularization Algorithms | Decision Tree Algorithms | Bayesian Algorithms | Artificial Neural Network Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src='Image/Regression-Algorithms.png' width='150'> | <img src='Image/Instance-based-Algorithms.png' width='150'> | <img src='Image/Regularization-Algorithms.png' width='150'> | <img src='Image/Decision-Tree-Algorithms.png' width='150'> | <img src='Image/Bayesian-Algorithms.png' width='150'> | <img src='Image/Artificial-Neural-Network-Algorithms.png' width='150'> |\n",
    "| Ordinary Least Squares Regression (OLSR) | k-Nearest Neighbor (kNN) | Ridge Regression | Classification and Regression Tree (CART) | Naive Bayes | Perceptron |\n",
    "| Linear Regression | Learning Vector Quantization (LVQ) | Least Absolute Shrinkage and Selection Operator (LASSO) | Iterative Dichotomiser 3 (ID3) | Gaussian Naive Bayes | Back-Propagation |\n",
    "| Logistic Regression | Self-Organizing Map (SOM) | Elastic Net | C4.5 and C5.0 (different versions of a powerful approach) | Multinomial Naive Bayes | Hopfield Network |\n",
    "| Stepwise Regression | Locally Weighted Learning (LWL) | Least-Angle Regression (LARS) | Chi-squared Automatic Interaction Detection (CHAID) | Averaged One-Dependence Estimators (AODE) | Radial Basis Function Network (RBFN) |\n",
    "| Multivariate Adaptive Regression Splines (MARS) | - | - | Decision Stump | Bayesian Belief Network (BBN) | - |\n",
    "| Locally Estimated Scatterplot Smoothing (LOESS) | - | - | M5 | Bayesian Network (BN) | - |\n",
    "| - | - | - | Conditional Decision Trees | - | - |\n",
    "\n",
    "- **Target Algorithm:**\n",
    "> - (Simple/Multiple/Multivariate) Linear regression\n",
    "> - Polynomial regression\n",
    "> - Stepwise regression\n",
    "> - Ridge regression\n",
    "> - Lasso regression\n",
    "> - ElasticNet regression\n",
    "> - Bayesian Linear Regression\n",
    "> - Quantile Regression\n",
    "> - Decision Tree Regression\n",
    "> - Random Forest Regression\n",
    "> - Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statsmodels 예시\n",
    "\n",
    "> - 대기중 CO2농도 데이터:\n",
    "> ```python\n",
    "> import statsmodels.api as sm\n",
    "> data = sm.datasets.get_rdataset(\"CO2\", package=\"datasets\")\n",
    "> ```\n",
    "> - 황체형성 호르몬(Luteinizing Hormone)의 수치 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"lh\")\n",
    "> ```\n",
    "> - 1974~1979년 사이의 영국의 호흡기 질환 사망자 수 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"deaths\", \"MASS\")\n",
    "> ```\n",
    "> - 1949~1960년 사이의 국제 항공 운송인원 데이터:\n",
    "> ```\n",
    "> data = sm.datasets.get_rdataset(\"AirPassengers\")\n",
    "> ```\n",
    "> - 미국의 강수량 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"precip\")\n",
    "> ```\n",
    "> - 타이타닉호의 탑승자들에 대한 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"Titanic\", package=\"datasets\")\n",
    "> ```\n",
    "\n",
    "- **Output:**\n",
    "    - package: 데이터를 제공하는 R 패키지 이름\n",
    "    - title: 데이터 이름\n",
    "    - data: 데이터를 담고 있는 데이터프레임\n",
    "    - __doc__: 데이터에 대한 설명 문자열(R 패키지의 내용 기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn 예시\n",
    "\n",
    "**1) 패키지에 포함된 소량의 데이터(load 계열 명령)**\n",
    "    \n",
    "> - load_boston: 회귀 분석용 보스턴 집값\n",
    "> ```python\n",
    "> from sklearn.datasets import load_boston\n",
    "> raw = load_boston()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - load_diabetes: 회귀 분석용 당뇨병 자료\n",
    "> - load_linnerud: 회귀 분석용 linnerud 자료\n",
    "> - load_iris: 분류용 붓꽃(iris) 자료\n",
    "> - load_digits: 분류용 숫자(digit) 필기 이미지 자료\n",
    "> - load_wine: 분류용 포도주(wine) 등급 자료\n",
    "> - load_breast_cancer: 분류용 유방암(breast cancer) 진단 자료\n",
    "\n",
    "**2) 인터넷에서 다운로드할 수 있는 대량의 데이터(fetch 계열 명령)**\n",
    "\n",
    "> - fetch_california_housing: : 회귀분석용 캘리포니아 집값 자료\n",
    "> ```python\n",
    "> from sklearn.datasets import fetch_california_housing\n",
    "> raw = fetch_california_housing()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - fetch_covtype : 회귀분석용 토지 조사 자료\n",
    "> - fetch_20newsgroups : 뉴스 그룹 텍스트 자료\n",
    "> - fetch_olivetti_faces : 얼굴 이미지 자료\n",
    "> - fetch_lfw_people : 유명인 얼굴 이미지 자료\n",
    "> - fetch_lfw_pairs : 유명인 얼굴 이미지 자료\n",
    "> - fetch_rcv1 : 로이터 뉴스 말뭉치\n",
    "> - fetch_kddcup99 : Kddcup 99 Tcp dump 자료\n",
    "\n",
    "**3) 확률분포를 사용한 가상의 데이터(make 계열 명령)**\n",
    "\n",
    "> - make_regression: 회귀 분석용 가상 데이터 생성\n",
    "> ```python\n",
    "> from sklearn.datasets import make_regression\n",
    "> X, y, c = make_regression(n_samples=100, n_features=10, n_targets=1, bias=0, noise=0, coef=True, random_state=0)\n",
    "> ```\n",
    "> - make_classification: 분류용 가상 데이터 생성\n",
    "> - make_blobs: 클러스터링용 가상 데이터 생성\n",
    "\n",
    "- **Output:** Bunch 라는 클래스 객체 형식으로 생성\n",
    "    - data: (필수) 독립 변수 ndarray 배열\n",
    "    - target: (필수) 종속 변수 ndarray 배열\n",
    "    - feature_names: (옵션) 독립 변수 이름 리스트\n",
    "    - target_names: (옵션) 종속 변수 이름 리스트\n",
    "    - DESCR: (옵션) 자료에 대한 설명\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제별 예시\n",
    "\n",
    "<center><img src='Image/ML_Type_Application_Circle.jpg' width='600'></center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:57:10.696384Z",
     "start_time": "2021-04-19T14:57:09.485578Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n",
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n",
      "(506, 13) (506,)\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "from sklearn.datasets import load_boston\n",
    "raw = load_boston()\n",
    "print(raw.DESCR)\n",
    "print(raw.keys())\n",
    "print(raw.data.shape, raw.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:57:10.711316Z",
     "start_time": "2021-04-19T14:57:10.697354Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "raw = load_breast_cancer()\n",
    "print(raw.DESCR)\n",
    "print(raw.keys())\n",
    "print(raw.data.shape, raw.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T14:57:10.726302Z",
     "start_time": "2021-04-19T14:57:10.712356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Clustering\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=100, n_features=10, random_state=123)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 방향(Preprocessing)\n",
    "\n",
    "- **목표:** \n",
    "> - 대량으로 수집된 데이터는 그대로 활용 어려움\n",
    "> - 잘못 수집/처리 된 데이터는 엉뚱한 결과를 발생\n",
    "> - 알고리즘이 학습이 가능한 형태로 데이터를 정리\n",
    "<center><img src='Image/DataAnalysis_Time.jpg' width='500'></center> \n",
    "---\n",
    "\n",
    "> **세부항목:**  \n",
    "> - 데이터 결합\n",
    "> - 결측값 처리\n",
    "> - 이상치 처리\n",
    "> - 자료형 변환\n",
    "> - 데이터 분리\n",
    "> - 데이터 변환\n",
    "> - 스케일 조정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 결합(Data Integration)\n",
    "\n",
    "- **목표:** 여러개로 구분된 데이터 이거나 빅데이터로 확장 할 경우 결합\n",
    "\n",
    "> - 중복 데이터 제거\n",
    "> - 의미는 같으나 단위나 이름의 표현이 다른 경우 일치 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자료형 변환(Type)\n",
    "\n",
    "- **목표:** 각 변수의 특성을 확인하고 범주형과 연속형에 맞도록 변경\n",
    "\n",
    "<center><img src='Image/Basic_DataType.PNG' width='1000'></center>\n",
    "\n",
    "- **사람의 데이터 분류:**\n",
    "\n",
    "> - **질적변수 vs 양적변수**: 데이터의 특성에 따른 분류\n",
    ">> - 질적변수(Qualitative Variable): 변수의 값이 비수치적 특정 카테고리에 포함 시키도록 하는 변수 (ex.색상, 성별, 종교)\n",
    ">>> - 명목변수(Nominal Variable): 변수의 값이 특정한 범주(Category)에 들어가지만 해당 범주간 순위는 존재하지 않는 것 (ex.혈액형)\n",
    ">>> - 순위변수(Ordinal Variable): 변수의 값이 특정 범주에 들어가면서 변수의 값이 순위를 가지는 경우 (ex.성적)\n",
    ">> - 양적변수(Quantitative Variable): 변수의 값을 숫자로 나타 낼 수 있는 변수 (ex. 키, 몸무게, 소득)\n",
    ">>> - 이산변수(Discrete Variable): 하나하나 셀 수 있는 변수 (ex.정수)\n",
    ">>> - 연속변수(Continuous Variable): 이산변수와 다르게 변수의 값 사이에 무수히 많은 또 다른 값들이 존재하는 경우 (ex.실수)\n",
    "\n",
    "- **컴퓨터의 데이터 분류:**\n",
    "\n",
    "> - **문자형 vs 숫자형**:\n",
    ">> - 문자형: 명목변수\n",
    ">> - 숫자형: 순위변수, 이산변수, 연속변수\n",
    "> - **범주형 vs 연속형**:\n",
    ">> - 범주형: 명목변수, 순위변수\n",
    ">> - 연속형(숫자형): 이산변수, 연속변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측값 처리(Missing Value)\n",
    "\n",
    "- **목표:** 결측값은 알고리즘 작동을 어렵게 하고, 작동이 되어도 해석의 왜곡 가능성\n",
    "\n",
    "> - **삭제:** 결측값이 발생한 모든 변수(Column)를 삭제하거나 일부(Row)를 삭제\n",
    "> - **대체:** 결측값을 제외한 값들의 통계량으로 결측값을 대체\n",
    ">> - 중심 통계량\n",
    ">> - 분포 기반 랜점 추출\n",
    "> - **예측:** 별도 분석을 통해 결측값을 예측하여 삽입\n",
    ">> - Regression Imputation\n",
    ">> - EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이상치 처리(Outlier)\n",
    "\n",
    "- **목표:** 일반적인 데이터와 동떨어진 관측치로 분석 결과를 왜곡할 가능성\n",
    "\n",
    "> - **검출:**\n",
    ">> - 전통적으로 분포(Boxplot / Histogram / Scatter Plot 등)에서 중심에서 벗어난 값을 지칭\n",
    ">> - 빅데이터 시대에서는 이상치의 존재 유무? 논란이 존재\n",
    "> - **처리:**\n",
    ">> - **삭제:** Human Error 등은 보통 삭제 처리\n",
    ">> - **대체:** 스몰데이터의 경우 삭제시 데이터의 양이 적어지기에 다른 값으로 대체\n",
    ">> - **예측:** 별도 분석을 통해 이상치 대신 예측값으로 반영\n",
    ">> - **변수화:** 이상치를 변수화 하여 유의성을 판별\n",
    ">> - **별도 분석:** 이상치 포함 분석과 이상치 미포함 분석을 병행 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T06:45:06.962299Z",
     "start_time": "2019-06-15T06:45:06.947692Z"
    }
   },
   "source": [
    "### 데이터 분리(Data Split)\n",
    "\n",
    "> **배경:**\n",
    "> - 독립변수 종속변수 구분\n",
    ">> - 독립변수(Independent Variable): 다른 변수에 영향을 미치는 변수\n",
    ">> - 종속변수(Dependent Variable): 다른 변수에 영향을 미치지 못하고 다른 변수의 영향을 받는 변수\n",
    "> - 과거과 현재의 상황을 분석하고, 미래를 예측분석 할 수 있는 환경 마련\n",
    "\n",
    "<center><img src='Image/DataSplit_Concept1.png' width='700'></center>\n",
    "<center><img src='Image/DataSplit_Concept2.png' width='700'></center>\n",
    "\n",
    "> **일반적준비(Simple Validation):**\n",
    "- **훈련셋(Training set):** 이름처럼 일반적으로 전체 데이터의 60%를 사용하여 기계학습을 하는데 사용됨  \n",
    "- **검증셋(Validation set):** \n",
    "    - 개발셋이라고도 하며, 일반적으로 전체 데이터의 20%를 사용함\n",
    "    - 훈련된 여러가지 모델들의 성능을 테스트 하는데 사용되며 모델 선택의 기준이 됨\n",
    "- **테스트셋(Testing set):** 전체 데이터의 20%를 사용하며 최종 모델의 정확성을 확인하는 목적에 사용됨\n",
    "\n",
    "<center><img src='Image/DataSplit_Simple.png' width='500'></center>\n",
    "\n",
    "> **$K$교차검사($K$-fold Cross Validation):**  \n",
    "1. 훈련셋을 복원없이 $K$개로 분리한 후, $K-1$는 하위훈련셋으로 나머지 1개는 검증셋으로 사용함  \n",
    "2. 검증셋과 하위훈련셋을 번갈아가면서 $K$번 반복하여 각 모델별로 $K$개의 성능 추정치를 계산  \n",
    "3. $K$개의 성능 추정치 평균을 최종 모델 성능 기준으로 사용  \n",
    "\n",
    "<center><img src='Image/DataSplit_Kfold.png' width='500'></center>\n",
    "\n",
    "> **간단한준비(Holdout Validation):**\n",
    "- **훈련셋(Training set):** 일반적으로 전체 데이터의 70% 사용 \n",
    "- **테스트셋(Testing set):** 일반적으로 전체 데이터의 30% 사용\n",
    "\n",
    "> **$K$-fold vs. Random-subsamples vs. Leave-one-out vs. Leave-$p$-out**  \n",
    ">- **$K$-fold**\n",
    "<center><img src='Image/DataSplit_ver1.png' width='500'></center>\n",
    "\n",
    ">- **Random-subsamples**\n",
    "<center><img src='Image/DataSplit_ver2.png' width='500'></center>\n",
    "\n",
    ">- **Leave-one-out**\n",
    "<center><img src='Image/DataSplit_ver3.png' width='500'></center>\n",
    "\n",
    ">- **Leave-$p$-out**\n",
    "<center><img src='Image/DataSplit_ver4.png' width='500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 변환(Categorical Features)\n",
    "\n",
    "- **목표:** 범주형과 연속형으로 구분한 후 범주형 데이터가 알고리즘이 이해하도록 숫자 형태로 변환\n",
    "\n",
    "> - **문자형 vs 숫자형**: 문자를 숫자로 바꾸어 컴퓨터가 이해가능하도록 변환\n",
    ">> - 문자형: 명목변수\n",
    ">> - 숫자형: 순위변수, 이산변수, 연속변수\n",
    "> - **범주형 vs 연속형**: 범주형을 컴퓨터가 이해가능하도록 변환\n",
    ">> - 연속형(숫자형): 이산변수, 연속변수\n",
    ">>> - 연속형 변수들은 대부분 알고리즘에서 자동으로 처리됨\n",
    ">> - 범주형: 명목변수, 순위변수\n",
    ">>> - <U>기계학습(Machine Learning)은 범주형 데이터를 처리하는데서 출발\n",
    ">>> - 경우에 따라선 연속형 변수를 범주형으로 변환 필요\n",
    ">>> - 범주형 데이터를 알고리즘이 처리하도록 방법 필요</U>\n",
    "\n",
    "**1) Binning(구간화):** 연속형 변수를 범주형 변수로 변환\n",
    "\n",
    "- 숫자로 구성된 연속형 값이 넓을 경우 그룹을 지어 이해도를 높임\n",
    "- 변수의 선형적 특성 이외에 비선형적 특성을 반영\n",
    "\n",
    "\n",
    "**2) Label Encoding:** 범주형 변수의 값들을 숫자 값(레이블)로 변경\n",
    "\n",
    "<center><img src='Image/Label_Encoding.png' width='250'></center>\n",
    "\n",
    "**3) Dummy Variable(가변수, $D_i$)**: 범주형 변수를 0 또는 1값을 가진 하나 이상의 새로운 변수로 변경(One-hot Encoding)\n",
    "\n",
    "- **생성법:** \"계절변수\"가 봄/여름/가을/겨울 이라는 값을 포함하는 경우, \"계절_봄\", \"계절_여름\", \"계절_가을\", \"계절_겨울\" 총 4개의 변수를 생성\n",
    "\n",
    "> 1. 범주형 변수의 독립 값을 확인 (봄/여름/가을/겨울)\n",
    "> 2. 독립 값의 갯수만큼 더미변수를 생성 ($D_1$ = 봄, $D_2$ = 여름, $D_3$ = 가을, $D_3$ = 겨울) \n",
    ">> *더미변수의 갯수는 최대 1개까지 줄일 수 있음*\n",
    "> 3. 각 더미변수들의 값은 변수의 정의와 같으면 1이고 나머지는 0으로 채움   \n",
    "\n",
    "<center><img src='Image/Dummy_Engineering.png' width='500'></center>\n",
    "\n",
    "<!-- <center><img src='Image/Dummy-variable-regression.jpg' width='400'></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스케일 조정(Scaling)\n",
    "\n",
    "- **목적:** 변수들의 크기를 일정하게 맞추어 \"크기\" 때문에 \"영향\"이 높은 현상을 회피\n",
    "> - **수학적:** 독립 변수의 공분산 행렬 조건수(Condition Number)를 감소시켜 최적화 안정성 및 수렴 속도 향상 \n",
    "> - **컴퓨터적:** PC 메모리를 고려하여 오버플로우(Overflow)나 언더플로우(Underflow)를 줄여줌 \n",
    "    \n",
    "\n",
    "**1) Standard Scaler:** <center>$\\dfrac{X_{it} - E(X_i)}{SD(X_i)}$</center>\n",
    "> 기본 스케일로 평균을 제외하고 표준편차를 나누어 변환  \n",
    "> 각 변수(Feature)가 정규분포를 따른다는 가정이기에 정규분포가 아닐 시 최선이 아닐 수 있음  \n",
    "> - 사용:\n",
    "> ```python\n",
    "> sklearn.preprocessing.StandardScaler().fit()\n",
    "> sklearn.preprocessing.StandardScaler().transform()\n",
    "> sklearn.preprocessing.StandardScaler().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Scaling_StandardScaler.png' width='500'></center>\n",
    "\n",
    "**2) Min-Max Scaler:** <center>$\\dfrac{X_{it} - min(X_i)}{max(X_i) - min(X_i)}$</center>\n",
    "> 가장 많이 활용되는 알고리즘으로 최소\\~최대 값이 0\\~1 또는 -1\\~1 사이의 값으로 변환  \n",
    "> 각 변수(Feature)가 정규분포가 아니거나 표준편차가 매우 작을 때 효과적 \n",
    "> - 사용:\n",
    "> ```python\n",
    "> sklearn.preprocessing.MinMaxScaler().fit()\n",
    "> sklearn.preprocessing.MinMaxScaler().transform()\n",
    "> sklearn.preprocessing.MinMaxScaler().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Scaling_MinMaxScaler.png' width='500'></center>\n",
    "\n",
    "**3) Robust Scaler:** <center>$\\dfrac{X_{it} - Q_1(X_i)}{Q_3(X_i) - Q_1(X_i)}$</center>\n",
    "> 최소-최대 스케일러와 유사하지만 최소/최대 대신에 IQR(Interquartile Range) 중 25%값/75%값을 사용하여 변환  \n",
    "> 이상치(Outlier)에 영향을 최소화하였기에 이상치가 있는 데이터에 효과적이고 적은 데이터에도 효과적인 편  \n",
    "> - 사용:\n",
    "> ```python\n",
    "> sklearn.preprocessing.RobustScaler().fit()\n",
    "> sklearn.preprocessing.RobustScaler().transform()\n",
    "> sklearn.preprocessing.RobustScaler().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Scaling_RobustScaler.png' width='500'></center>\n",
    "\n",
    "**4) Normalizer:** <center>$\\dfrac{X_{it}}{\\sqrt{X_{i}^2 + X_{j}^2 + ... + X_{k}^2}}$</center>\n",
    "> 각 변수(Feature)를 전체 $n$개 모든 변수들의 크기들로 나누어서 변환(by Cartesian Coordinates)  \n",
    "> 각 변수들의 값은 원점으로부터 반지름 1만큼 떨어진 범위 내로 변환  \n",
    "> - 사용:\n",
    "> ```python\n",
    "> sklearn.preprocessing.Normalizer().fit()\n",
    "> sklearn.preprocessing.Normalizer().transform()\n",
    "> sklearn.preprocessing.Normalizer().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Scaling_Normalizer.png' width='500'></center>\n",
    "\n",
    "- [**비교 예시**](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-download-auto-examples-preprocessing-plot-all-scaling-py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고\n",
    "\n",
    "- **데이터 과학자들은 보통 수동/자동 변수 처리 및 변환(Feature Engineering)에 익숙하지만, 새로운 변수를 생성하는 것은 분석에서 가장 중요하고 시간이 많이 걸리는 작업 중 하나**\n",
    "> **\"변수 생성시 주의할 점!\"**  \n",
    "> 1) 미래의 실제 종속변수 예측값이 어떤 독립/종속변수의 Feature Engineering에 의해 효과가 있을지 단정할 수 없음  \n",
    "> 2) 독립변수의 예측값을 Feature Engineering를 통해 생성될 수 있지만 이는 종속변수의 예측에 오류증가를 야기할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수세팅 및 추정 방향(Modeling)\n",
    "\n",
    "- **회귀분석(Regression):** 연속형 종속변수 Y의 값을 추론\n",
    "\n",
    "<center>\n",
    "$Y \\approx \\hat{Y} = f(X_1, X_2, ..., X_k) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_kX_k \\\\\n",
    "= [\\beta_0~\\beta_1~\\beta_2~\\cdots~\\beta_k]\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix}\n",
    "= [1~X_1~X_2~\\cdots~X_k]\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix}\n",
    "= \\begin{bmatrix} 1~X_{11}~X_{21}~\\cdots~X_{k1} \\\\ 1~X_{12}~X_{22}~\\cdots~X_{k2} \\\\ \\vdots \\\\ 1~X_{1t}~X_{2t}~\\cdots~X_{kt} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} = X\\beta$\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Example_LinearRegression.png' width='600'></center>\n",
    "\n",
    "- **비용함수(Cost Function):** 실제 데이터에 가장 근접한 결과를 추론?\n",
    "\n",
    "> - 나의 주장 기반 알고리즘의 예측값($\\hat{Y}$)과 실제 데이터($Y$)의 차이를 평가하는 함수\n",
    ">> - 손실함수(Loss Function): 하나의 데이터(Single Row)로 알고리즘의 예측값과 정답의 차이를 평가\n",
    ">> - 비용함수(Cost Function): 모든 데이터로 알고리즘의 예측값과 정답의 차이를 평가\n",
    "<center>$\\text{cost} = Y - \\hat{Y} = Y - X\\beta$</center> \n",
    "<center>$~~~~~~~~~~~~~~~~~ = \\sum_{i=1}^{m} \\left[ \\sum_{j=1}^{k} (Y_{i} - X_{j}\\beta_{j}) \\right]$</center> \n",
    "\n",
    "> - 비용함수를 최소로 하는 계수(coefficient)를 추정\n",
    "<center>$\\hat{\\beta} = \\underset{\\beta}{\\arg\\min} \\sum_{i=1}^{m} \\left[\\sum_{j=1}^{k} (Y_{i} - X_{j}\\beta_{j}) \\right]$</center> \n",
    "\n",
    "\n",
    "<!-- - **비선형변수 효과:** 로그 또는 제곱근 등의 변환된 변수 사용시 회귀분석 성능 향상 가능\n",
    "    - 독립 변수나 종속 변수가 심하게 한쪽으로 치우친 분포를 보이는 경우\n",
    "    - 독립 변수와 종속 변수간의 관계가 곱셈 혹은 나눗셉으로 연결된 경우\n",
    "    - 종속 변수와 예측치가 비선형 관계를 보이는 경우 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정론적 모형(Deterministic Model): 수리적 모형\n",
    "> **\"잔차제곱합(Residual Sum of Squares)을 최소로하는 $\\beta$를 추정\"**  \n",
    "\n",
    "**1) 잔차벡터(Residual Vector):** \n",
    "<center>$\\epsilon = Y - \\hat{Y} = Y - X\\beta$</center> \n",
    "\n",
    "**2) 잔차제곱합(Residual Sum of Squares):** \n",
    "<center>$RSS = \\epsilon^T\\epsilon = (Y - X\\beta)^T(Y - X\\beta)$</center>\n",
    "<center>$= Y^TY-2Y^TX\\beta+\\beta^TX^TX\\beta$</center> \n",
    "\n",
    "**3) 잔차제곱합의 그레디언트(Gradient):** \n",
    "<center>$\\dfrac{dRSS}{d\\beta} = -2X^TY + 2X^TX\\beta$</center> \n",
    "\n",
    "**4) 잔차가 최소가 되는 최적화 조건은 최저점에서의 그레디언트(미분,기울기)이 0이 되어야 함:** \n",
    "<center>$\\dfrac{dRSS}{d\\beta} = 0$</center> \n",
    "\n",
    "**5) 최적화를 위한 잔차제곱합의 그레디언트(Gradient):** \n",
    "<center>$\\dfrac{dRSS}{d\\beta} = -2X^TY + 2X^TX\\beta = 0$</center>\n",
    "<center>$X^TX\\beta = X^TY$</center> \n",
    "\n",
    "**6) 추정된 계수:** \n",
    "<center>$\\beta = (X^TX)^{-1}X^TY$</center>    \n",
    "\n",
    "- **Summary:**\n",
    "    - $X^TX$ 행렬이 역행렬이 존재해야 해 추정/존재 가능  \n",
    "    - 역행렬이 미존재  \n",
    "    = $X$가 서로 독립이 아님  \n",
    "    = $X$가 Full Rank가 아님  \n",
    "    = $X^TX$가 양의 정부호(Positive Definite)가 아님\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률론적 모형(Probabilistic Model): 통계적 모형\n",
    "> **\"종속변수의 발생가능성을 최대(최소)로하는 $\\beta$를 추정\"**\n",
    "- **필요성:** 결정론적 선형 회귀모형(OLS)는 데이터의 확률론적 가정이 없기에 단하나의 가중치(점추정)를 계산하나, 이 가중치의 신뢰도(구간추정)는 확인할 수 없음\n",
    "<center><img src='Image/Example_Interval_Estimation.png' width='500'></center>\n",
    "\n",
    "<!-- - **예시:** 집값에 대한 범죄율 영향력(가중치)이 -0.108이라면, 집값은 범죄율에 반비례한다 결론 내릴 수 있을까?\n",
    "    - -0.108는 오로지 우리가 보유한 테스트 1회성 결과일 뿐 오차가 존재가능\n",
    "    - 만약 오차가 0.0001이라면 실제 가중치 신뢰구간은 -0.108$\\pm$0.0001 (-0.1081 ~ -0.1079)이기에 집값과 범죄율 반비례 결론 가능\n",
    "    - 만약 오차가 0.2라면 실제 가중치는 (-0.308 ~ 0.092)이기에 가중치는 0이나 양수도 가능 -> 집값과 범죄율은 정비례도 가능 -->\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Main Equation} && Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) \\\\\n",
    "&& &= \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_kX_k \\\\\n",
    "&& &= E(Y|X_1, X_2, ... , X_k) \\\\ \n",
    "&& &\\sim \\mathcal{N}(X \\beta, \\sigma^2) \\\\\n",
    "&& Pr(Y \\mid X, \\theta) &= \\mathcal{N}(y \\mid X \\beta, \\sigma^2 ) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "\\text{Error Poperties} && p(\\epsilon \\mid \\theta) &= \\mathcal{N}(0, \\sigma^2 ) \\text{  from  } \\epsilon = Y - X \\beta \\\\\n",
    "&& \\text{E}(\\epsilon \\mid X) &= 0 \\\\\n",
    "&& \\text{E}(\\epsilon) &= \\text{E}(\\text{E}(\\epsilon \\mid X)) = 0 \\\\\n",
    "&& \\text{E}(\\epsilon X) &= \\text{E}(\\text{E}(\\epsilon X \\mid X)) = \\text{E}(X \\text{E}(\\epsilon\\mid X)) = 0 \\\\\n",
    "&& \\text{E}(\\epsilon^2) &= \\sigma^2 (N-K) \\\\\n",
    "&& \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) &= 0 \\;\\; (i,j=1,2,\\ldots,N)\n",
    "\\end{align*}\n",
    "\n",
    "- **Summary:**\n",
    "    - $X, Y$ 중 어느 것도 정규분포일 필요는 없음  \n",
    "    - $Y$는 $X$에 대해 조건부로 정규분포를 따르며 $Y$자체가 무조건부로 정규분포일 필요는 없음  \n",
    "    - 잔차의 기대값은 0  \n",
    "    - 잔차의 조건부 기대값은 0  \n",
    "    - 잔차와 독립변수 $X$는 상관관계 없음  \n",
    "    - $X$와 무관하게 잔차들간의 공분산은 0   -->\n",
    "    \n",
    "**1) Y의 발생가능성(Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "Pr(Y_{i} \\,\\big|\\, X_{i}, \\theta) &= \\prod_{i=1}^N \\mathcal{N}(Y_i \\,\\big|\\, X_i \\beta_i , \\sigma^2) \\\\\n",
    "&= \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(Y_i- X_i \\beta_i)^2}{2\\sigma^2} \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "**2) 더하기 사용을 위한 Log 변환(Log-Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{LL} &= \\log Pr(Y_{i} \\,\\big|\\, X_{i}, \\theta) \\\\\n",
    "&= \\log \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(Y_i-X_i \\beta_i)^2}{2\\sigma^2} \\right\\}  \\\\\n",
    "&= -\\dfrac{1}{2\\sigma^2} \\sum_{i=1}^N (Y_i-X_i \\beta_i)^2 - \\dfrac{N}{2} \\log{2\\pi}{\\sigma^2}  \\\\\n",
    "\\text{LL(Matrix Form)} &= -C_1 (Y - X\\beta)^T(y-X\\beta) - C_0 \\\\\n",
    "&= -C_1(\\beta^TX^TX\\beta -2 Y^TX\\beta + Y^TY) - C_0 \\\\\n",
    "& \\text{where } C_1=  -\\dfrac{1}{2\\sigma^2}, C_0 =  \\dfrac{N}{2} \\log{2\\pi}{\\sigma^2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**3) Log-Likelihood의 그레디언트(미분,기울기)는 0이 되어야 함:** \n",
    "\n",
    "\\begin{align*}\n",
    "\\dfrac{d}{d\\beta} \\text{LL} &= -C_1 \\left( 2X^TX \\hat{\\beta} - 2X^TY \\right) = 0 \\\\\n",
    "\\hat{\\beta} &= (X^TX)^{-1}X^T Y \\\\\n",
    "\\end{align*}\n",
    "\n",
    "- **Summary:**\n",
    "    - $X^TX$ 행렬이 역행렬이 존재해야 해 추정/존재 가능  \n",
    "    - 역행렬이 미존재  \n",
    "    = $X$가 서로 독립이 아님  \n",
    "    = $X$가 Full Rank가 아님  \n",
    "    = $X^TX$가 양의 정부호(Positive Definite)가 아님\n",
    "    \n",
    "\n",
    "- **회귀계수의 분포:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Main Equation} && \\hat{\\beta} &= (X^TX)^{-1} X^T Y \\\\\n",
    "&& &= (X^TX)^{-1} X^T (X \\beta + \\epsilon) \\\\\n",
    "&& &= \\beta + (X^TX)^{-1} X^T \\epsilon \\\\\n",
    "\\text{Expectation} && \\text{E}(\\hat{\\beta}) &=  \\text{E}( \\beta + (X^TX)^{-1} X^T \\epsilon ) \\\\\n",
    "&& &=  \\beta + (X^TX)^{-1} X^T \\text{E}( \\epsilon ) \\\\\n",
    "&& &= \\beta \\\\\n",
    "\\text{Variance} && \\text{Var}(\\hat{\\beta}_i)  &= \\left( \\text{Cov}(\\hat{\\beta}) \\right)_{ii} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Covariance} && \\text{Cov}(\\hat{\\beta}) &= E\\left((\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T \\right) \\\\\n",
    "&& &= E\\left(((X^TX)^{-1} X^T \\epsilon)((X^TX)^{-1} X^T \\epsilon)^T \\right) \\\\\n",
    "&& &= E\\left((X^TX)^{-1} X^T \\epsilon \\epsilon^T X(X^TX)^{−1} \\right) \\\\\n",
    "&& &= (X^TX)^{-1} X^T E(\\epsilon \\epsilon^T) X(X^TX)^{−1} \\\\\n",
    "&& &= (X^TX)^{-1} X^T (\\sigma^2 I) X(X^TX)^{−1} \\\\\n",
    "&& &= \\sigma^2  (X^TX)^{-1} \\\\\n",
    "\\text{Standard Deviation} && \\sqrt{\\text{Var}(\\hat{\\beta}_i)} \\approx {se_{\\hat{\\beta}_i}} &= \\sqrt{\\sigma^2 \\big((X^TX)^{-1}\\big)_{ii}} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Asymptotic} && \\dfrac{\\hat{\\beta}_i - \\beta_i}{se_{\\hat{\\beta}_i}} &\\sim t_{N-K} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증지표 방향(Evaluation Metrics)\n",
    "\n",
    "- **\"문제해결 지표와 알고리즘 지표는 같을 수 있으나 대부분은 다른 편\"**\n",
    "> **1) 문제해결 검증지표: 실제 문제를 잘 해결하는지 평가**   \n",
    "> **2) 알고리즘 검증지표: 데이터의 패턴이 잘 추출되고 예측의 정확성을 평가\"**   \n",
    ">> - 알고리즘 성능이 좋은것과 문제해결이 가능한 것은 다르기 때문에 문제해결 지표와 알고리즘 지표는 같을 수 있으나 대부분은 다른 편\n",
    ">> - 알고리즘 검증지표는 없어도 되지만 문제해결 검증지표는 반드시 필요    \n",
    ">> - (이론적)알고리즘들은 일반적으로 특정 \"알고리즘 검증지표\"를 향상시키는 방향으로 개발됨\n",
    "<center><img src='Image/Analysis_Process.png' width='800'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대표적인 검증지표\n",
    "\n",
    "<center><img src='Image/DataSplit_Concept1.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) 분류별 종류**\n",
    "\n",
    "<center><img src='Image/Evaluation_Metric_Types.png' width='600'></center>\n",
    "\n",
    "> - **Statistical Metrics:** Correlation\n",
    ">> - 입력(Input): -무한대 ~ 무한대 범위의 연속형 값\n",
    ">> - 출력(Output): 이론적으론 -1 ~ 1 범위의 연속형 값\n",
    "> - **Regression Metrics:** MSE, MSPE, RMSE, RMSLE, MAE, MAPE, MPE, R^2, Adjusted R^&2, ... (Y의 범위가 무한대가 가능한 연속형일때)\n",
    ">> - 입력(Input): -무한대 ~ 무한대 범위의 연속형 값\n",
    ">> - 출력(Output): 이론적으론 0 ~ 무한대 범위의 연속형 값\n",
    "> - **Classification Metrics:** Log Loss, Cross-entropy, ROC, AUC, Gini, Confusion Matrix, Accuracy, Precision, Recall, F1-score, Classification Report, KS Statistic, Concordant-Discordant Ratio ... (Y가 2개 또는 그 이상개수의 이산형일때)\n",
    ">> - 입력(Input): -무한대 ~ 무한대 범위의 연속형 값\n",
    ">> - 출력(Output): 알고리즘 종류에 따라 출력이 달라질 수 있음\n",
    ">>> - 확률(Probability): 0 ~ 1 범위의 연속형 값 (Logistic Regression, Random Forest, Gradient Boosting, Adaboost, ...)\n",
    ">>> - 집단(Class): 0 또는 1의 이산형 값 (SVM, KNN, ...)\n",
    "> - **Ranking Metrics:** Gain, Lift, MRR, DCG, NDCG, ...\n",
    "> - **Computer Vision Metrics:** PSNR, SSIM, IoU, ...\n",
    "> - **NLP Metrics:** Perplexity, BLEU score, ...\n",
    "> - **Deep Learning Related Metrics:** Inception score, Frechet Inception distance, ...\n",
    "> - **Real Problem:** ???\n",
    "\n",
    "**2) Regression Metrics:** MSE, MSPE, RMSE, RMSLE, MAE, MAPE, MPE, R^2, Adjusted R^&2, ... (Y의 범위가 무한대가 가능한 연속형일때)\n",
    "\n",
    "<center><img src='Image/Evaluation_Metric1.jpg' width='300'></center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric2.jpg' width='300'>(Mean Absolute Error)</center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric3.jpg' width='300'>(Mean Squared Error)</center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric4.jpg' width='300'>(Mean Absolute Percentage Error)</center>  \n",
    "    \n",
    "<center><img src='Image/Evaluation_Metric5.jpg' width='250'>(Mean Percentage Error)</center>\n",
    "\n",
    "- **사용예시:** [Comparison of Algorithm Performance Metrics](https://pkg.robjhyndman.com/forecast/reference/accuracy.html)\n",
    "\n",
    "**3) 검증지표 성능의 종류:** 데이터/분석은 높은 정확도를 낳거나 높은 에러를 발생시킴\n",
    "> - **높은정확도(High Accuracy):** 과거 패턴이 미래에도 그대로 유지가 된다면 예측 정확도가 높아짐  \n",
    "> - **높은에러(High Error):** 패턴이 점차적으로 또는 갑자기 변경되면 예측값은 실제값에서 크게 벗어날 수 있음  \n",
    ">> - **Black Swan:** <U>일어날 것 같지 않은 일이 일어나는 현상</U>\n",
    ">> - **White Swan:** <U>과거 경험들로 충분히 예상되는 위기지만 대응책이 없고 반복될 현상</U>\n",
    ">> - **Gray Swan:** <U>과거 경험들로 충분히 예상되지만 발생되면 충격이 지속되는 현상</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀분석 검증지표 및 해석하기\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Statmodels_OLS_Result.png' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) R-squared(R^2):** 추정된 (선형)모형이 주어진 데이터에 잘 적합된 정도, $(- \\infty, 1]$  \n",
    "<center>\n",
    "$R^2$ = $\\dfrac{ESS}{TSS}$ = $\\dfrac{\\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$ = $1 - \\dfrac{RSS}{TSS}$ = $1 - \\dfrac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "</center>\n",
    "\n",
    "> <center><img src='Image/R2_decomposition1.png' width='600'></center>\n",
    "\n",
    "> <center><img src='Image/R2_decomposition2.png' width='600'></center>\n",
    "\n",
    "- **(비수학적 이해)**\n",
    "    - TSS: 종속변수값의 움직임의 범위\n",
    "    - ESS: 모형에서 나온 예측값의 움직임의 범위\n",
    "    - RSS: 잔차의 움직임의 범위, 즉 오차의 크기\n",
    "    > 모형 예측치의 움직임의 크기(분산)은 종속변수의 움직임의 크기(분산)보다 클 수 없다  \n",
    "    > 모형의 성능이 좋을수록 모형 예측치의 움직임의 크기는 종속변수의 움직임의 크기와 비슷해진다  \n",
    "\n",
    "**2) t-검정:** t분포를 따르는 추정계수로 독립변수와 종속변수 간의 선형관계(관련성) 존재 의사결정을 위한 신뢰성 정도\n",
    "\n",
    "- **검정통계량(t-통계량)**: <center>$t = \\dfrac{\\hat{\\beta}_i - \\beta_i}{se_{\\hat{\\beta}_i}}$</center>  \n",
    "    \n",
    "    - $t$ 값이 작다는 것은 표준편차가 크다는 것 -> 독립변수와 종속변수의 상관성이 낮음  \n",
    "    - $t$ 값이 크다는 것은 표준편차가 작다는 것 -> 독립변수와 종속변수의 상관성이 높음  \n",
    "\n",
    ">- **가설확인**\n",
    "    - **대중주장(귀무가설, Null Hypothesis, $H_0$)**  \n",
    "    : $\\beta_i = 0 \\;\\; (i=0, \\ldots, K-1)$ / 추정계수는 0이다 / 독립변수와 종속변수의 상관관계(선형관계)가 없다\n",
    "    - **나의주장(대립가설, Alternate Hypothesis, $H_1$)**  \n",
    "    : $\\beta_i \\neq 0 \\;\\; (i=0, \\ldots, K-1)$ / 추정계수는 0이 아니다 / 독립변수와 종속변수의 상관관계(선형관계)가 있다\n",
    "> - **의사결정**\n",
    "    - **p-value >= 내기준(ex. 0.05):** 대중주장 참    \n",
    "    : **분석한 변수는 모델링에 영향력이 없다**\n",
    "    - **p-value < 내기준(ex. 0.05):** 나의주장 참   \n",
    "    : **분석한 변수는 모델링에 영향력이 있다**\n",
    "    \n",
    "- **회귀계수의 분포:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Main Equation} && \\hat{\\beta} &= (X^TX)^{-1} X^T Y \\\\\n",
    "&& &= (X^TX)^{-1} X^T (X \\beta + \\epsilon) \\\\\n",
    "&& &= \\beta + (X^TX)^{-1} X^T \\epsilon \\\\\n",
    "\\text{Expectation} && \\text{E}(\\hat{\\beta}) &=  \\text{E}( \\beta + (X^TX)^{-1} X^T \\epsilon ) \\\\\n",
    "&& &=  \\beta + (X^TX)^{-1} X^T \\text{E}( \\epsilon ) \\\\\n",
    "&& &= \\beta \\\\\n",
    "\\text{Variance} && \\text{Var}(\\hat{\\beta}_i)  &= \\left( \\text{Cov}(\\hat{\\beta}) \\right)_{ii} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Covariance} && \\text{Cov}(\\hat{\\beta}) &= E\\left((\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T \\right) \\\\\n",
    "&& &= E\\left(((X^TX)^{-1} X^T \\epsilon)((X^TX)^{-1} X^T \\epsilon)^T \\right) \\\\\n",
    "&& &= E\\left((X^TX)^{-1} X^T \\epsilon \\epsilon^T X(X^TX)^{−1} \\right) \\\\\n",
    "&& &= (X^TX)^{-1} X^T E(\\epsilon \\epsilon^T) X(X^TX)^{−1} \\\\\n",
    "&& &= (X^TX)^{-1} X^T (\\sigma^2 I) X(X^TX)^{−1} \\\\\n",
    "&& &= \\sigma^2  (X^TX)^{-1} \\\\\n",
    "\\text{Standard Deviation} && \\sqrt{\\text{Var}(\\hat{\\beta}_i)} \\approx {se_{\\hat{\\beta}_i}} &= \\sqrt{\\sigma^2 \\big((X^TX)^{-1}\\big)_{ii}} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Asymptotic} && \\dfrac{\\hat{\\beta}_i - \\beta_i}{se_{\\hat{\\beta}_i}} &\\sim t_{N-K} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**3) 분산분석을 통한 F-검정:**\n",
    "- **필요성:** \n",
    "    - 변수의 단위 즉, 스케일이 달라지면 회귀분석과 상관없이 잔차제곱합(Residula Sum of Square) 달라짐  \n",
    "    - 분산 분석(Analysis of Variance(ANOVA))은 종속변수의 분산과 독립변수의 분산간의 관계를 사용하여 성능 평가  \n",
    "\n",
    "<!-- - **검정통계량(F-통계량):** 분산분석표(ANOVA Table)를 통해 쉽게 계산되며, $T$는 데이터의 갯수, $K$는 변수의 갯수\n",
    "\n",
    "| Source | Degree of Freedom | Sum of Square | Mean Square | F test-statstics | p-value |\n",
    "|------------|-------------------|---------------|------------------------------------------|-------------------------------------------------------|-----------|\n",
    "| Regression | $K-1$ | $ESS$ | $\\sigma_{\\hat{Y}}^2 = \\dfrac{ESS}{K-1}$ | $F = \\dfrac{\\sigma_{\\hat{Y}}^2}{\\sigma_{\\epsilon}^2}$ | $p-value$ |\n",
    "| Residual | $T-K$ | $RSS$ | $\\sigma_{\\epsilon}^2 = \\dfrac{RSS}{T-K}$ | - | - |\n",
    "| Total | $T-1$ | $TSS$ | $\\sigma_{Y}^2 = \\dfrac{TSS}{T-1}$ | - | - |\n",
    "| $R^2$ | - | $ESS/TSS$ | - | - | - |\n",
    "\n",
    "<center>$\\dfrac{ESS}{K-1} \\div \\dfrac{RSS}{T-K} \\sim F(K-1, T-K)$</center> -->\n",
    "\n",
    ">- **가설확인**\n",
    "    - **대중주장(귀무가설, Null Hypothesis, $H_0$)**  \n",
    "    : $\\beta_0  = \\beta_1 = \\cdots = \\beta_{K-1} = 0$ / 모든 추정계수는 0이다 / 모형은 아무 효과가 없다 / $R^2$ = 0\n",
    "    - **나의주장(대립가설, Alternate Hypothesis, $H_1$)**  \n",
    "    : $\\beta_0  \\neq \\beta_1 \\neq \\cdots \\neq \\beta_{K-1} \\neq 0$ / 모든 추정계수는 0이 아니다 / 모형은 효과가 있다 / $R^2 \\neq 0$  \n",
    "\n",
    "> - **의사결정**    \n",
    "    - **p-value >= 내기준(ex. 0.05):** 대중주장 참    \n",
    "    : **분석한 모델링은 효과가 없다 / 모델은 데이터 패턴을 잘 추정하지 못한다**\n",
    "    - **p-value < 내기준(ex. 0.05):** 나의주장 참   \n",
    "    : **분석한 모델링은 효과가 있다 / 모델은 데이터 패턴을 잘 추정한다**\n",
    "    \n",
    "**3) 정보량기준(Information Criterion):** 회귀분석 외에도 다양한 알고리즘에 활용, 값이 작을수록 올바른 모형 (Likelihood는 클수록 올바른 모형)\n",
    "\n",
    "- **[AIC(Akaike Information Criterion)](https://en.wikipedia.org/wiki/Akaike_information_criterion)**  \n",
    ": 모형과 데이터의 확률 분포 사이의 Kullback-Leibler 수준을 가장 크게하기 위한 시도 \n",
    "\n",
    "<center>$AIC = -2log(L) + 2K$</center>\n",
    "<center>($L$: likelihood, $K$: 추정할 파라미터의 수(일반적으로 column수))</center>\n",
    "\n",
    "- **[BIC(Bayesian Information Criterion)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)**  \n",
    ": 데이터가 exponential family라는 가정하에 주어진 데이터에서 모형의 likelihood를 측정하기 위한 값에서 유도 \n",
    "\n",
    "<center>$BIC = -2log(L) + Klog(T)$</center>\n",
    "<center>($L$: likelihood, $K$: 추정할 파라미터의 수(일반적으로 column수), $T$: 데이터의 수(일반적으로 row수))</center>\n",
    "\n",
    "<center><img src='Image/AIC_BIC.gif' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잔차진단 방향(Residuals Diagnostics)\n",
    "\n",
    ">**\"예측 분석 이후 예측이 잘 되었는지 그리고 데이터의 패턴이 잘 추출 되었는지 평가하는 것이 중요\"**\n",
    "> - 패턴이 잘 추출되었는지 확인하기 위해선 잔차(또는 에러) 진단을 통해 백색잡음(White Noise)과 얼마나 유사한지 측정   \n",
    "=> <U>\"Residual Diagnostics\" or \"Error Analysis\"</U>\n",
    ">> **\"모델링이 데이터의 패턴을 최대한 반영했을 경우 분석을 마무리 해도 좋다\"**\n",
    "\n",
    "- **백색잡음(White Noise, $WN$):**   \n",
    ">**\"백색잡음(White Noise)는 2가지의 속성을 만족해야 하며 하나라도 만족하지 못하면 모델이 개선의 여지가 있음을 의미합니다.\"**  \n",
    "\n",
    "<center><img src='Image/White_Noise.png' width='300'></center>\n",
    "\n",
    ">**1) 잔차들은 정규분포이고, (unbiased) 평균 0과 일정한 분산을 가져야 함:**  \n",
    "\\begin{align*}\n",
    "\\{\\epsilon_t : t = \\dots, -2, -1, 0, 1, 2, \\dots\\} \\sim N(0,\\sigma^2_{\\epsilon_t}) \\\\\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "where~~ \\epsilon_t \\sim  i.i.d(independent~and~identically~distributed) \\\\\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "\\epsilon_t = Y_t - \\hat{Y_t}, \\;\\; E(\\epsilon_t) = 0, \\;\\; Var(\\epsilon_t) = \\sigma^2_{\\epsilon_t} \\\\\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "Cov(\\epsilon_s, \\epsilon_k) = 0~for~different~times!(s \\ne k)\n",
    "\\end{align*}\n",
    "\n",
    "- **회귀분석 가정:**\n",
    "    - 종속변수와 독립변수 간에 선형성의 관계를 가져야 함\n",
    "    - 독립변수들 간에 서로 독립이어야 함\n",
    "    - 잔차의 분포가 정규분포이어야 함\n",
    "    - 잔차들이 서로 독립적으로 움직여야 함\n",
    "    - 잔차들의 분산이 서로 같아야 함\n",
    "    \n",
    "---\n",
    "\n",
    "<center><img src='Image/Statmodels_OLS_Result.png' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [정규분포 테스트(Normality Test)](https://en.wikipedia.org/wiki/Normality_test)\n",
    "\n",
    "- [**Shapiro–Wilk test:**](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)\n",
    "    - **가설확인**\n",
    "        - **대중주장(귀무가설, Null Hypothesis, $H_0$):** 데이터는 정규분포 형태이다\n",
    "        - **나의주장(대립가설, Alternative Hypothesis, $H_1$):** 데이터는 정규분포가 아닌 형태다\n",
    "    - **의사결정**\n",
    "        - **p-value >= 내기준(ex. 0.05):** 대중주장 참\n",
    "        > 내가 수집한(분석한) 데이터는 정규분포 형태이다\n",
    "        - **p-value < 내기준(ex. 0.05):** 나의주장 참\n",
    "        > 내가 수집한(분석한) 데이터는 정규분포가 아닌 형태다\n",
    "\n",
    "- [**Kolmogorov–Smirnov test:**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Lilliefors test:**](https://en.wikipedia.org/wiki/Lilliefors_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Anderson–Darling test:**](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Jarque–Bera test:**](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Pearson's chi-squared test:**](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**D'Agostino's K-squared test:**](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "    \n",
    "    \n",
    "- **예시:**\n",
    "\n",
    "<!-- <center><img src='Image/SW_example1.png' width='600'></center> -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- <center><img src='Image/SW_example2.png' width='300'></center> -->\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/SW_example3.png' width='400'></center>\n",
    "\n",
    "---\n",
    "\n",
    "<!-- <center><img src='Image/SW_example4.png' width='600'></center> -->\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/SW_example5.png' width='500'></center>\n",
    "\n",
    "---\n",
    "\n",
    "<!-- <center><img src='Image/SW_example6.png' width='600'></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자기상관 테스트(Autocorrelation Test)\n",
    "\n",
    "- [**Ljung–Box test:**](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test)\n",
    "    - **가설확인**\n",
    "        - **대중주장(귀무가설, Null Hypothesis, $H_0$):** 데이터의 Autocorrelation은 0이다(존재하지 않는다)\n",
    "        - **나의주장(대립가설, Alternative Hypothesis, $H_1$):** 데이터의 Autocorrelation은 0이 아니다(존재한다)\n",
    "    - **의사결정**\n",
    "        - **p-value >= 내기준(ex. 0.05):** 대중주장 참\n",
    "        > **내가 수집한(분석한) 데이터의 Autocorrelation은 존재하지 않는다**\n",
    "        - **p-value < 내기준(ex. 0.05):** 나의주장 참\n",
    "        > **내가 수집한(분석한) 데이터의 Autocorrelation은 존재한다**\n",
    "\n",
    "- [**Portmanteau test:**](https://en.wikipedia.org/wiki/Portmanteau_test)\n",
    "    - **가설확인:** Ljung–Box와 동일\n",
    "\n",
    "- [**Breusch–Godfrey test:**](https://en.wikipedia.org/wiki/Breusch%E2%80%93Godfrey_test)\n",
    "    - **가설확인:** Ljung–Box와 동일\n",
    "  \n",
    "- [**Durbin–Watson statistic:**](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic)\n",
    "    - **가설확인:** Ljung–Box와 동일\n",
    "    - **의사결정:** 검정통계량 범위 - $[0, 4]$\n",
    "        - **2 근방:** 대중주장 참\n",
    "        > **내가 수집한(분석한) 데이터의 Autocorrelation은 존재하지 않는다**\n",
    "        - **0 또는 4 근방:** 나의주장 참\n",
    "        > **내가 수집한(분석한) 데이터의 Autocorrelation은 존재한다**\n",
    "            - 0: 양(Positive)의 Autocorrelation 존재한다\n",
    "            - 4: 음(Negative)의 Autocorrelation 존재한다\n",
    "            \n",
    "            \n",
    "- **예시:**\n",
    "\n",
    "<center><img src='Image/LB_example1.jpg' width='400'></center>\n",
    "\n",
    "---\n",
    "\n",
    "<!-- <center><img src='Image/LB_example2.jpg' width='600'></center> -->\n",
    "\n",
    "---\n",
    "\n",
    "<!-- <center><img src='Image/DW_example1.png' width='600'></center> -->\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/DW_example2.png' width='300'></center>\n",
    "\n",
    "---\n",
    "\n",
    "<!-- <center><img src='Image/DW_example3.png' width='600'></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [등분산성 테스트(Homoscedasticity Test)](https://en.wikipedia.org/wiki/Homoscedasticity)\n",
    "\n",
    "- [**Goldfeld–Quandt test:**](https://en.wikipedia.org/wiki/Goldfeld%E2%80%93Quandt_test)\n",
    "    - **가설확인**\n",
    "        - **대중주장(귀무가설, Null Hypothesis, $H_0$):** 데이터의 Homoscedasticity 상태다(등분산이다)\n",
    "        - **나의주장(대립가설, Alternative Hypothesis, $H_1$):** 데이터의 Heteroscedasticity 상태다(등분산이 아니다 / 발산하는 분산이다)\n",
    "    - **의사결정**\n",
    "        - **p-value >= 내기준(ex. 0.05):** 대중주장 참\n",
    "        > **내가 수집한(분석한) 데이터는 등분산이다**\n",
    "        - **p-value < 내기준(ex. 0.05):** 나의주장 참\n",
    "        > **내가 수집한(분석한) 데이터는 등분산이 아니다**\n",
    "\n",
    "- [**Breusch–Pagan test:**](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test)\n",
    "    - **가설확인:** Goldfeld–Quandt와 동일\n",
    "\n",
    "- [**Bartlett's test:**](https://en.wikipedia.org/wiki/Bartlett%27s_test)\n",
    "    - **가설확인:** Goldfeld–Quandt와 동일\n",
    "    \n",
    "    \n",
    "- **예시:**\n",
    "\n",
    "<!-- <center><img src='Image/GQ_example1.jpg' width='500'></center> -->\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/GQ_example2.jpg' width='400'></center>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "390px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
