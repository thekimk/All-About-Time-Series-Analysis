{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **데이터분석 단계(Data Analysis Cycle)**\n",
    "\n",
    "[![Open in Colab](http://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thekimk/All-About-Machine-Learning/blob/main/Lecture2-2_DataAnalysis_SupervisedRegression_KK.ipynb)\n",
    "\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle0.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle1.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle2.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle3.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle4.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle5.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle6.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle7.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle8.png' width='800'></center>\n",
    "<center><img src='Image/Advanced/DataAnalysis_Cycle9.png' width='800'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **지도학습(Supervised) 알고리즘:** 회귀분석\n",
    "\n",
    "---\n",
    "\n",
    "- **데이터분석 과정:** `학습` + `추론/예측`\n",
    "\n",
    "<center><img src='Image/Advanced/DataSplit_Concept1.png' width='700'></center>\n",
    "<center><img src='Image/Advanced/DataSplit_Concept2.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "- **회귀분석:** `지도학습` 알고리즘 중 `예측`을 위해 사용되는 가장 `기본(Baseline) 알고리즘`\n",
    "\n",
    "> **\"연속형 출력(`Y, 종속변수`)에 영향을 주는 입력(`X, 독립변수`)과의 관계를 정량적으로 추론/추정하여 `미래 값을 예측`하는 알고리즘\"**\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/ML_Type_Application_Upgrade.png' width='700'></center>  \n",
    "<center><img src='Image/Advanced/ML_Type_Category.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "| Regression Algorithms | Instance-based Algorithms | Regularization Algorithms | Decision Tree Algorithms | Bayesian Algorithms | Artificial Neural Network Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src='Image/Advanced/Regression-Algorithms.png' width='150'> | <img src='Image/Advanced/Instance-based-Algorithms.png' width='150'> | <img src='Image/Advanced/Regularization-Algorithms.png' width='150'> | <img src='Image/Advanced/Decision-Tree-Algorithms.png' width='150'> | <img src='Image/Advanced/Bayesian-Algorithms.png' width='150'> | <img src='Image/Advanced/Artificial-Neural-Network-Algorithms.png' width='150'> |\n",
    "| Ordinary Least Squares Regression (OLSR) | k-Nearest Neighbor (kNN) | Ridge Regression | Classification and Regression Tree (CART) | Naive Bayes | Perceptron |\n",
    "| Linear Regression | Learning Vector Quantization (LVQ) | Least Absolute Shrinkage and Selection Operator (LASSO) | Iterative Dichotomiser 3 (ID3) | Gaussian Naive Bayes | Back-Propagation |\n",
    "| Logistic Regression | Self-Organizing Map (SOM) | Elastic Net | C4.5 and C5.0 (different versions of a powerful approach) | Multinomial Naive Bayes | Hopfield Network |\n",
    "| Stepwise Regression | Locally Weighted Learning (LWL) | Least-Angle Regression (LARS) | Chi-squared Automatic Interaction Detection (CHAID) | Averaged One-Dependence Estimators (AODE) | Radial Basis Function Network (RBFN) |\n",
    "| Multivariate Adaptive Regression Splines (MARS) | - | - | Decision Stump | Bayesian Belief Network (BBN) | - |\n",
    "| Locally Estimated Scatterplot Smoothing (LOESS) | - | - | M5 | Bayesian Network (BN) | - |\n",
    "| - | - | - | Conditional Decision Trees | - | - |\n",
    "\n",
    "- **Target Algorithm:**\n",
    "> - `Linear Regression` (Simple/Multiple/Multivariate)\n",
    "> - Polynomial Regression\n",
    "> - Stepwise Regression\n",
    "> - Ridge Regression\n",
    "> - Lasso Regression\n",
    "> - ElasticNet Regression\n",
    "> - Bayesian Linear Regression\n",
    "> - Quantile Regression\n",
    "> - Decision Tree Regression\n",
    "> - Random Forest Regression\n",
    "> - Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **예제 데이터셋(Dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodels 모듈 사용 데이터셋\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "import statsmodels.api as sm\n",
    "```\n",
    "\n",
    "> - 대기중 `CO2농도` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"CO2\", package=\"datasets\")\n",
    "> ```\n",
    "> - 황체형성 `호르몬(Luteinizing Hormone)`의 수치 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"lh\")\n",
    "> ```\n",
    "> - 1974~1979년 사이의 영국의 `호흡기 질환 사망자 수` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"deaths\", \"MASS\")\n",
    "> ```\n",
    "> - 1949~1960년 사이의 `국제 항공 운송인원` 데이터:\n",
    "> ```\n",
    "> data = sm.datasets.get_rdataset(\"AirPassengers\")\n",
    "> ```\n",
    "> - 미국의 `강수량` 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"precip\")\n",
    "> ```\n",
    "> - `타이타닉호의 탑승자`들에 대한 데이터:\n",
    "> ```python\n",
    "> data = sm.datasets.get_rdataset(\"Titanic\", package=\"datasets\")\n",
    "> ```\n",
    "\n",
    "- **data가 포함하는 정보:**\n",
    "> - `package`: 데이터를 제공하는 R 패키지 이름\n",
    "> - `title`: 데이터 이름\n",
    "> - `data`: 데이터를 담고 있는 데이터프레임\n",
    "> - `__doc__`: 데이터에 대한 설명 문자열(R 패키지의 내용 기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 모듈 사용 데이터셋\n",
    "\n",
    "**1) 패키지에 포함된 데이터(`load 명령어`)**\n",
    "    \n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import load_boston\n",
    "```\n",
    "\n",
    "> - load_boston: 회귀용 `보스턴 집값`\n",
    "> ```python\n",
    "> raw = load_boston()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - load_diabetes: 회귀용 `당뇨병` 자료\n",
    "> - load_linnerud: 회귀용 `linnerud` 자료\n",
    "> - load_iris: 분류용 `붓꽃(iris)` 자료\n",
    "> - load_digits: 분류용 `숫자(digit) 필기 이미지` 자료\n",
    "> - load_wine: 분류용 `포도주(wine) 등급` 자료\n",
    "> - load_breast_cancer: 분류용 `유방암(breast cancer)` 진단 자료\n",
    "\n",
    "**2) 인터넷에서 다운로드할 수 있는 데이터(`fetch 명령어`)**\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "```\n",
    "\n",
    "> - fetch_california_housing: : 회귀용 `캘리포니아 집값`\n",
    "> ```python\n",
    "> raw = fetch_california_housing()\n",
    "> print(raw.DESCR)\n",
    "> print(raw.keys())\n",
    "> print(raw.data.shape, raw.target.shape)\n",
    "> ```\n",
    "> - fetch_covtype : 회귀용 `토지` 조사 자료\n",
    "> - fetch_20newsgroups : `뉴스 그룹` 텍스트 자료\n",
    "> - fetch_olivetti_faces : `얼굴 이미지` 자료\n",
    "> - fetch_lfw_people : `유명인 얼굴 이미지` 자료\n",
    "> - fetch_lfw_pairs : `유명인 얼굴 이미지` 자료\n",
    "> - fetch_rcv1 : 로이터 `뉴스 말뭉치`\n",
    "> - fetch_kddcup99 : `Kddcup 99 Tcp dump` 자료\n",
    "\n",
    "**3) 확률분포를 사용한 가상 데이터(`make 명령어`)**\n",
    "\n",
    "```python\n",
    "# 라이브러리 불러오기\n",
    "from sklearn.datasets import make_regression\n",
    "```\n",
    "\n",
    "> - make_regression: `회귀용` 가상 데이터\n",
    "> ```python\n",
    "> X, y, c = make_regression(n_samples=100, n_features=10, n_targets=1, bias=0, noise=0, coef=True, random_state=0)\n",
    "> ```\n",
    "> - make_classification: `분류용` 가상 데이터 생성\n",
    "> - make_blobs: `클러스터링용` 가상 데이터 생성\n",
    "\n",
    "---\n",
    "\n",
    "**4) `load/fetch 명령어 데이터`에서 raw가 포함하는 정보:** Bunch 라는 `클래스 객체 형식`으로 생성\n",
    "\n",
    "> - `data`: (필수) 독립 변수 ndarray 배열\n",
    "> - `target`: (필수) 종속 변수 ndarray 배열\n",
    "> - `feature_names`: (옵션) 독립 변수 이름 리스트\n",
    "> - `target_names`: (옵션) 종속 변수 이름 리스트\n",
    "> - `DESCR`: (옵션) 자료에 대한 설명\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀문제 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:31:46.883237Z",
     "start_time": "2022-04-21T18:31:45.082116Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:28.289051Z",
     "iopub.status.busy": "2025-03-01T12:56:28.289051Z",
     "iopub.status.idle": "2025-03-01T12:56:30.288629Z",
     "shell.execute_reply": "2025-03-01T12:56:30.288629Z",
     "shell.execute_reply.started": "2025-03-01T12:56:28.289051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon Dioxide Uptake in Grass Plants\n",
      ".. container::\n",
      "\n",
      "   .. container::\n",
      "\n",
      "      === ===============\n",
      "      CO2 R Documentation\n",
      "      === ===============\n",
      "\n",
      "      .. rubric:: Carbon Dioxide Uptake in Grass Plants\n",
      "         :name: carbon-dioxide-uptake-in-grass-plants\n",
      "\n",
      "      .. rubric:: Description\n",
      "         :name: description\n",
      "\n",
      "      The ``CO2`` data frame has 84 rows and 5 columns of data from an\n",
      "      experiment on the cold tolerance of the grass species *Echinochloa\n",
      "      crus-galli*.\n",
      "\n",
      "      .. rubric:: Usage\n",
      "         :name: usage\n",
      "\n",
      "      .. code:: R\n",
      "\n",
      "         CO2\n",
      "\n",
      "      .. rubric:: Format\n",
      "         :name: format\n",
      "\n",
      "      An object of class\n",
      "      ``c(\"nfnGroupedData\", \"nfGroupedData\", \"groupedData\", \"data.frame\")``\n",
      "      containing the following columns:\n",
      "\n",
      "      Plant\n",
      "         an ordered factor with levels ``Qn1`` < ``Qn2`` < ``Qn3`` < ...\n",
      "         < ``Mc1`` giving a unique identifier for each plant.\n",
      "\n",
      "      Type\n",
      "         a factor with levels ``Quebec`` ``Mississippi`` giving the\n",
      "         origin of the plant\n",
      "\n",
      "      Treatment\n",
      "         a factor with levels ``nonchilled`` ``chilled``\n",
      "\n",
      "      conc\n",
      "         a numeric vector of ambient carbon dioxide concentrations\n",
      "         (mL/L).\n",
      "\n",
      "      uptake\n",
      "         a numeric vector of carbon dioxide uptake rates\n",
      "         (``\\mu\\mbox{mol}/m^2`` sec).\n",
      "\n",
      "      .. rubric:: Details\n",
      "         :name: details\n",
      "\n",
      "      The ``CO_2`` uptake of six plants from Quebec and six plants from\n",
      "      Mississippi was measured at several levels of ambient ``CO_2``\n",
      "      concentration. Half the plants of each type were chilled overnight\n",
      "      before the experiment was conducted.\n",
      "\n",
      "      This dataset was originally part of package ``nlme``, and that has\n",
      "      methods (including for ``[``, ``as.data.frame``, ``plot`` and\n",
      "      ``print``) for its grouped-data classes.\n",
      "\n",
      "      .. rubric:: Source\n",
      "         :name: source\n",
      "\n",
      "      Potvin, C., Lechowicz, M. J. and Tardif, S. (1990) “The\n",
      "      statistical analysis of ecophysiological response curves obtained\n",
      "      from experiments involving repeated measures”, *Ecology*, **71**,\n",
      "      1389–1400.\n",
      "\n",
      "      Pinheiro, J. C. and Bates, D. M. (2000) *Mixed-effects Models in S\n",
      "      and S-PLUS*, Springer.\n",
      "\n",
      "      .. rubric:: Examples\n",
      "         :name: examples\n",
      "\n",
      "      .. code:: R\n",
      "\n",
      "         require(stats); require(graphics)\n",
      "\n",
      "         coplot(uptake ~ conc | Plant, data = CO2, show.given = FALSE, type = \"b\")\n",
      "         ## fit the data for the first plant\n",
      "         fm1 <- nls(uptake ~ SSasymp(conc, Asym, lrc, c0),\n",
      "            data = CO2, subset = Plant == \"Qn1\")\n",
      "         summary(fm1)\n",
      "         ## fit each plant separately\n",
      "         fmlist <- list()\n",
      "         for (pp in levels(CO2$Plant)) {\n",
      "           fmlist[[pp]] <- nls(uptake ~ SSasymp(conc, Asym, lrc, c0),\n",
      "               data = CO2, subset = Plant == pp)\n",
      "         }\n",
      "         ## check the coefficients by plant\n",
      "         print(sapply(fmlist, coef), digits = 3)\n",
      "\n",
      "dict_keys(['data', '__doc__', 'package', 'title', 'from_cache'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plant</th>\n",
       "      <th>Type</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>conc</th>\n",
       "      <th>uptake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qn1</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>nonchilled</td>\n",
       "      <td>95</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qn1</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>nonchilled</td>\n",
       "      <td>175</td>\n",
       "      <td>30.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qn1</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>nonchilled</td>\n",
       "      <td>250</td>\n",
       "      <td>34.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qn1</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>nonchilled</td>\n",
       "      <td>350</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qn1</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>nonchilled</td>\n",
       "      <td>500</td>\n",
       "      <td>35.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Mc3</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>chilled</td>\n",
       "      <td>250</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Mc3</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>chilled</td>\n",
       "      <td>350</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Mc3</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>chilled</td>\n",
       "      <td>500</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Mc3</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>chilled</td>\n",
       "      <td>675</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Mc3</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>chilled</td>\n",
       "      <td>1000</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Plant         Type   Treatment  conc  uptake\n",
       "0    Qn1       Quebec  nonchilled    95    16.0\n",
       "1    Qn1       Quebec  nonchilled   175    30.4\n",
       "2    Qn1       Quebec  nonchilled   250    34.8\n",
       "3    Qn1       Quebec  nonchilled   350    37.2\n",
       "4    Qn1       Quebec  nonchilled   500    35.3\n",
       "..   ...          ...         ...   ...     ...\n",
       "79   Mc3  Mississippi     chilled   250    17.9\n",
       "80   Mc3  Mississippi     chilled   350    17.9\n",
       "81   Mc3  Mississippi     chilled   500    17.9\n",
       "82   Mc3  Mississippi     chilled   675    18.9\n",
       "83   Mc3  Mississippi     chilled  1000    19.9\n",
       "\n",
       "[84 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Regression\n",
    "import statsmodels.api as sm\n",
    "data = sm.datasets.get_rdataset(\"CO2\", package=\"datasets\")\n",
    "print(data.title)\n",
    "print(data.__doc__)\n",
    "print(data.keys())\n",
    "display(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:31:47.033237Z",
     "start_time": "2022-04-21T18:31:47.004238Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:30.290642Z",
     "iopub.status.busy": "2025-03-01T12:56:30.290642Z",
     "iopub.status.idle": "2025-03-01T12:56:30.417540Z",
     "shell.execute_reply": "2025-03-01T12:56:30.417540Z",
     "shell.execute_reply.started": "2025-03-01T12:56:30.290642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n",
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "raw = fetch_california_housing()\n",
    "print(raw.DESCR)\n",
    "print(raw.keys())\n",
    "print(raw.data.shape, raw.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T18:31:47.048241Z",
     "start_time": "2022-04-21T18:31:47.034239Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:30.418544Z",
     "iopub.status.busy": "2025-03-01T12:56:30.418544Z",
     "iopub.status.idle": "2025-03-01T12:56:30.422962Z",
     "shell.execute_reply": "2025-03-01T12:56:30.422962Z",
     "shell.execute_reply.started": "2025-03-01T12:56:30.418544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10) (100,) (10,)\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "from sklearn.datasets import make_regression\n",
    "X, y, c = make_regression(n_samples=100, n_features=10, n_targets=1, bias=0, noise=0, coef=True, random_state=0)\n",
    "print(X.shape, y.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://lib.stat.cmu.edu/datasets/boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:30.423971Z",
     "iopub.status.busy": "2025-03-01T12:56:30.423971Z",
     "iopub.status.idle": "2025-03-01T12:56:32.551343Z",
     "shell.execute_reply": "2025-03-01T12:56:32.551343Z",
     "shell.execute_reply.started": "2025-03-01T12:56:30.423971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_17908\\3444797937.py:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  raw = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw.values[::2, :], raw.values[1::2, :2]])\n",
    "target = raw.values[1::2, 2]\n",
    "display(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:32.553357Z",
     "iopub.status.busy": "2025-03-01T12:56:32.553357Z",
     "iopub.status.idle": "2025-03-01T12:56:32.580808Z",
     "shell.execute_reply": "2025-03-01T12:56:32.580808Z",
     "shell.execute_reply.started": "2025-03-01T12:56:32.553357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>22.4</td>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.9</td>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>11.9</td>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price     CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  \\\n",
       "0     24.0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0   \n",
       "1     21.6  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0   \n",
       "2     34.7  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0   \n",
       "3     33.4  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0   \n",
       "4     36.2  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0   \n",
       "..     ...      ...   ...    ...   ...    ...    ...   ...     ...  ...   \n",
       "501   22.4  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0   \n",
       "502   20.6  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0   \n",
       "503   23.9  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0   \n",
       "504   22.0  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0   \n",
       "505   11.9  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0   \n",
       "\n",
       "       TAX  PTRATIO       B  LSTAT  \n",
       "0    296.0     15.3  396.90   4.98  \n",
       "1    242.0     17.8  396.90   9.14  \n",
       "2    242.0     17.8  392.83   4.03  \n",
       "3    222.0     18.7  394.63   2.94  \n",
       "4    222.0     18.7  396.90   5.33  \n",
       "..     ...      ...     ...    ...  \n",
       "501  273.0     21.0  391.99   9.67  \n",
       "502  273.0     21.0  396.90   9.08  \n",
       "503  273.0     21.0  396.90   5.64  \n",
       "504  273.0     21.0  393.45   6.48  \n",
       "505  273.0     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression\n",
    "import os\n",
    "import pandas as pd\n",
    "raw = pd.read_csv(os.path.join('.', 'Data', 'BostonHousingPrice', 'Price.csv'))\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **전처리 방향(Preprocessing)**\n",
    "\n",
    "- **목표:** \n",
    "> - 대량으로 수집된 데이터는 `그대로 활용 어려움`\n",
    "> - `잘못 수집/처리 된 데이터`는 엉뚱한 결과를 발생\n",
    "> - 알고리즘이 `학습이 가능한 형태`로 데이터를 정리\n",
    "<center><img src='Image/Advanced/DataAnalysis_Time.jpg' width='500'></center> \n",
    "---\n",
    "\n",
    "> **일반적인 전처리 필요항목:**  \n",
    "> - 데이터 결합\n",
    "> - 결측값 처리\n",
    "> - 이상치 처리\n",
    "> - 자료형 변환\n",
    "> - 데이터 분리\n",
    "> - 데이터 변환\n",
    "> - 스케일 조정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 결합(Data Integration)\n",
    "\n",
    "- **목표:** 여러개로 `구분된 데이터` 이거나 `빅데이터로 확장` 할 경우 결합\n",
    "\n",
    "> - 중복 데이터 제거\n",
    "> - 의미는 같으나 단위나 이름의 표현이 다른 경우 일치 필요\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "pd.merge(베이스 데이터프레임, 결합할 데이터프레임, how= , on= , ...)\n",
    "```\n",
    "- `how`: left, right, inner, outer (2개의 데이터프레임을 어떤 방식으로 결합할지 결정)\n",
    "- `on`: 2개의 데이터프레임 결합을 위한 key 설정\n",
    "- `left_on`: key 이름이 서로 다를 경우 `베이스 데이터프레임`의 key 설정\n",
    "- `right_on`: key 이름이 서로 다를 경우 `결합할 데이터프레임`의 key 설정    \n",
    "\n",
    "<center><img src='Image/Advanced/Pandas_Merge.jpg' width='500'></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자료형 변환(Type)\n",
    "\n",
    "- **목표:** 각 `변수의 특성을 확인`하고 `범주형`과 `연속형`에 맞도록 `변경`\n",
    "\n",
    "<center><img src='Image/Advanced/Basic_DataType.PNG' width='1000'></center>\n",
    "\n",
    "- **사람의 데이터 분류:**\n",
    "\n",
    "> - **데이터 관계에 따라**: `Y = f(X)` \n",
    ">\n",
    "> |  **대분류**  |  **의미/예시**  |\n",
    "|:---:|:---:|\n",
    "|  **독립변수(Independent Variable)**  |  다른 변수에 `영향을 미치는` 변수 (X)  |\n",
    "|  **종속변수(Dependent Variable)**  |  다른 변수에 의해 `영향을 받는` 변수 (Y)  |\n",
    ">\n",
    "> - **데이터 특성에 따라**:\n",
    ">\n",
    "> |  **대분류**  |  **소분류**  |  **의미/예시**  |\n",
    "|:---:|:---:|:---:|\n",
    "|  **질적변수(Qualitative Variable)**  |  **-**  |  내부 값이 특정 범주(Category)로 분류된 변수(색상,성별,종교)  |\n",
    "|    |  **명목형 변수(Nominal Variable)**  |  값이 순위가 존재하지 않는 경우(혈액형)  |\n",
    "|    |  **순위형 변수(Ordinal Variable)**  |  값이 순위가 존재하는 경우(성적)  |\n",
    "|  **양적변수(Quantitative Variable)**  |  **-**  |  내부 값이 다양한 숫자 분포로 구성된 변수(키,몸무게,소득)  |\n",
    "|    |  **이산형 변수(Discrete Variable)**  |  값이 셀수 있는 경우(정수)  |\n",
    "|    |  **연속형 변수(Continuous   Variable)**  |  값이 셀수 없는 경우(실수)  |\n",
    "\n",
    "- **컴퓨터의 데이터 분류:**\n",
    "\n",
    "| **대분류** | **소분류** | **컴퓨터의 분류1** | **컴퓨터의 분류2** |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| **질적변수(Qualitative Variable)** | **-** | - | 범주형 |\n",
    "|  | **명목형 변수(Nominal Variable)** | 문자 | 범주형 |\n",
    "|  | **순위형 변수(Ordinal Variable)** | 숫자 | 범주형 |\n",
    "| **양적변수(Quantitative Variable)** | **-** | - | 연속형 |\n",
    "|  | **이산형 변수(Discrete Variable)** | 숫자 | 연속형 |\n",
    "|  | **연속형 변수(Continuous Variable)** | 숫자 | 연속형 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측값 처리(Missing Value)\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/Pandas_DataFrameStructure.png' width='700'>(https://www.geeksforgeeks.org/creating-a-pandas-dataframe/)</center>\n",
    "\n",
    "- **목표:** 결측값이란 `값이 비어있는 것(NaN)`을 의미하며, `알고리즘 작동을 어렵게` 하고 작동이 되어도 `해석의 왜곡` 가능성 존재하기 때문에 `처리 필요`\n",
    "\n",
    "> - **삭제:** 결측값이 발생한 모든 변수(Column)를 삭제하거나 일부(Row)를 삭제\n",
    ">\n",
    ">```python\n",
    ">df.dropna(axis=0)    # 행 삭제\n",
    ">df.dropna(axis=1)    # 열 삭제\n",
    ">```\n",
    "> - **대체:** 결측값을 제외한 값들의 `통계량으로 결측값을 대체`\n",
    ">> - 중심 통계량\n",
    ">> - 분포 기반 랜던 추출\n",
    ">>\n",
    ">>```python\n",
    ">>df.fillna(df.mean())    # 평균치로 대체\n",
    ">>df.fillna(df.median())    # 중앙값으로 대체\n",
    ">>df.fillna(df.mode())    # 최빈값으로 대체\n",
    ">>```\n",
    "> - **예측:** 별도 분석을 통해 `결측값을 예측`하여 삽입\n",
    ">> - Interpolation\n",
    ">> - Regression Imputation\n",
    ">> - EM Algorithm\n",
    ">>\n",
    ">>```python\n",
    ">>df.interpolate(method='linear')    # 선형방식으로 삽입\n",
    ">>df.interpolate(method='time')    # 인덱스 날짜고려 선형방식으로 삽입\n",
    ">>df.interpolate(method='spline')    # 비선형방식으로 삽입\n",
    ">>df.interpolate(method='polynomial')    # 비선형 다항식으로 삽입\n",
    ">>```\n",
    ">\n",
    "> | **결측치 비율** | **처리 방향** |\n",
    "|:---:|:---:|\n",
    "| **10% 미만** | 삭제 또는 통계량기반 대체 |\n",
    "| **10% ~ 30%** | 모델링기반 예측 |\n",
    "| **30% 이상** | 변수의 완전성/신뢰성 문제로 삭제 |\n",
    "<center>(Hair et al., 2016)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상치 처리(Outlier)\n",
    "\n",
    "- **목표:** 일반적인 데이터와 `동떨어진 관측치`로 분석 결과를 `왜곡할 가능성을 줄이는 것`\n",
    "\n",
    "**1) 검출:**\n",
    "> - 전통적으로 분포(Boxplot / Histogram / Scatter Plot 등)의 `중심에서 벗어난 값`을 지칭\n",
    "> - `빅데이터` 시대에서는 `이상치의 존재 유무? 논란`이 존재\n",
    "> <center><img src='Image/Advanced/BoxPlot.png' width='600'></center>\n",
    "\n",
    "**2) 처리:**\n",
    "> - **삭제:** `Human Error` 등은 보통 삭제 처리\n",
    "> - **대체:** `스몰데이터`의 경우 삭제시 데이터의 양이 적어지기에 다른 값으로 대체\n",
    "> - **예측:** 별도 `분석을 통해` 이상치 대신 `예측값`으로 반영\n",
    "> - **변수화:** 이상치를 변수화 하여 유의성을 판별\n",
    "> - **별도 분석:** 이상치 포함 분석과 이상치 미포함 분석을 `병행 진행`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T06:45:06.962299Z",
     "start_time": "2019-06-15T06:45:06.947692Z"
    }
   },
   "source": [
    "## 데이터 분리(Data Split)\n",
    "\n",
    "- **배경:**\n",
    "> **(1) 독립변수와 종속변수 구분**\n",
    "> \n",
    "> |  **대분류**  |  **의미/예시**  |\n",
    "|:---:|:---:|\n",
    "|  **독립변수(Independent Variable)**  |  다른 변수에 `영향을 미치는` 변수 (X)  |\n",
    "|  **종속변수(Dependent Variable)**  |  다른 변수에 의해 `영향을 받는` 변수 (Y)  |\n",
    ">\n",
    "> **(2) 과거/현재와 미래 기간 구분:** 과거/현재의 상황을 분석하고, 미래를 예측 할 수 있는 환경 구축\n",
    "> <br>\n",
    ">\n",
    "> - **Training Period:** 과거/현재의 상황을 분석\n",
    ">\n",
    "> <center><img src='Image/Advanced/DataSplit_Concept1.png' width='700'></center>\n",
    "> <br>\n",
    ">\n",
    "> - **Testing Period:** 미래를 예측 할 수 있는 환경\n",
    ">\n",
    "> <center><img src='Image/Advanced/DataSplit_Concept2.png' width='700'></center>\n",
    "\n",
    "**1) 간단한 방법(Holdout Validation):**\n",
    "\n",
    "- **훈련셋(Training set):** 일반적으로 `전체 데이터의 70%` 사용 \n",
    "- **테스트셋(Testing set):** 일반적으로 `전체 데이터의 30%` 사용\n",
    "\n",
    "**2) 일반적 방법(Simple Validation):**\n",
    "- **훈련셋(Training set):** 일반적으로 `전체 데이터의 60%`를 사용\n",
    "- **검증셋(Validation set):** \n",
    "> - 개발셋이라고도 하며, 일반적으로 `전체 데이터의 20%`를 사용함\n",
    "> - `훈련된 여러가지 모델들의 성능을 테스트` 하는데 사용되며 모델 선택의 기준이 됨\n",
    "- **테스트셋(Testing set):** 일반적으로 `전체 데이터의 20%`를 사용하며 최종 모델의 정확성을 확인하는 목적에 사용됨\n",
    "<center><img src='Image/Advanced/DataSplit_Simple.png' width='500'></center>\n",
    "\n",
    "**3) $K$교차검사($K$-fold Cross Validation):**  \n",
    "> (1) 훈련셋을 복원없이 `K`개로 분리한 후, `K-1`는 하위훈련셋으로 나머지 `1개`는 검증셋으로 사용함  \n",
    "> (2) 검증셋과 하위훈련셋을 `번갈아가면서` `K번 반복`하여 각 모델별로 `K`개의 `성능 추정치`를 계산  \n",
    "> (3) `K개의 성능 추정치 평균`을 최종 모델 성능 기준으로 사용  \n",
    "<center><img src='Image/Advanced/DataSplit_Kfold.png' width='500'></center>\n",
    "\n",
    "**4) $K$-fold vs. Random-subsamples vs. Leave-one-out vs. Leave-$p$-out**  \n",
    "- **$K$-fold**\n",
    "<center><img src='Image/Advanced/DataSplit_ver1.png' width='500'></center>\n",
    "\n",
    "- **Random-subsamples**\n",
    "<center><img src='Image/Advanced/DataSplit_ver2.png' width='500'></center>\n",
    "\n",
    "- **Leave-one-out**\n",
    "<center><img src='Image/Advanced/DataSplit_ver3.png' width='500'></center>\n",
    "\n",
    "- **Leave-$p$-out**\n",
    "<center><img src='Image/Advanced/DataSplit_ver4.png' width='500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 변환(Categorical Features)\n",
    "\n",
    "- **목표:** 컴퓨터와 알고리즘이 이해하도록 `숫자 형태로 변환`\n",
    "\n",
    "| **대분류** | **소분류** | **컴퓨터의 분류1** | **컴퓨터의 분류2** |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| **질적변수(Qualitative Variable)** | **-** | - | 범주형 |\n",
    "|  | **명목형 변수(Nominal Variable)** | 문자 | 범주형 |\n",
    "|  | **순위형 변수(Ordinal Variable)** | 숫자 | 범주형 |\n",
    "| **양적변수(Quantitative Variable)** | **-** | - | 연속형 |\n",
    "|  | **이산형 변수(Discrete Variable)** | 숫자 | 연속형 |\n",
    "|  | **연속형 변수(Continuous Variable)** | 숫자 | 연속형 |\n",
    "\n",
    "> - **문자형:** 일반적으로 유한개면 `범주형 숫자`, 무한개면 `연속형 숫자`로 변환\n",
    "> - **숫자형:** 해석 목적에 따라 `범주형->연속형` 또는 `연속형->범주형`으로 변환\n",
    "> - 연속형 변수들은 대부분 알고리즘에서 자동으로 처리됨\n",
    "> - `기계학습(Machine Learning)은 범주형 데이터를 처리`하는데서 출발\n",
    "\n",
    "---\n",
    "\n",
    "**1) Binning(구간화):** `연속형 변수를 범주형` 변수로 변환\n",
    "\n",
    "- 숫자로 구성된 `연속형 값이 넓을 경우` 그룹을 지어 이해도를 높임\n",
    "- 변수의 선형적 특성 이외에 `비선형적 특성을 반영`\n",
    "\n",
    "**2) Label Encoding:** `범주형 변수`의 값들을 `숫자 값(레이블)`로 변경\n",
    "\n",
    "<center><img src='Image/Advanced/Label_Encoding.png' width='250'></center>\n",
    "\n",
    "**3) Dummy Variable(가변수, $D_i$)**: `범주형 변수`를 `0 또는 1값`을 가진 `하나 이상의 새로운 변수`로 변경(One-hot Encoding)\n",
    "\n",
    "- **생성법:** `계절변수`가 봄/여름/가을/겨울 이라는 값을 포함하는 경우, `계절_봄`, `계절_여름`, `계절_가을`, `계절_겨울` 총 4개의 변수를 생성\n",
    "\n",
    "> (1) 범주형 변수의 `독립 값을 확인` (봄/여름/가을/겨울)\n",
    ">\n",
    "> (2) 독립 값의 `갯수만큼 더미변수`를 생성 ($D_1$ = 봄, $D_2$ = 여름, $D_3$ = 가을, $D_3$ = 겨울) \n",
    ">> *더미변수의 갯수는 최대 1개까지 줄일 수 있음*\n",
    ">\n",
    "> (3) 각 `더미변수들의 값`은 변수의 정의와 `같으면 1`이고 `나머지는 0`으로 채움   \n",
    "\n",
    "<center><img src='Image/Advanced/Dummy_Engineering.png' width='500'></center>\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/Dummy-variable-regression.jpg' width='400'></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [스케일 조정(Scaling)](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-download-auto-examples-preprocessing-plot-all-scaling-py)\n",
    "\n",
    "- **목적:** 변수들의 `크기를 일정하게 맞추어` `크기` 때문에 `영향`이 높은 현상을 회피\n",
    "\n",
    "> - **수학적:** 독립 변수의 공분산 `행렬 조건수(Condition Number)를 감소`시켜 최적화 안정성 및 수렴 속도 향상 \n",
    "> - **컴퓨터적:** PC 메모리를 고려하여 `오버플로우(Overflow)나 언더플로우(Underflow)를 줄여줌` \n",
    "    \n",
    "---\n",
    "\n",
    "**1) Standard Scaler:** <center>$\\dfrac{X_{it} - E(X_i)}{SD(X_i)}$</center>\n",
    "> - 기본 스케일로 평균을 제외하고 표준편차를 나누어 변환  \n",
    "> - 각 변수(Feature)가 `정규분포를 따른다는 가정`이기에 정규분포가 아닐 시 최선이 아닐 수 있음  \n",
    ">\n",
    "> ```python\n",
    "> sklearn.preprocessing.StandardScaler().fit()\n",
    "> sklearn.preprocessing.StandardScaler().transform()\n",
    "> sklearn.preprocessing.StandardScaler().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Advanced/Scaling_StandardScaler.png' width='500'></center>\n",
    "\n",
    "**2) Min-Max Scaler:** <center>$\\dfrac{X_{it} - min(X_i)}{max(X_i) - min(X_i)}$</center>\n",
    "> - 가장 많이 활용되는 방식으로 최소\\~최대 값이 `0~1` 또는 `-1~1` 사이의 값으로 변환  \n",
    "> - 각 변수(Feature)가 `정규분포가 아니거나 표준편차가 매우 작을 때` 효과적 \n",
    ">\n",
    "> ```python\n",
    "> sklearn.preprocessing.MinMaxScaler().fit()\n",
    "> sklearn.preprocessing.MinMaxScaler().transform()\n",
    "> sklearn.preprocessing.MinMaxScaler().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Advanced/Scaling_MinMaxScaler.png' width='500'></center>\n",
    "\n",
    "**3) Robust Scaler:** <center>$\\dfrac{X_{it} - Q_1(X_i)}{Q_3(X_i) - Q_1(X_i)}$</center>\n",
    "> - 최소-최대 스케일러와 유사하지만 `최소/최대 대신`에 IQR(Interquartile Range) 중 `25%값/75%값`을 사용하여 변환  \n",
    "> - 이상치(Outlier)에 영향을 최소화하였기에 `이상치가 있는 데이터에 효과적`이고 `적은 데이터에도 효과적`인 편  \n",
    ">\n",
    "> ```python\n",
    "> sklearn.preprocessing.RobustScaler().fit()\n",
    "> sklearn.preprocessing.RobustScaler().transform()\n",
    "> sklearn.preprocessing.RobustScaler().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Advanced/Scaling_RobustScaler.png' width='500'></center>\n",
    "\n",
    "**4) Normalizer:** <center>$\\dfrac{X_{it}}{\\sqrt{X_{i}^2 + X_{j}^2 + ... + X_{k}^2}}$</center>\n",
    "> - 각 변수(Feature)를 전체 `n`개 `모든 변수들의 크기들로 나누어`서 변환(by Cartesian Coordinates)  \n",
    "> - 각 변수들의 값은 `원점으로부터 반지름 1만큼 떨어진 범위 내`로 변환  \n",
    ">\n",
    "> ```python\n",
    "> sklearn.preprocessing.Normalizer().fit()\n",
    "> sklearn.preprocessing.Normalizer().transform()\n",
    "> sklearn.preprocessing.Normalizer().fit_transform()\n",
    "> ```\n",
    "\n",
    "<center><img src='Image/Advanced/Scaling_Normalizer.png' width='500'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고\n",
    "\n",
    "- **데이터 과학자들은 보통 `수동/자동 변수 처리 및 변환(Feature Engineering)`에 익숙하지만, `새로운 변수를 생성하는 것`은 분석에서 가장 중요하고 시간이 많이 걸리는 작업 중 하나이며, `머신러닝과 딥러닝의 발전`으로 점차 자동화 중**\n",
    "\n",
    "<center><img src='Image/Advanced/DL_AutoFE.png' width='700'></center>\n",
    "\n",
    "> **\"변수 생성시 주의할 점!\"**  \n",
    "> 1) 미래의 실제 `종속변수 예측값`이 `어떤 독립/종속변수의 Feature Engineering`에 의해 효과가 있을지 `단정할 수 없음`  \n",
    ">\n",
    "> 2) 독립변수의 예측값을 `Feature Engineering`를 통해 생성될 수 있지만 이는 종속변수의 `예측에 오류증가를 야기할 수` 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **함수세팅 및 추정 방향(Modeling):** Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝의 배경과 등장\n",
    "\n",
    "> **\"`인공지능(Artificial Intelligece)`은 여러 의미를 포괄하지만, 일반적으로 `기계학습(Machine Learning) + 딥러닝(Deep Learning)`을 의미\"**\n",
    "\n",
    "---\n",
    "\n",
    "**1) 배경:** 기존 사람과 프로그래밍 `접근방식 한계`\n",
    "\n",
    "- **예시:** 주어진 사진에서 고양이와 강아지를 판단하기\n",
    "\n",
    "<center><img src='Image/Advanced/ML_CatDog.png' width='800'>(https://codong.tistory.com/37?category=952287)</center>\n",
    "\n",
    "> - **사람:** `높은 정확도`로 `직관적`으로 분류 가능\n",
    ">\n",
    "> - **프로그래밍:** `수많은 특징`을 규칙으로 `사람이 모두 작성`하는 것은 `거의 불가능`\n",
    ">\n",
    "> ```python\n",
    "def prediction(이미지파일):\n",
    "    if 눈코귀가 있을 때:   \n",
    "        if 근데 강아지는 아닐때\n",
    "    if 털이 있고 꼬리 있을 때:\n",
    "        if 다른 동물이 아닐 때\n",
    "    ...\n",
    "    어캐하누...\n",
    "    return 결과\n",
    ">```\n",
    ">\n",
    "> - **프로그래밍 고도화:** 특징규칙들을 `수학적으로 고도화`해왔지만 `한계`\n",
    "\n",
    "---\n",
    "\n",
    "**2) 머신러닝의 등장:** `사람이 규칙을 일일이 정의 <<< 머신러닝으로 규칙을 탐색`\n",
    "\n",
    "> **\"데이터에 규칙을 적용시켜 `결과를 찾는 것`이 아니라 데이터와 결과를 학습하여 `규칙을 찾는 것`으로, 기존 프로그래밍 방식의 `한계를 해결`하게 된 새로운 계기\"**\n",
    "\n",
    "<center><img src='Image/Advanced/Programming_MachineLearning.png' width='600'></center>\n",
    "\n",
    "> - **학습(Learning):** `주어진 데이터`를 기계/컴퓨터에 `학습시켜 규칙성을 찾는 과정`\n",
    "> - 일반적으로 과거데이터를 학습하기 때문에 `훈련(Training)`이라고도 하며, 이렇게 발견된 규칙성으로 `새로운 미래데이터에 적용하여 정답을 추정(Testing)`\n",
    "> - 예를들어, `구글 번역기`는 사람이 직접 규칙을 정의한 것이 아니라, `딥러닝이 스스로 규칙을 찾아` 높은 성능으로 번역 수행\n",
    "\n",
    "---\n",
    "\n",
    "- **예시:**\n",
    "\n",
    "<center><img src='Image/Advanced/Programming_MachineLearningEx.png' width='600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀분석 작동방식으로 머신러닝 이해\n",
    "\n",
    "> **\"선형회귀분석을 포함하여 머신러닝과 딥러닝 등의 `모든 알고리즘은 큰 틀에서 작동방식이 동일`\"**\n",
    ">\n",
    "> **\"머신러닝과 딥러닝의 작동방식을 이해하기 위해 가장 기초 예측 알고리즘인 `선형회귀분석(Linear Regression)` 작동방식부터\"**\n",
    ">\n",
    "> **\"`선형회귀분석`을 포함한 대부분의 알고리즘은 큰 틀에서 `3가지 도구`를 사용하여 작동\"**\n",
    ">\n",
    "> **1)** `방정식(Equation) = 함수(Function) = 가설(Hypothesis)`\n",
    ">\n",
    "> **2)** `비용함수(Cost Function)`\n",
    ">\n",
    "> **3)** `옵티마이저(Optimizer)`\n",
    "\n",
    "---\n",
    "\n",
    "**0) 선형회귀분석(Linear Regression):** `어떤 변수 값`에 따라 `다른 변수 값이 영향`을 받는 관계성을 분석\n",
    "\n",
    "- **X(독립변수):** 다른 변수 값을 `변하게 하는 변수`\n",
    "- **Y(종속변수):** 변수 X에 의해서 종속되어 `변하는 변수`\n",
    "- **선형회귀분석:** 1개 이상의 `독립변수 X와 종속변수 Y의 관계`를 모델링\n",
    "\n",
    "| **관계로직** | **관계식** |\n",
    "|:---:|:---:|\n",
    "| `Y는 X와 같다` | $$Y = X$$ |\n",
    "| `Y는 초기값과 X들의 비율의 합과 같다` | $$Y = w_0 + w_1 X_1 + w_2 X_2 + \\cdots + w_k X_k$$ |\n",
    "\n",
    "> - `편향(Bias)` = `Y절편(Y-intercept)` = `초기값` = $w_0$\n",
    "> - `가중치(Weight)` = `기울기` = `비율` = $w_1, w_2, \\cdots, w_k$\n",
    "\n",
    "<center><img src='Image/Advanced/Regression-Constant.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1)** `방정식(Equation) = 함수(Function) = 가설(Hypothesis)`\n",
    "\n",
    "- **회귀문제:** `나이에 따라서 혈압`이 어떤 관련? / `공부 시간에 따라 성적`은 어떤 관련?\n",
    "- **가설(Hypothesis):** 머신러닝에서는 이런 `관련성을 표현한 관계식` \n",
    "\n",
    "| **종류** | **가설: $H(X)$** |\n",
    "|:---:|:---:|\n",
    "| `Multiple Linear Regression` | $$Y = w_0 + w_1 X_1 + w_2 X_2 + \\cdots + w_k X_k$$ |\n",
    "| `Simple Linear Regression` | $$Y = w_0 + w_1 X_1$$ |\n",
    "\n",
    "- **예시:** `나이에 따라서 혈압`이 어떤 관련?\n",
    "\n",
    "<center><img src='Image/Advanced/Example_LinearRegression.png' width='900'></center>\n",
    "\n",
    "<span style=\"color:red\">$\\Rightarrow$ **\"선형회귀분석은 주어진 데이터로부터 $Y \\text{와}X$의 관계를 가장 잘 나타내는 `직선을 찾는 것` 또는 `가설을 검증하는 것` 또는 `가중치와 편향을 추정하는 것`\"**</span>\n",
    "\n",
    "---\n",
    "\n",
    "**2)** `비용함수(Cost Function)`: 어떻게 `가설을 검증?` + `가중치와 편향을 추정?`\n",
    "\n",
    "- **아이디어:** 데이터를 사용하여 가설로부터 얻은 `예측값과 실제값의 차이를 최소화`하는 방향으로 `가설 또는 가중치와 편향을 업데이트`\n",
    "- **오차(Error):** 예측값과 실제값의 차이\n",
    "\n",
    "<center><img src='Image/Advanced/Cost_Function.png' width='400'>(임의 직선/가설/가중치에 따른 오차의 크기, https://wikidocs.net/21670)</center>\n",
    "\n",
    "- **오차를 반영한 관계식:** `비용함수(Cost Function) = 손실함수(Loss Function)` \n",
    "\n",
    "> - 단순히 `오차를 반영`하는 것 뿐만 아니라 `오차를 줄이는일에 최적화 된 식`\n",
    "> - 다양한 문제들마다 `적합한 비용함수들`이 있을 수 있음\n",
    "> - `회귀문제`의 경우 주로 `평균 제곱 오차(Mean Squared Error, MSE)`가 사용\n",
    ">\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "\\text{Cost Function} = \\sum_{i=1}^{m} \\left[\\sum_{j=0}^{k} (Y_{i} - w_{j}X_{j})^2 \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> - `오차의 크기`를 측정하기 위해 오차의 부호와 무관하도록 `제곱하여 더하여 절대적 크기 추정`\n",
    "> - 오차가 클수록 비용함수가 커지기 때문에, 결과적으로 `비용함수가 최소화되는 가중치와 편향`을 추정하면 $Y \\text{와}X$의 `관계를 가장 잘 나타내는 직선`\n",
    "\n",
    "<center><img src='Image/Advanced/Cost_Function_GDbyLine.png' width='400'></center>\n",
    "<center><img src='Image/Advanced/Cost_Function_GD.png' width='400'>(임의 직선/가설/가중치에 따른 오차의 크기 변화, https://realblack0.github.io/2020/03/27/linear-regression.html)</center>\n",
    "\n",
    "<span style=\"color:red\">$\\Rightarrow$ **\"선형회귀분석은 주어진 데이터로부터 $Y \\text{와}X$의 관계를 가장 잘 나타내는 `가중치와 편향을 추정`하기 위해 `비용함수를 최소화` 하면 모든 데이터에 `위치적으로 가장 가까운 직선`이 추정됨\"**</span>\n",
    "\n",
    "---\n",
    "\n",
    "**3)** `옵티마이저(Optimizer)`: 어떻게 `비용함수를 최소화?`\n",
    "\n",
    "- 선형회귀분석을 포함한 `수많은 머신러닝과 딥러닝 알고리즘`은 결국 비용함수를 최소화하는 작업\n",
    "- 비용함수를 최소화 시키는 알고리즘을 `최적화 알고리즘 = 옵티마이저(Optimizer)`\n",
    "- **학습(Learning):** 데이터로 비용함수를 추정하며 `최적화 또는 옵티마이저`를 통해 적절한 `가중치와 편향`을 찾아내는 과정\n",
    "- **경사하강법(Gradient Descent):** 가장 `기본적인` 옵티마이저\n",
    "\n",
    "> - 비용함수가 `가장 최소값`을 갖게 하는 `가중치` $W$를 찾는 알고리즘\n",
    "> - 임의의 `기울기(Gradient) = 미분값` $W_0$에서 시작하여 기울기를 낮추다가 0이 될 때까지 `가중치 업데이트`하기에 `Gradient Descent`\n",
    "\n",
    "<center><img src='Image/Advanced/Cost_flow2d.png' width='500'></center>\n",
    "\n",
    "> - **수학적 추정 과정:** 틀린 초기 가중치를 `여러번 반복해서 업데이트`\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_1 &:= W_0 - \\alpha \\frac{\\partial}{\\partial w} \\left[ \\text{Cost Function} \\right] \\\\ &= W_0 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_0} \\\\\n",
    "W_2 &:= W_1 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_1} \\\\\n",
    "W_3 &:= W_2 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_2} \\\\\n",
    "& \\vdots \\\\\n",
    "W &:= W - \\alpha \\frac{\\partial C(W)}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> - **$\\alpha$:** `가중치를 업데이트 하는 속도`로 학습률(Learning Rate)이라고 하며, 속도가 `빠르면 정확성이 낮아질` 수 있고 속도가 `느리면 오래걸리기`에 적당한 수치 필요\n",
    "\n",
    "---\n",
    "\n",
    "> **\"기울기가 `가장작은/0인 곳`을 `한번에/수학적으로` 안찾고, 왜 결국 `틀린 가중치를 여러번 반복`해서 찾으며 업데이트하나요?\"**\n",
    ">\n",
    "> - 수학적으로 `비용함수를 미분하고 0인 지점`을 찾으면 되지만 `모든 경우 가능하진 않음`\n",
    "> - 현실문제에 적합한 비용함수는 `알수가 없는 편이고 창의적인 영역` + `기울기(미분)를 꼭 계산할 수 있지 않음`\n",
    "> - 사람은 반복이 귀찮지만 컴퓨터/기계는 반복이 쉽고 `틀린 가중치를 반복적으로 업데이트하는 것`이 완벽한 정답은 아니지만 `정답에 가까운 근사치`이며 `정확성을 별도 추정`하여 신뢰도 의사결정 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀문제 해결을 위한 가설 및 비용함수\n",
    "\n",
    "**1) 알고리즘 함수세팅:** \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) = w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \n",
    "= [w_0~w_1~w_2~\\cdots~w_k]\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix} \\\\\n",
    "&= [1~X_1~X_2~\\cdots~X_k]\\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_k \\end{bmatrix} \n",
    "= \\begin{bmatrix} 1~X_{11}~X_{21}~\\cdots~X_{k1} \\\\ 1~X_{12}~X_{22}~\\cdots~X_{k2} \\\\ \\vdots \\\\ 1~X_{1t}~X_{2t}~\\cdots~X_{kt} \\end{bmatrix}\n",
    "\\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_k \\end{bmatrix} = XW = WX\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/Example_LinearRegression.png' width='900'></center>\n",
    "\n",
    "**2) 함수 추정을 위한 비용함수:** 나의 주장 기반 알고리즘의 `예측값`($\\hat{Y}$)과 `실제 데이터`($Y$)의 차이를 평가하는 함수\n",
    "> - **손실함수(Loss Function):** `하나의 데이터(Single Row)`에서 예측값과 정답의 차이를 평가\n",
    "> - **비용함수(Cost Function):** `모든 데이터`에서 예측값과 정답의 차이를 평가\n",
    "> \n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Y - \\hat{Y} &= Y - WX = \\text{residual} = \\text{cost} \\\\\n",
    "&= \\sum_{i=1}^{m} \\left[ \\sum_{j=1}^{k} (Y_{i} - w_{j}X_{j}) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    ">\n",
    "> - `회귀분석`은 여러가지의 비용함수 중 `최소제곱법/최소자승법`을 사용 \n",
    "> - `최소제곱법/최소자승법`를 최소로 하는 `직선`을 추정하여 `계수(coefficient)`를 결정\n",
    ">\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{W} = \\underset{W}{\\arg\\min} \\sum_{i=1}^{m} \\left[\\sum_{j=1}^{k} (Y_{i} - w_{j}X_{j})^2 \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<!-- - **비선형변수 효과:** 로그 또는 제곱근 등의 변환된 변수 사용시 회귀분석 성능 향상 가능\n",
    "    - 독립 변수나 종속 변수가 심하게 한쪽으로 치우친 분포를 보이는 경우\n",
    "    - 독립 변수와 종속 변수간의 관계가 곱셈 혹은 나눗셉으로 연결된 경우\n",
    "    - 종속 변수와 예측치가 비선형 관계를 보이는 경우 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **결정론적 모형(Deterministic Model):** 수학적 모형\n",
    "> **\"잔차제곱합(Residual Sum of Squares)을 최소로하는 $W$를 추정\"**  \n",
    "\n",
    "**1) 잔차벡터(Residual Vector):**\n",
    "$$\\epsilon = Y - \\hat{Y} = Y - XW$$\n",
    "\n",
    "**2) 잔차제곱합(Residual Sum of Squares):** \n",
    "$$\n",
    "\\begin{aligned}\n",
    "RSS &= \\epsilon^T\\epsilon = (Y - XW)^T(Y - XW) \\\\\n",
    "&= Y^TY-2Y^TXW+W^TX^TXW\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**3) 잔차제곱합의 기울기 추정:** 그레디언트(미분, 기울기)\n",
    "$$\\dfrac{dRSS}{dW} = -2X^TY + 2X^TXW$$\n",
    "\n",
    "**4) 잔차제곱합이 최소가 되는 계수는 그레디언트가 0이 되는 곳:** 최적화 알고리즘의 작동원리\n",
    "$$\\dfrac{dRSS}{dW} = 0$$\n",
    "\n",
    "**5) 최적화 실행:** \n",
    "$$\n",
    "\\dfrac{dRSS}{dW} = -2X^TY + 2X^TXW = 0 \\\\\n",
    "X^TXW = X^TY\n",
    "$$\n",
    "\n",
    "**6) 추정 계수:** \n",
    "$$W = (X^TX)^{-1}X^TY$$\n",
    "\n",
    "---\n",
    "\n",
    "- **특징:**\n",
    "> - $X^TX$ 행렬의 `역행렬이 존재해야` 해 추정/존재 가능  \n",
    "> - `역행렬이 미존재`  \n",
    "> = $X$가 서로 `독립이 아님`  \n",
    "> = $X$가 Full Rank가 아님  \n",
    "> = $X^TX$가 양의 정부호(Positive Definite)가 아님\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **확률론적 모형(Probabilistic Model):** 통계적 모형\n",
    "\n",
    "> **\"종속변수의 발생가능성을 최대(최소)로하는 $W$를 추정\"**\n",
    "> - **필요성:** 결정론적 방식은 데이터의 확률적 가정이 없기에 `1회성으로 가중치를 추정(점추정)`하나, 이 `반복추정으로 가중치가 발생할 범위(구간추정)는 알 수 없음`\n",
    ">\n",
    "> <center><img src='Image/Advanced/Example_Interval_Estimation.png' width='500'></center>\n",
    ">\n",
    "> - **예시:** 집값에 대한 범죄율 영향력(가중치)이 `-1.08`이라면, 범죄율이 높은 곳은 집값이 떨어진다 결론 내릴 수 있을까?\n",
    ">> - `-1.08`는 1회성 결과일 뿐 오차가 존재\n",
    ">> - 만약 오차가 `0.1`이라면 실제 `영향력의 추정 범위(신뢰구간)`는 -1.08$\\pm$0.1 (`-1.18 ~ -0.98`)이기에 범죄율이 높은 곳은 집값이 떨어진다 결론 가능\n",
    ">> - 만약 오차가 `2`라면 실제 `영향력의 추정 점위(신뢰구간)`는 -1.08$\\pm$2 (`-3.08 ~ 0.92`)이기에 범죄율이 높은 곳은 집값이 떨어질수도 오를수도 있다 결론 가능\n",
    "\n",
    "---\n",
    "\n",
    "**0) 실제 Y추정값의 확률론적 표현:** \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Main Equation} && Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) \\\\\n",
    "&& &= w_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&& &= E(Y|X_1, X_2, ... , X_k) \\\\ \n",
    "&& &\\sim \\mathcal{N}(X W, \\sigma^2) \\\\\n",
    "&& Pr(Y \\mid X, \\theta) &= \\mathcal{N}(y \\mid X W, \\sigma^2 ) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "\\text{Error Poperties} && p(\\epsilon \\mid \\theta) &= \\mathcal{N}(0, \\sigma^2 ) \\text{  from  } \\epsilon = Y - X W \\\\\n",
    "&& \\text{E}(\\epsilon \\mid X) &= 0 \\\\\n",
    "&& \\text{E}(\\epsilon) &= \\text{E}(\\text{E}(\\epsilon \\mid X)) = 0 \\\\\n",
    "&& \\text{E}(\\epsilon X) &= \\text{E}(\\text{E}(\\epsilon X \\mid X)) = \\text{E}(X \\text{E}(\\epsilon\\mid X)) = 0 \\\\\n",
    "&& \\text{E}(\\epsilon^2) &= \\sigma^2 (N-K) \\\\\n",
    "&& \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) &= 0 \\;\\; (i,j=1,2,\\ldots,N)\n",
    "\\end{align*}\n",
    "\n",
    "- **Summary:**\n",
    "    - $X, Y$ 중 어느 것도 정규분포일 필요는 없음  \n",
    "    - $Y$는 $X$에 대해 조건부로 정규분포를 따르며 $Y$자체가 무조건부로 정규분포일 필요는 없음  \n",
    "    - 잔차의 기대값은 0  \n",
    "    - 잔차의 조건부 기대값은 0  \n",
    "    - 잔차와 독립변수 $X$는 상관관계 없음  \n",
    "    - $X$와 무관하게 잔차들간의 공분산은 0   -->\n",
    "    \n",
    "**1) 실제 Y값의 추정가능성(Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "Pr(Y_{i} \\,\\big|\\, X_{i}, \\theta) &= \\prod_{i=1}^N \\mathcal{N}(Y_i \\,\\big|\\, X_i w_i , \\sigma^2) \\\\\n",
    "&= \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(Y_i- X_i w_i)^2}{2\\sigma^2} \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "**2) 추정가능성의 더하기 표시 변환을 위한 Log함수 적용(Log-Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{LL} &= \\log Pr(Y_{i} \\,\\big|\\, X_{i}, \\theta) \\\\\n",
    "&= \\log \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(Y_i-X_i w_i)^2}{2\\sigma^2} \\right\\}  \\\\\n",
    "&= -\\dfrac{1}{2\\sigma^2} \\sum_{i=1}^N (Y_i-X_i w_i)^2 - \\dfrac{N}{2} \\log{2\\pi}{\\sigma^2}  \\\\\n",
    "\\text{LL(Matrix Form)} &= -C_1 (Y - XW)^T(y-XW) - C_0 \\\\\n",
    "&= -C_1(W^TX^TXW -2 Y^TXW + Y^TY) - C_0 \\\\\n",
    "& \\text{where } C_1=  \\dfrac{1}{2\\sigma^2}, C_0 =  \\dfrac{N}{2} \\log{2\\pi}{\\sigma^2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**3) Log-Likelihood의 음수화 및 그레디언트가 0이 되는 곳:** \n",
    "\n",
    "\\begin{align*}\n",
    "-\\dfrac{d}{dW} \\text{LL} &= C_1 \\left( 2X^TXW - 2X^TY \\right) = 0 \\\\\n",
    "W &= (X^TX)^{-1}X^T Y \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "- **특징:**\n",
    "> - $X^TX$ 행렬의 `역행렬이 존재해야` 해 추정/존재 가능  \n",
    "> - `역행렬이 미존재`  \n",
    "> = $X$가 서로 `독립이 아님`  \n",
    "> = $X$가 Full Rank가 아님  \n",
    "> = $X^TX$가 양의 정부호(Positive Definite)가 아님    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수학통계적 vs 머신러닝기계\n",
    "\n",
    "<center><img src='Image/Advanced/Example_LinearRegression.png' width='900'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:32.581813Z",
     "iopub.status.busy": "2025-03-01T12:56:32.581813Z",
     "iopub.status.idle": "2025-03-01T12:56:32.585267Z",
     "shell.execute_reply": "2025-03-01T12:56:32.585267Z",
     "shell.execute_reply.started": "2025-03-01T12:56:32.581813Z"
    }
   },
   "outputs": [],
   "source": [
    "# 분석 라이브러리 불러오기\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 'always'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:32.587275Z",
     "iopub.status.busy": "2025-03-01T12:56:32.586275Z",
     "iopub.status.idle": "2025-03-01T12:56:32.611248Z",
     "shell.execute_reply": "2025-03-01T12:56:32.611248Z",
     "shell.execute_reply.started": "2025-03-01T12:56:32.587275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BloodPressure_Max</th>\n",
       "      <th>Constant</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BloodPressure_Max  Constant  Age\n",
       "0                  118         1    8\n",
       "1                  134         1   50\n",
       "2                  129         1   55\n",
       "3                  137         1   30\n",
       "4                   92         1   45\n",
       "..                 ...       ...  ...\n",
       "995                117         1   60\n",
       "996                122         1   60\n",
       "997                104         1   55\n",
       "998                133         1   70\n",
       "999                121         1   30\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 데이터\n",
    "df = pd.read_csv(r'.\\Data\\MedicalCheckup\\hme.csv')\n",
    "Y = df[['BloodPressure_Max']]\n",
    "X = df[['Age']]\n",
    "X.loc[:,'Constant'] = 1\n",
    "X = X[['Constant', 'Age']]\n",
    "pd.concat([Y, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:32.612254Z",
     "iopub.status.busy": "2025-03-01T12:56:32.612254Z",
     "iopub.status.idle": "2025-03-01T12:56:32.621408Z",
     "shell.execute_reply": "2025-03-01T12:56:32.621408Z",
     "shell.execute_reply.started": "2025-03-01T12:56:32.612254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>w0</th>\n",
       "      <td>108.671945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w1</th>\n",
       "      <td>0.259873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        weight\n",
       "w0  108.671945\n",
       "w1    0.259873"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수학통계적 가중치 계산\n",
    "# 관계식이 복잡해진다면? 비용함수가 달라진다면? 일반화 가능한가?\n",
    "# 매번 손으로 기계 대신 미분을 할 수 있는가?\n",
    "weight = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), Y)\n",
    "weight.columns = ['weight']\n",
    "weight.index = ['w0', 'w1']\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-01T12:56:32.622415Z",
     "iopub.status.busy": "2025-03-01T12:56:32.622415Z",
     "iopub.status.idle": "2025-03-01T12:56:32.642346Z",
     "shell.execute_reply": "2025-03-01T12:56:32.642346Z",
     "shell.execute_reply.started": "2025-03-01T12:56:32.622415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>BloodPressure_Max</td> <th>  R-squared:         </th> <td>   0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   75.08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 01 Mar 2025</td>  <th>  Prob (F-statistic):</th> <td>1.80e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:56:32</td>      <th>  Log-Likelihood:    </th> <td> -4014.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>       <th>  AIC:               </th> <td>   8032.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   998</td>       <th>  BIC:               </th> <td>   8042.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Constant</th> <td>  108.6719</td> <td>    1.638</td> <td>   66.343</td> <td> 0.000</td> <td>  105.458</td> <td>  111.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Age</th>      <td>    0.2599</td> <td>    0.030</td> <td>    8.665</td> <td> 0.000</td> <td>    0.201</td> <td>    0.319</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>51.299</td> <th>  Durbin-Watson:     </th> <td>   1.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  64.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.497</td> <th>  Prob(JB):          </th> <td>1.21e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.742</td> <th>  Cond. No.          </th> <td>    211.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    & BloodPressure\\_Max & \\textbf{  R-squared:         } &     0.070   \\\\\n",
       "\\textbf{Model:}            &        OLS         & \\textbf{  Adj. R-squared:    } &     0.069   \\\\\n",
       "\\textbf{Method:}           &   Least Squares    & \\textbf{  F-statistic:       } &     75.08   \\\\\n",
       "\\textbf{Date:}             &  Sat, 01 Mar 2025  & \\textbf{  Prob (F-statistic):} &  1.80e-17   \\\\\n",
       "\\textbf{Time:}             &      21:56:32      & \\textbf{  Log-Likelihood:    } &   -4014.1   \\\\\n",
       "\\textbf{No. Observations:} &         1000       & \\textbf{  AIC:               } &     8032.   \\\\\n",
       "\\textbf{Df Residuals:}     &          998       & \\textbf{  BIC:               } &     8042.   \\\\\n",
       "\\textbf{Df Model:}         &            1       & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &     nonrobust      & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Constant} &     108.6719  &        1.638     &    66.343  &         0.000        &      105.458    &      111.886     \\\\\n",
       "\\textbf{Age}      &       0.2599  &        0.030     &     8.665  &         0.000        &        0.201    &        0.319     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 51.299 & \\textbf{  Durbin-Watson:     } &    1.942  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   64.091  \\\\\n",
       "\\textbf{Skew:}          &  0.497 & \\textbf{  Prob(JB):          } & 1.21e-14  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.742 & \\textbf{  Cond. No.          } &     211.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:      BloodPressure_Max   R-squared:                       0.070\n",
       "Model:                            OLS   Adj. R-squared:                  0.069\n",
       "Method:                 Least Squares   F-statistic:                     75.08\n",
       "Date:                Sat, 01 Mar 2025   Prob (F-statistic):           1.80e-17\n",
       "Time:                        21:56:32   Log-Likelihood:                -4014.1\n",
       "No. Observations:                1000   AIC:                             8032.\n",
       "Df Residuals:                     998   BIC:                             8042.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Constant     108.6719      1.638     66.343      0.000     105.458     111.886\n",
       "Age            0.2599      0.030      8.665      0.000       0.201       0.319\n",
       "==============================================================================\n",
       "Omnibus:                       51.299   Durbin-Watson:                   1.942\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               64.091\n",
       "Skew:                           0.497   Prob(JB):                     1.21e-14\n",
       "Kurtosis:                       3.742   Cond. No.                         211.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 머신러닝기계로 가중치 계산\n",
    "model = sm.OLS(Y, X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **검증지표 방향(Evaluation Metrics)**\n",
    "\n",
    "<center><img src='Image/Advanced/Analysis_Process.png' width='800'></center>\n",
    "\n",
    "> **\"`문제해결 검증지표`와 `알고리즘 검증지표`는 같을 수 있으나 대부분은 다른 편\"**\n",
    ">\n",
    "> **(1) 문제해결 검증지표:** `실제 문제`를 잘 해결하는지 평가 `(3단계)`\n",
    ">\n",
    "> **(2) 알고리즘 검증지표:** `데이터의 패턴`이 잘 추출되고 `예측의 정확성`을 평가 `(2단계)`\n",
    ">\n",
    "> - `알고리즘 성능`이 좋은것과 `문제해결`이 가능한 것은 다르기 때문에 문제해결 지표와 알고리즘 지표는 대부분은 다른 편\n",
    "> - 알고리즘 검증지표는 없어도 되지만 `문제해결 검증지표는 반드시 필요`    \n",
    "> - `(이론적)알고리즘`들은 일반적으로 특정 `알고리즘 검증지표`를 향상시키는 방향으로 개발됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대표적인 검증지표\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/DataSplit_Concept1.png' width='700'></center>\n",
    "<center><img src='Image/Advanced/DataSplit_Concept2.png' width='700'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) 문제별 종류:**\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_Metric_Types.png' width='600'></center>\n",
    "\n",
    "> - **Statistical Metrics:** `Correlation`\n",
    ">> - 입력(Input): `-무한대 ~ 무한대` 범위의 연속형 값\n",
    ">> - 출력(Output): 이론적으론 `-1 ~ 1` 범위의 연속형 값\n",
    "> - **Regression Metrics:** `MSE, MSPE, RMSE, RMSLE, MAE, MAPE, MPE, R^2, Adjusted R^&2, ...` (Y의 범위가 무한대가 가능한 연속형일때)\n",
    ">> - 입력(Input): `-무한대 ~ 무한대` 범위의 연속형 값\n",
    ">> - 출력(Output): 이론적으론 `0 ~ 무한대` 범위의 연속형 값\n",
    "> - **Classification Metrics:** `Log Loss, Cross-entropy, ROC, AUC, Gini, Confusion Matrix, Accuracy, Precision, Recall, F1-score, Classification Report, KS Statistic, Concordant-Discordant Ratio, (ARI, NMI, AMI), ...` (Y가 2개 또는 그 이상개수의 이산형일때)\n",
    ">> - 입력(Input): `-무한대 ~ 무한대` 범위의 연속형 값\n",
    ">> - 출력(Output): 알고리즘 종류에 따라 출력이 달라질 수 있음\n",
    ">>> - 확률(Probability): `0 ~ 1` 범위의 연속형 값 (Logistic Regression, Random Forest, Gradient Boosting, Adaboost, ...)\n",
    ">>> - 집단(Class): `0 또는 1`의 이산형 값 (SVM, KNN, ...)\n",
    "> - **Clustering:** `Dunn Index, Silhouette, ...`\n",
    "> - **Ranking Metrics:** `Gain, Lift, MRR, DCG, NDCG, ...`\n",
    "> - **Computer Vision Metrics:** `PSNR, SSIM, IoU, ...`\n",
    "> - **NLP Metrics:** `Perplexity, BLEU score, ...`\n",
    "> - **Deep Learning Related Metrics:** `Inception score, Frechet Inception distance, ...`\n",
    "> - **Real Problem:** `???`\n",
    "\n",
    "---\n",
    "\n",
    "**2) 검증지표 성능의 종류:** 데이터/분석은 `높은 정확도`를 낳거나 `높은 에러`를 발생시킴\n",
    "> - **높은정확도(High Accuracy):** `과거 패턴`이 미래에도 그대로 유지가 된다면 예측 정확도가 높아짐  \n",
    "> - **높은에러(High Error):** `패턴이 점차적으로 또는 갑자기 변경되면` 예측값은 실제값에서 크게 벗어날 수 있음  \n",
    ">> - **Black Swan:** <U>일어날 것 같지 않은 일이 일어나는 현상</U>\n",
    ">> - **White Swan:** <U>과거 경험들로 충분히 예상되는 위기지만 대응책이 없고 반복될 현상</U>\n",
    ">> - **Gray Swan:** <U>과거 경험들로 충분히 예상되지만 발생되면 충격이 지속되는 현상</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀분석 검증지표 및 해석하기\n",
    "\n",
    "**1) 예측문제 검증지표:** `MSE, MSPE, RMSE, RMSLE, MAE, MAPE, MPE, R^2, Adjusted R^&2, ...` (Y의 범위가 무한대가 가능한 연속형일때)\n",
    "\n",
    "<center><img src='Image/Advanced/Evaluation_Metric1.jpg' width='300'></center>  \n",
    "    \n",
    "<center><img src='Image/Advanced/Evaluation_Metric2.jpg' width='300'>(Mean Absolute Error)</center>  \n",
    "    \n",
    "<center><img src='Image/Advanced/Evaluation_Metric3.jpg' width='300'>(Mean Squared Error)</center>  \n",
    "    \n",
    "<center><img src='Image/Advanced/Evaluation_Metric4.jpg' width='300'>(Mean Absolute Percentage Error)</center>  \n",
    "    \n",
    "<center><img src='Image/Advanced/Evaluation_Metric5.jpg' width='250'>(Mean Percentage Error)</center>\n",
    "\n",
    "- **사용예시:** [Comparison of Algorithm Performance Metrics](https://www.researchgate.net/figure/Model-performances-based-on-the-MAE-MAPE-RMSE-MSE-and-R-2-metrics-Italic-and-bold_tbl3_354075433)\n",
    "\n",
    "---\n",
    "\n",
    "**2) 회귀분석 알고리즘 적합/추정 후 결과지표:** `추정성능(빨간색)` + `잔차진단(보라색)`\n",
    "\n",
    "<center><img src='Image/Advanced/Statmodels_OLS_Result2.png' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**2-1) R-squared(R^2):** 추정된 모형이 데이터에 잘 적합된 정도, $(- \\infty, 1]$  \n",
    "\n",
    "> **\"`TSS = ESS + RSS`\"**\n",
    "> - **TSS(Total Sum of Squares):** `실제 종속변수` $Y$의 움직임 범위\n",
    "> - **ESS(Explained Sum of Squares):** `예측된 종속변수` $\\hat{Y}$의 움직임 범위\n",
    "> - **RSS(Residual Sum of Squares):** `잔차` $\\epsilon$의 움직임 범위\n",
    ">> - **\"예측값의 움직임 범위 <<< 실제 움직임 범위보다 클 수 없음\"**\n",
    ">> - **\"예측 성능이 좋을수록 예측값의 움직임 범위는 실제 움직임 범위와 비슷해짐\"**\n",
    "\n",
    "<center><img src='Image/Advanced/R2_decomposition1.png' width='600'></center>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R^2 &= \\dfrac{ESS}{TSS} = \\dfrac{\\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\\\\n",
    "&= \\text{종속변수 움직임 범위 대비 예측변수 움직임 범위도 비슷할수록 굳} \\\\\n",
    "&= 1 - \\dfrac{RSS}{TSS} = 1 - \\dfrac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\\\\n",
    "&= \\text{잔차 움직임 범위가 작을수록 굳}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<center><img src='Image/Advanced/R2_decomposition2.png' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**2-2) t-검정:** 추정계수가 `t분포`의 움직임을 보이기 때문에, `t분포` 기반 독립변수와 종속변수 간의 `영향력정도 의사결정`\n",
    "\n",
    "- **추정계수의 분포:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Main Equation} && \\hat{W} &= (X^TX)^{-1} X^T Y \\\\\n",
    "&& &= (X^TX)^{-1} X^T (X W + \\epsilon) \\\\\n",
    "&& &= W + (X^TX)^{-1} X^T \\epsilon \\\\\n",
    "\\text{Expectation} && \\text{E}(\\hat{W}) &=  \\text{E}( W + (X^TX)^{-1} X^T \\epsilon ) \\\\\n",
    "&& &=  W + (X^TX)^{-1} X^T \\text{E}( \\epsilon ) \\\\\n",
    "&& &= W \\\\\n",
    "\\text{Variance} && \\text{Var}(\\hat{W}_i)  &= \\left( \\text{Cov}(\\hat{W}) \\right)_{ii} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Covariance} && \\text{Cov}(\\hat{W}) &= E\\left((\\hat{W} - W)(\\hat{W} - W)^T \\right) \\\\\n",
    "&& &= E\\left(((X^TX)^{-1} X^T \\epsilon)((X^TX)^{-1} X^T \\epsilon)^T \\right) \\\\\n",
    "&& &= E\\left((X^TX)^{-1} X^T \\epsilon \\epsilon^T X(X^TX)^{−1} \\right) \\\\\n",
    "&& &= (X^TX)^{-1} X^T E(\\epsilon \\epsilon^T) X(X^TX)^{−1} \\\\\n",
    "&& &= (X^TX)^{-1} X^T (\\sigma^2 I) X(X^TX)^{−1} \\\\\n",
    "&& &= \\sigma^2  (X^TX)^{-1} \\\\\n",
    "\\text{Standard Deviation} && \\sqrt{\\text{Var}(\\hat{W}_i)} \\approx {se_{\\hat{W}_i}} &= \\sqrt{\\sigma^2 \\big((X^TX)^{-1}\\big)_{ii}} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Asymptotic} && \\dfrac{\\hat{W}_i - W_i}{se_{\\hat{W}_i}} &\\sim t_{N-K} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "- **검정통계량(t-통계량) 의미:**\n",
    "\n",
    "$$t = \\dfrac{\\hat{W}_i - W_i}{se_{\\hat{W}_i}} = \\dfrac{\\hat{W}_i - 0}{se_{\\hat{W}_i}} = \\dfrac{\\hat{W}_i}{se_{\\hat{W}_i}}$$\n",
    "    \n",
    "> - $t$ 값이 `작으면`, 독립변수와 종속변수의 상관성이 `없다`  \n",
    "> - $t$ 값이 `크면`, 독립변수와 종속변수의 상관성이 `있다`\n",
    "\n",
    "- **의사결정:**\n",
    "\n",
    "> **(1) 가설확인:**\n",
    ">\n",
    "> | **종류** | **해석** |\n",
    "|:---:|:---|\n",
    "| **대중주장<br>(귀무가설, Null Hypothesis, $H_0$)** | 독립변수와 종속변수의 상관관계(선형관계)가 `없다` |\n",
    "| **나의주장<br>(대립가설, Alternative   Hypothesis, $H_1$)** | 독립변수와 종속변수의 상관관계(선형관계)가 `있다` |\n",
    ">\n",
    ">\n",
    "> **(2) 유의수준 설정 및 유의확률 확인**\n",
    "> - 유의수준: 5% (0.05) 분석가가 알아서 결정\n",
    "> - 유의확률(p-value): 컴퓨터가 알아서 추정\n",
    ">\n",
    "> **(3) 의사결정**\n",
    ">\n",
    "> | **기준** | **의사결정** | **해석** |\n",
    "|:---:|:---:|:---|\n",
    "| **p-value >= 유의수준(ex. 0.05)** | `대중주장 참` | 독립변수와 종속변수의 상관관계(선형관계)가 `없다` <br> 분석한 변수는 모델링에 영향력이 `없다` |\n",
    "| **p-value < 유의수준(ex. 0.05)** | `나의주장 참` | 독립변수와 종속변수의 상관관계(선형관계)가 `있다` <br> 분석한 변수는 모델링에 영향력이 `있다` |\n",
    "\n",
    "---\n",
    "\n",
    "**2-3) F검정:** `전체 계수`에 대한 $ESS/RSS$가 `F분포`의 움직임을 보이기 때문에, `F분포` 기반 독립변수와 종속변수 간의 `알고리즘 신뢰성 의사결정`\n",
    "\n",
    "- **필요성:** \n",
    "> - `변수의 갯수`와 크기가 커지면 잔차제곱합(Residula Sum of Square)이 `무조건 감소`\n",
    "> - `분산 분석(Analysis of Variance(ANOVA))`은 종속변수의 분산과 모든 독립변수의 분산간의 관계를 사용하여 `F분포` 기준 알고리즘 성능 평가  \n",
    "\n",
    "- **검정통계량(F-통계량):** `분산분석표(ANOVA Table)`를 통해 쉽게 계산되며, $T$는 데이터의 갯수, $K$는 변수의 갯수\n",
    "\n",
    "$$\\dfrac{ESS}{K-1} \\div \\dfrac{RSS}{T-K} \\sim F(K-1, T-K)$$\n",
    "\n",
    "| Source | Degree of Freedom | Sum of Square | Mean Square | F test-statstics | p-value |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| Estimation | $$K-1$$ | $$ESS$$ | $$\\sigma_{\\hat{Y}}^2 = \\dfrac{ESS}{K-1}$$ | $$F = \\dfrac{\\sigma_{\\hat{Y}}^2}{\\sigma_{\\epsilon}^2}$$ | $$p-value$$ |\n",
    "| Residual | $$T-K$$ | $$RSS$$ | $$\\sigma_{\\epsilon}^2 = \\dfrac{RSS}{T-K}$$ |  |  |\n",
    "| Total | $$T-1$$ | $$TSS$$ | $$\\sigma_{Y}^2 = \\dfrac{TSS}{T-1}$$ |  |  |\n",
    "| $$R^2$$ |  | $$\\dfrac{ESS}{TSS}$$ |  |  |  |\n",
    "\n",
    "- **의사결정:**\n",
    "\n",
    "> **(1) 가설확인:**\n",
    ">\n",
    "> | **종류** | **해석** |\n",
    "|:---:|:---|\n",
    "| **대중주장<br>(귀무가설, Null Hypothesis, $H_0$)** | 모형은 아무 효과가 `없다` <br> $$W_0 = W_1 = \\cdots = W_{K-1} = 0$$ <br> $$R^2 = 0$$ |\n",
    "| **나의주장<br>(대립가설, Alternative   Hypothesis, $H_1$)** | 모형은 효과가 `있다` <br> $$W_0  \\neq W_1 \\neq \\cdots \\neq W_{K-1} \\neq 0$$ <br> $$R^2 \\neq 0$$ |\n",
    ">\n",
    ">\n",
    "> **(2) 유의수준 설정 및 유의확률 확인**\n",
    "> - 유의수준: 5% (0.05) 분석가가 알아서 결정\n",
    "> - 유의확률(p-value): 컴퓨터가 알아서 추정\n",
    ">\n",
    "> **(3) 의사결정**\n",
    ">\n",
    "> | **기준** | **의사결정** | **해석** |\n",
    "|:---:|:---:|:---|\n",
    "| **p-value >= 유의수준(ex. 0.05)** | `대중주장 참` | 분석한 모델링은 효과가 `없다` <br> 모델은 데이터 패턴을 잘 `추정하지 못한다` |\n",
    "| **p-value < 유의수준(ex. 0.05)** | `나의주장 참` | 분석한 모델링은 효과가 `있다` <br> 모델은 데이터 패턴을 잘 `추정한다` |\n",
    " \n",
    "---\n",
    "\n",
    "**2-4) 정보량기준(Information Criterion):** `회귀분석` 외에도 `다양한 알고리즘`에 활용, `값이 작을수록 좋은 모형결과` (`Likelihood`는 `클수록 좋은 모형결과`)\n",
    "\n",
    "> - **[AIC(Akaike Information Criterion)](https://en.wikipedia.org/wiki/Akaike_information_criterion)**  \n",
    ">\n",
    "> : 모형과 데이터 확률분포의 Kullback-Leibler 수준을 가장 크게하기 위한 시도 \n",
    ">\n",
    "> $$AIC = -2log(L) + 2K$$\n",
    ">\n",
    "> <center>($L$: likelihood, $K$: 추정할 파라미터의 수(column수))</center>\n",
    "\n",
    "> - **[BIC(Bayesian Information Criterion)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)**  \n",
    ">\n",
    "> : 데이터가 exponential family라는 가정하에 데이터에서 모형의 likelihood를 측정하기 위한 값에서 유도 \n",
    ">\n",
    "> $$BIC = -2log(L) + Klog(T)$$\n",
    ">\n",
    "> <center>($L$: likelihood, $K$: 추정할 파라미터의 수(column수), $T$: 데이터의 수(row수))</center>\n",
    "\n",
    "> - **사용 예시:**\n",
    ">\n",
    "> <center><img src='Image/Advanced/AIC_BIC.gif' width='600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **잔차진단 방향(Residual Diagnostics)**\n",
    "\n",
    "- **회귀분석 알고리즘 적합/추정 후 결과지표:** `추정성능(빨간색)` + `잔차진단(보라색)`\n",
    "\n",
    "<center><img src='Image/Advanced/Statmodels_OLS_Result2.png' width='600'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**0) 검증지표 vs 잔차진단:** Y수치 비교가 가능한 `예측문제`에서 Y라벨이 있는 `지도학습`에 적용 가능\n",
    "\n",
    "<br>\n",
    "<center><img src='Image/Advanced/Evaluation_Residual.png' width='800'></center>\n",
    "\n",
    "---\n",
    "\n",
    "**1) 잔차진단의 2가지 목적:**\n",
    "\n",
    "> **\"`예측 성능`도 중요하지만(추정성능), 추정/분석 이후 `데이터의 패턴이 모델링에 잘 반영되었는지`(잔차진단) 평가하는 것도 중요\"**\n",
    ">\n",
    "> (1) 추가할만한 `데이터 전처리` 또는 다른 `모델링의 대안` 파악\n",
    ">> - 잔차에 `남아있는 패턴`을 전처리 단계에서 `추가 반영` 가능\n",
    ">> - 잔차의 남은 패턴으로 `다른 분석 알고리즘 고려` 가능\n",
    ">\n",
    "> (2) 분석 시작은 여러분들이 시작했지만, `분석 종료`는 `잔차 진단`이 알려줌\n",
    ">> - 잔차의 남은 패턴이 없다는 것은 `모델링이 데이터의 패턴을 최대한 반영` 의미\n",
    ">> - 모델링으로 더이상 할 수 있는 것들이 없으니 `분석을 마무리` 해도 됨을 의미\n",
    "\n",
    "---\n",
    "\n",
    "**2) 잔차진단의 목표: 잔차가 `백색잡음`과 얼마나 유사한지 측정**\n",
    "\n",
    "- 모델링에 데이터의 패턴이 잘 반영 되었다면, 추정 후의 `잔차`에는 아무 패턴도 없어야 함\n",
    "- `잔차`에 아무런 패턴도 남아있지 않은 경우 `백색잡음`의 형태로 분포\n",
    "- `잔차 분석/진단`을 통해 잔차가 `백색잡음(White Noise)`라면 역으로 `모델링`에서 데이터의 패턴을 잘 반영하여 성능이 좋음을 의미\n",
    "\n",
    "---\n",
    "\n",
    "**3) 백색잡음:** 잔차가 백색잡음이 아니라면 `모델링으로 개선의 여지가 있음`을 의미  \n",
    "\n",
    "<center><img src='Image/Advanced/White_Noise.png' width='450'></center>\n",
    "    \n",
    "> **(1) 잔차들은 `정규분포`이고, (unbiased) `평균 0이고 일정한 분산`을 가져야 함:** \n",
    ">\n",
    "> $$\n",
    "\\begin{align*}\n",
    "\\{\\epsilon_t : t = \\dots, -1, 0, 1, \\dots\\} & \\sim N(0,\\sigma^2_{\\epsilon_t}) \\\\\n",
    "\\text{where  } \\epsilon_t & \\sim \\text{i.i.d(independent and identically distributed)} \\\\\n",
    "\\epsilon_t &= Y_t - \\hat{Y_t} \\\\\n",
    "E(\\epsilon_t) &= 0 \\\\\n",
    "Var(\\epsilon_t) &= \\sigma^2_{\\epsilon_t} \\\\\n",
    "Cov(\\epsilon_s, \\epsilon_k) &= 0 \\text{ for different times!(s $\\ne$ k)}\n",
    "\\end{align*}\n",
    "$$\n",
    ">\n",
    "> **(2) 잔차들이 시간의 흐름에 따라 `상관성이 없어야` 함:** `자기상관함수(Autocorrelation Fundtion, ACF)=0` 확인\n",
    "> - 공분산(Covariance): \n",
    ">\n",
    "> <center>$Cov(Y_s, Y_k)$ = $E[(Y_s-E(Y_s))$$(Y_k-E(Y_k))]$ = $\\gamma_{s,k}$</center>\n",
    ">\n",
    "> - 자기상관함수(Autocorrelation Function): \n",
    ">\n",
    "> <center>$Corr(Y_s, Y_k)$ = $\\dfrac{Cov(Y_s, Y_k)}{\\sqrt{Var(Y_s)Var(Y_k)}}$ = $\\dfrac{\\gamma_{s,k}}{\\sqrt{\\gamma_s \\gamma_k}}$</center>\n",
    ">\n",
    "> - 편자기상관함수(Partial Autocorrelation Function): $s$와 $k$사이의 `상관성을 제거한 자기상관함수`\n",
    ">\n",
    "> <center>$Corr[(Y_s-\\hat{Y}_s, Y_{s-t}-\\hat{Y}_{s-t})]$  for $1<t<k$</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [정규분포 테스트(Normality Test)](https://en.wikipedia.org/wiki/Normality_test)\n",
    "\n",
    "- [**Shapiro–Wilk test:**](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)\n",
    "\n",
    "> **(1) 가설확인:**\n",
    ">\n",
    "> | **종류** | **해석** |\n",
    "|:---:|:---|\n",
    "| **대중주장<br>(귀무가설, Null Hypothesis, $H_0$)** | 데이터는 `정규분포 형태`이다 |\n",
    "| **나의주장<br>(대립가설, Alternative   Hypothesis, $H_1$)** | 데이터는 `정규분포 형태`가 아니다 |\n",
    ">\n",
    ">\n",
    "> **(2) 유의수준 설정 및 유의확률 확인**\n",
    "> - 유의수준: 5% (0.05) 분석가가 알아서 결정\n",
    "> - 유의확률(p-value): 컴퓨터가 알아서 추정\n",
    ">\n",
    "> **(3) 의사결정**\n",
    ">\n",
    "> | **기준** | **의사결정** | **해석** |\n",
    "|:---:|:---:|:---|\n",
    "| **p-value >= 유의수준(ex. 0.05)** | `대중주장 참` | 내가 수집/분석한 데이터는 <br> `정규분포 형태`이다 |\n",
    "| **p-value < 유의수준(ex. 0.05)** | `나의주장 참` | 내가 수집/분석한 데이터는 <br> `정규분포 형태`가 아니다 |\n",
    "\n",
    "- [**Kolmogorov–Smirnov test:**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Lilliefors test:**](https://en.wikipedia.org/wiki/Lilliefors_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Anderson–Darling test:**](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Jarque–Bera test:**](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**Pearson's chi-squared test:**](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "  \n",
    "- [**D'Agostino's K-squared test:**](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test)\n",
    "    - **가설확인:** Shapiro–Wilk와 동일\n",
    "    \n",
    "    \n",
    "- **예시:**\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/SW_example1.png' width='600'></center> -->\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/SW_example2.png' width='300'></center> -->\n",
    "\n",
    "<center><img src='Image/Advanced/SW_example3.png' width='400'></center>\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/SW_example4.png' width='600'></center> -->\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/SW_example5.png' width='500'></center> -->\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/SW_example6.png' width='600'></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [등분산성 테스트(Homoscedasticity Test)](https://en.wikipedia.org/wiki/Homoscedasticity)\n",
    "\n",
    "- [**Goldfeld–Quandt test:**](https://en.wikipedia.org/wiki/Goldfeld%E2%80%93Quandt_test)\n",
    "\n",
    "> **(1) 가설확인:**\n",
    ">\n",
    "> | **종류** | **해석** |\n",
    "|:---:|:---|\n",
    "| **대중주장<br>(귀무가설, Null Hypothesis, $H_0$)** | 데이터는 Homoscedasticity 상태다 <br> (`등분산이다`) |\n",
    "| **나의주장<br>(대립가설, Alternative   Hypothesis, $H_1$)** | 데이터는 Heteroscedasticity 상태다 <br> (`등분산이 아니다 / 발산하는 분산이다`) |\n",
    ">\n",
    ">\n",
    "> **(2) 유의수준 설정 및 유의확률 확인**\n",
    "> - 유의수준: 5% (0.05) 분석가가 알아서 결정\n",
    "> - 유의확률(p-value): 컴퓨터가 알아서 추정\n",
    ">\n",
    "> **(3) 의사결정**\n",
    ">\n",
    "> | **기준** | **의사결정** | **해석** |\n",
    "|:---:|:---:|:---|\n",
    "| **p-value >= 유의수준(ex. 0.05)** | `대중주장 참` | 내가 수집/분석한 데이터는 <br> `등분산`이다 |\n",
    "| **p-value < 유의수준(ex. 0.05)** | `나의주장 참` | 내가 수집/분석한 데이터는 <br> `등분산이 아니다` |\n",
    "\n",
    "- [**Breusch–Pagan test:**](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test)\n",
    "    - **가설확인:** Goldfeld–Quandt와 동일\n",
    "\n",
    "- [**Bartlett's test:**](https://en.wikipedia.org/wiki/Bartlett%27s_test)\n",
    "    - **가설확인:** Goldfeld–Quandt와 동일\n",
    "    \n",
    "    \n",
    "- **예시:**\n",
    "\n",
    "<center><img src='Image/Advanced/GQ_example1.jpg' width='500'></center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src='Image/Advanced/GQ_example2.jpg' width='400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자기상관 테스트(Autocorrelation Test)\n",
    "\n",
    "- [**Ljung–Box test:**](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test)\n",
    "\n",
    "> **(1) 가설확인:**\n",
    ">\n",
    "> | **종류** | **해석** |\n",
    "|:---:|:---|\n",
    "| **대중주장<br>(귀무가설, Null Hypothesis, $H_0$)** | 데이터는 Autocorrelation은 0이다(`존재하지 않는다`) |\n",
    "| **나의주장<br>(대립가설, Alternative   Hypothesis, $H_1$)** | 데이터는 Autocorrelation은 0이 아니다(`존재한다`) |\n",
    ">\n",
    ">\n",
    "> **(2) 유의수준 설정 및 유의확률 확인**\n",
    "> - 유의수준: 5% (0.05) 분석가가 알아서 결정\n",
    "> - 유의확률(p-value): 컴퓨터가 알아서 추정\n",
    ">\n",
    "> **(3) 의사결정**\n",
    ">\n",
    "> | **기준** | **의사결정** | **해석** |\n",
    "|:---:|:---:|:---|\n",
    "| **p-value >= 유의수준(ex. 0.05)** | `대중주장 참` | 내가 수집/분석한 데이터는 <br> Autocorrelation은 `존재하지 않는다` |\n",
    "| **p-value < 유의수준(ex. 0.05)** | `나의주장 참` | 내가 수집/분석한 데이터는 <br> Autocorrelation은 `존재한다` |\n",
    "\n",
    "- [**Portmanteau test:**](https://en.wikipedia.org/wiki/Portmanteau_test)\n",
    "    - **가설확인:** Ljung–Box와 동일\n",
    "\n",
    "- [**Breusch–Godfrey test:**](https://en.wikipedia.org/wiki/Breusch%E2%80%93Godfrey_test)\n",
    "    - **가설확인:** Ljung–Box와 동일\n",
    "  \n",
    "- [**Durbin–Watson statistic:**](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic)\n",
    "\n",
    "> **(1) 가설확인:**\n",
    ">\n",
    "> | **종류** | **해석** |\n",
    "|:---:|:---|\n",
    "| **대중주장<br>(귀무가설, Null Hypothesis, $H_0$)** | 데이터는 Autocorrelation은 0이다(`존재하지 않는다`) |\n",
    "| **나의주장<br>(대립가설, Alternative   Hypothesis, $H_1$)** | 데이터는 Autocorrelation은 0이 아니다(`존재한다`) |\n",
    ">\n",
    ">\n",
    "> **(2) 유의수준 설정 및 유의확률 확인**\n",
    "> - 유의수준: 5% (0.05) 분석가가 알아서 결정\n",
    "> - 유의확률(p-value): 컴퓨터가 알아서 추정\n",
    ">\n",
    "> **(3) 의사결정**\n",
    ">\n",
    "> | **기준** | **의사결정** | **해석** |\n",
    "|:---:|:---:|:---|\n",
    "| **2 근방** | `대중주장 참` | 내가 수집/분석한 데이터는 <br> Autocorrelation은 `존재하지 않는다` |\n",
    "| **0 또는 4 근방** | `나의주장 참` | 내가 수집/분석한 데이터는 <br> Autocorrelation은 `존재한다` <br> - **0:** `양(Positive)의 Autocorrelation` 존재한다 <br> - **4:** `음(Negative)의 Autocorrelation` 존재한다 |\n",
    "     \n",
    "- **예시:**\n",
    "\n",
    "<center><img src='Image/Advanced/LB_example1.jpg' width='400'></center>\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/LB_example2.jpg' width='600'></center> -->\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/DW_example1.png' width='600'></center> -->\n",
    "\n",
    "<center><img src='Image/Advanced/DW_example2.png' width='300'></center>\n",
    "\n",
    "<!-- <center><img src='Image/Advanced/DW_example3.png' width='600'></center> -->"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "393px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
