{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 딥러닝 (Supervised Learning)\n",
    "\n",
    "### CNN(비시계열 딥러닝) vs RNN(시계열 딥러닝)\n",
    "\n",
    "> **\"문제 유형에 따라 적절한 아키텍쳐 선택!\"**\n",
    "\n",
    "<center><img src='Image/DL_Comparing_CNNRNN.png' width='800'></center>\n",
    "\n",
    "> **CNN:** 이미지처럼 여러 값이 격자의 형태로 구성된 데이터를 처리하는데 특화된 모델\n",
    ">> - 이미지나 영상에서의 인식이나 분류 문제 등에서 뛰어난 결과\n",
    ">> - 입력된 이미지보다 더 큰 이미지로 손쉽게 확장될 수 있는 특징\n",
    ">> - YOLO, GAN 등의 모델들은 CNN을 기반\n",
    ">> - **은닉층 노드를 두번 이상 건드리지 않고 한번만 실행**\n",
    "\n",
    "> **RNN:** 순서가 있는 데이터(시계열, 자연어처리 등)를 처리하는데 특화된 모델\n",
    ">> - 예측이 가장 큰 분석 목적이자 활용분야\n",
    ">> - 비현실적으로 긴 시계열도 쉽게 확장 가능\n",
    ">> - 가변 길이의 시계열 데이터도 처리 가능\n",
    ">> - **은닉층 노드를 공통으로 사용하여 계속 갱신**\n",
    "\n",
    "- **비교:**\n",
    "\n",
    "| 데이터 | 분야 | 알고리즘 |\n",
    "|:-:|:-:|:-:|\n",
    "| 스냅샷<br>(Snapshot) | 이미지, 영상, 바둑 | CNN |\n",
    "| 시퀀스<br>(Sequence) | 기상, 주가, 언어, 음성 | RNN or LSTM |\n",
    "\n",
    "<center><img src='Image/DLTS_CNN_RNN.jpeg' width='600'></center>\n",
    "<!-- (https://miro.medium.com/max/700/1*L83_FGCXFXIoFthh0R0Dgg.jpeg) -->\n",
    "\n",
    "\n",
    "### Recurrent Neural Network: RNN\n",
    "\n",
    "> **\"입력층->은닉층->출력층으로 연결된 단방향 신경망 외에 이전 출력값이 다시 입력으로 연결되는 순환신경망(Rumelhart et al.1986)\"**\n",
    "<center><img src='Image/DL_RNN_UnfoldFold.PNG' width='600'>\"길이가 T인 시계열 데이터의 각 시간당 T개의 은닉상태가 계산되지만, 모듈화를 생각해 '하나의 계층'으로 구현\"</center>\n",
    "\n",
    "<center><img src='Image/DL_Understand_Flow.gif' width='600'></center>\n",
    "<!-- (https://wiki.pathmind.com/lstm) -->\n",
    "\n",
    "<center><img src='Image/DL_Understand_Flow.jpg' width='600'></center>\n",
    "<!-- (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) -->\n",
    "\n",
    "- **배경:**   \n",
    "1) 과거가 현재와 미래에 영향을 미치는 시계열에선, 과거 데이터를 저장하고 참조할 수 있는 메모리 효과 필요  \n",
    "2) 현재를 위해 과거를 순차적으로 보고, 미래를 위해 현재를 포함한 과거를 순차로 보며 반복되는, 순환의 과정 반영  \n",
    "> - 시간적으로 순서를 갖는 입력값의 자기 종속 구조의 분석을 가능케 함  \n",
    "> - Recurrent 과정을 통해 각 시점은 이전상태를 기억하는 메모리 역할을 하기에 이전상태 종속된 구조에 활용 가능  \n",
    "> - 기상예측, 주가예측 등의 시계열 분석, 자연어처리, 번역, 언어 모델링 등에 활용 (데이터의 순서가 중요한 분야)  \n",
    "> -> CNN으로도 적절한 아이디어로 데이터 처리하면 가능할 순 있지만 힘들것이며 굳이 상성이 다른 모델을 고려할 필요 없을듯\n",
    "\n",
    "- **종류:** 입력값과 출력값의 개수에 따라 크게 4가지 유형으로 분류 (무엇을 입력/출력하는지에 따라 유연하게 활용 가능)\n",
    "\n",
    "> - One-to-One: Vanilla Neural Networks\n",
    "> - Many-to-One: Classification, Time Series Analysis, Sentiment Analysis, Spam Detection\n",
    "> - One-to-Many: Image Captioning, Target Explanation\n",
    "> - Many-to-Many: Time Series Analysis, Machine Translation, Prediction of Next Word\n",
    "\n",
    "<center><img src='Image/DL_RNN_Type.PNG' width='700'></center>\n",
    "<!-- (http://cs231n.stanford.edu/2017/syllabus.html) -->\n",
    "\n",
    "<!-- https://mblogthumb-phinf.pstatic.net/MjAxOTA0MDVfODAg/MDAxNTU0NDM2OTk0NzI4.emSaigZ4IRMZ4lh4w_uGOW8vHEo81pl9ygUBI4QWaCsg.o1OGGNQHMFSX-WR5yvWGQsCqIyrdbrufSJPWIJWw4hkg.PNG.fininsight/image.png?type=w800 -->\n",
    "\n",
    "- **예시: Many-to-Many**\n",
    "<center><img src='Image/DL_RNN_Example.PNG' width='600'></center>\n",
    "<center><img src='Image/DL_RNN_Flow_Example.png' width='500'></center>\n",
    "<!-- (http://cs231n.stanford.edu/2017/syllabus.html) -->\n",
    "\n",
    "- **RNN의 학습원리:** \n",
    "\n",
    "> **1) 순전파(Feed Forward Network:FFN):**\n",
    "> - **은닉층:** $H_t = f(H_{t-1}, X_t) = \\sigma_H (U H_{t-1} + W X_t) = tanh (U H_{t-1} + W X_t)$ -> Markov Chain과 유사  \n",
    ">> - **입력1:**: $X_t \\in R^{m \\times n}, ~ m(입력벡터 ~ 차원수), n(미니배치 ~ 크기)$\n",
    ">> - **입력2:**: $H_{t-1} \\in R^{k \\times n}, ~ k(은닉벡터 ~ 차원수), n(미니배치 ~ 크기)$\n",
    ">> - **파라미터1:**: $W \\in R^{k \\times m}, ~ k(은닉벡터 ~ 차원수), m(입력벡터 ~ 차원수)$\n",
    ">> - **파라미터2:**: $U \\in R^{k \\times k}, ~ k(은닉벡터 ~ 차원수)$\n",
    ">> - **출력:**: $H_t \\in R^{k \\times n}, ~ k(은닉벡터 ~ 차원수), n(미니배치 ~ 크기)$\n",
    "> - **출력층:** $Y_t = f(H) = \\sigma_Y (VH_t) = softmax(VH_t), ~ V \\in R^{K \\times 1}$\n",
    ">> - **입력:**: $H_t \\in R^{k \\times n}, ~ k(은닉벡터 ~ 차원수), n(미니배치 ~ 크기)$\n",
    ">> - **파라미터:**: $V \\in R^{o \\times k}, ~ o(출력벡터 ~ 차원수), k(은닉벡터 ~ 차원수)$\n",
    ">> - **출력:**: $Y_t \\in R^{o \\times n}, ~ o(출력벡터 ~ 차원수), n(미니배치 ~ 크기)$\n",
    "\n",
    "<center><img src='Image/DL_RNN_LSTM_Compare1.PNG' width='600'></center>\n",
    "<!-- (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) -->\n",
    "\n",
    "> - **활성함수(tanh) 역할:** 값들이 층(Layer)를 통과할떄 특정 범위의 값으로 제한\n",
    "\n",
    "<center><img src='Image/DL_RNN_Activation_Without.gif' width='800'></center>\n",
    "<center><img src='Image/DL_RNN_Activation_With.gif' width='800'></center>\n",
    "<!-- (https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) -->\n",
    "\n",
    "> **2) 역전파(Backpropagation Through Time: BPTT):** 역전파를 시간흐름에 따라 확장하는 BPTT   \n",
    "> **\"앞으로 전진하는 Feed Forward Network(FFN)과 달리 각 시점마다 Loss Function에서 추정된 Error를 이전 시점으로 전파함\"**\n",
    "> - **공통 파라미터:** 각 시점마다 출력 희망값($\\hat{Y}$)과 실제 출력값($Y$)의 Error로 가중치($w$)들을 업데이트 함  \n",
    "> (The error they generate will return via backpropagation and be used to adjust their weights until error can't go any lower.)\n",
    ">> - **FFN Step 1:** 최종 오류에서 뒤로 이동하며 은닉층의 출력, 가중치, 입력의 편도함수(Partial Derivatives: $\\delta E / \\delta W$) 계산\n",
    ">> - **FFN Step 2:** 계산된 편도함수의 비율만큼 오류의 가중치 책임 할당\n",
    ">> - **BPTT Step 2':** 과거 시점으로 역전파 확장을 위해 추가된 시간요소에 대한 연쇄 규칙으로 미분 확장하여 오류 가중치 책임 할당\n",
    ">> - **FFN Step 3:** 경사하강법 알고리즘으로 할당된 가중치 책임만큼 가중치를 위아래로 조정(-> 어느 방향이든 최종 오류는 감소)\n",
    "\n",
    "<center><img src='Image/DL_RNN_Cycle.JPG' width='700'></center>\n",
    "<!-- (https://www.techleer.com/articles/185-backpropagation-through-time-recurrent-neural-network-training-technique/) -->\n",
    "\n",
    "- **한계:** (Bengio et al.1994) \n",
    "\n",
    "> - **Gradients:** 증가하는 Gradients는 해결하기 쉬운 반면 감소하는 Gradients는 학습하기 어렵게 함\n",
    "> - **Computing:** 시계열 데이터의 길이에 비례하여 BPTT가 사용하는 컴퓨팅 비용도 증가\n",
    "> - **Long-term Dependency:** 은닉층의 과거 정보가 마지막까지 전달되지 못하는 현상 (성능 하락)\n",
    ">> $H_t = \\sigma_H (U H_{t-1} + W X_t) = \\sigma_H (U \\sigma_H (U H_{t-2} + W X_{t-1}) + W X_t)$  \n",
    ">> **-> 다음 단어를 예측한다고 할 때 초반 단어의 정보가 뒷단까지 충분히 전달되기 어려워 예측이 어려워짐**   \n",
    ">> **-> 이런 문제를 해결하기 위해 장기간의 메모리 유지를 위한 Cell이 개발되고 대표적으로 LSTM과 GRU임** \n",
    ">> <center><img src='Image/DL_RNN_LongDepend.PNG' width='600'></center>\n",
    "> - **Vanishing Gradients:** BPTT는 멀리 전파될 때 계산량이 많아지지만 전파 양이 점차 작아지는 문제 존재 (RNN 구조문제)   \n",
    "> (*Exploding Gradients: VG와 반대로 gradient 값이 무한대로 커지는 경우를 말하는데 미리 최대값을 지정해주는 방법(기울기 클리핑)으로 해결 가능)\n",
    ">> **-> 가장 일반적인 해결책은 'sigmoid'나 'tanh' 대신에 'relu'를 사용하는 것 (sigmoid < tanh < relu)**   \n",
    ">> **-> 'relu'가 다른 네트워크 구조에서는 훨씬 효과적이나 RNN 계열은 같은 레이어를 반복하기 때문에 미사용**   \n",
    ">> **-> 이러한 문제에 대응하기 위해 Error를 일부(보통 5-step)만 전파시키는 Truncated BPTT를 사용하거나 LSTM을 사용**   \n",
    ">> (Truncated BPTT is an approximation of full BPTT that is preferred for long sequences, since full BPTT's forward/backward cost per parameter update becomes very high over many time steps.)\n",
    ">> <center><img src='Image/DL_RNN_VanishingGradient.png' width='700'></center>\n",
    "<!-- (https://mblogthumb-phinf.pstatic.net/) -->\n",
    "\n",
    "\n",
    "### Long Short-Term Memory: LSTM\n",
    "\n",
    "> **\"RNN의 단기기억문제 해결책으로 고안됨 (Hochreiter et al. 1997)\"**  \n",
    "> **\"LSTMs can capture on different time scales simultaneously\"**  \n",
    ">> - 원인과 결과의 관계를 장시간까지 확장하여 반영하도록 함\n",
    ">> - 일정한 오류를 유지함으로써 여러 네트워크가 장시간(1000개 이상)에 걸쳐 학습 가능하도록 함\n",
    ">> - Cell 개념을 도입하여 열리고 닫히는 게이트를 통해 어떤 데이터를 저장/읽기/쓰기/삭제 결정\n",
    ">> - 기존 RNN에 Cell State($C$)를 추가하여 얼마나 과거의 데이터를 기억할지를 제안함\n",
    ">> - RNN의 은닉층을 LSTM Block으로 대체, 각 Block은 기존 $H$에 $C$가 추가된 네트워크 구조\n",
    "\n",
    "<center>\"계산을 단순화 하기 위해 직사각형 노드로 단순화 재표현\"<img src='Image/DL_RNN_LSTM_Compare1.PNG' width='600'></center>\n",
    "<center><img src='Image/DL_RNN_LSTM_Compare2.PNG' width='600'></center>\n",
    "<!-- (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) -->\n",
    "\n",
    "- **LSTM Block:** \n",
    "\n",
    "> **\"곱하기 이외에 더하기를 사용하여 문제해결 + 두개의 가중치(C:long-term & H:short-term)를 사용한 가중평균\"**   \n",
    ">> - 핵심은! $C$라는 기억셀을 추가하여 이를 바탕으로 외부 계층에 은닉상태 $H$를 출력 ($H_t = tanh(C_t)$)\n",
    ">> - $H$는 다른계층과 위쪽으로 출력되는 반면, $C$는 LSTM 계층 내에서만 전달되며 과거부터 $t$까지 모든 정보 저장\n",
    "\n",
    "> - **0) 게이트(Gate):** 데이터의 흐름을 제어하는 '문'과 같은 존재   \n",
    "> **-> (0.7(70%) or 0.3(30%)처럼 얼마나 열지라는 가중치를 학습)**   \n",
    "> - **1) 망각게이트(Forget Gate):** $F_t = \\sigma_F (U_F H_{t-1} + W_F X_t)$   \n",
    "-> 과거(장기)를 얼마나 잊을지 제어\n",
    "> - **2) 새로운입력(Input Candidate):** $G_t = tanh(U_G H_{t-1} + W_G X_t)$   \n",
    "-> 입력과 단기상태 분석하여 새로운 정보 추출(RNN의 은닉층과 같음)\n",
    "> - **3) 입력게이트(Input Gate):** $I_t = \\sigma_I (U_I H_{t-1} + W_I X_t)$   \n",
    "-> $G_t$의 가치를 판단하여 어떤 값이 장기상태에 가치있을지 추정\n",
    "> - **4) 기억업데이트(Cell Update):** $C_t = F_t*C_{t-1} + I_t*G_t$   \n",
    "-> 관련성이 있는 새로운 값으로 업데이트 하기 위해, 과거를 얼마나 잊을건지 + 현재를 얼마나 반영할지(0:과거만, 1:현재만) 추정 반영\n",
    "> - **5) 출력게이트(Output Gate):** $O_t = \\sigma_O (U_O H_{t-1} + W_O X_t) \\\\ H_t = O_t * tanh(C_t)$   \n",
    "-> 다음 시간에 어떤 값이 될지 $H_t$로 출력 (과거 정보를 갖고 있는 것과 동시에 예측에 사용됨)\n",
    "> - **6) 출력층:** $Y_t = f(H) = softmax(VH_t)$   \n",
    "> **-> $C_{t-1}$는 Forget Gate에서 기억을 잃고 Input Gate에서 새로운 기억을 추가하여 $C_t$가 출력되고 복사되어 Output Gate의 tanh로 전달되어 단기상태($H_t$)와 출력인 $Y_t$를 생성**\n",
    "\n",
    "<center>\"The Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be.\"<img src='Image/DL_RNN_Operation.png' width='700'></center>\n",
    "<!-- (http://colah.github.io/posts/2015-08-Understanding-LSTMs/) -->\n",
    "\n",
    "- **LSTM의 순전파 및 역전파:**\n",
    "\n",
    "> **\"RNN은 시간이 지나면 이전 값을 잊어버리는 반면, LSTM은 각 게이트를 통해 이전 입력값이 계속 저장되어 필요한 시점에 출력 및 반영됨\"**\n",
    "<center>\"RNN\"<img src='Image/DL_Compare_RNN.PNG' width='500'>\"LSTM\"<img src='Image/DL_Compare_LSTM.PNG' width='500'></center>\n",
    "<!-- (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4531750) -->\n",
    "\n",
    "> **\"역전파는 장기기억셀(C)의 더하기와 곱하기 노드에만 전달되기에 기울기 변화(감소)는 발생하지 않음\"**   \n",
    "> -> 곱하기 노드에서는 행렬곱이 아닌 원소별 곱(아마다르 곱)을 사용하고 매 시각 다른 게이트 값을 이용하기에 누적되지 않음   \n",
    "> -> FG의 곱하기 노드가 잊어야 할건 기울기를 줄이고 그렇지 않은건 과거로 전해지기에 '기억셀' 자체는 기울기 소실 없음   \n",
    "\n",
    "<!-- ![LSTM_new](https://mblogthumb-phinf.pstatic.net/MjAxOTA0MDVfMjI4/MDAxNTU0NDQyNDkzNTc2.bDFeo4zfqCPvO5-Mtevtx-G5i52H5H83fdgFdjxMFFcg.ro97P9ymMRe7zNQDharjULt23LDna62KUAL5D9AkZ30g.PNG.fininsight/image.png?type=w800) -->\n",
    "\n",
    "<!-- ![ss](https://media.vlpt.us/images/dscwinterstudy/post/24d75848-2a19-4403-a78d-7a558042c8d0/fig%206-19.png) -->\n",
    "\n",
    "\n",
    "### Gated Recurrent Unit: GRU\n",
    "\n",
    "> **\"RNN의 단기기억문제 해결책으로 고안됨\"**   \n",
    "> **\"Gate 구조가 적용된 RNN 일종으로 LSTM에서 영감을 받아 구조를 보다 간결하게 변경함에도 빠른속도와 유사한 성능(Cho et al. 2014)\"**\n",
    ">> - LSTM은 Gate가 3개지만 GRU는 2개   \n",
    ">> -> Reset Gate & Update Gate(:LSTM의 Forget+Input과 유사)   \n",
    ">> -> Convex 조합을 수행하는 두개의 벡터라고 볼 수 있음 (0 or 1)   \n",
    ">> - Output Gate가 없는 LSTM이기에 메모리셀에 담기는 정보 양 증가\n",
    ">> - LSTM의 $C_t$와 $H_t$가 하나의 벡터 $H_t$로 통합\n",
    ">> - GRU가 LSTM보다 학습할 가중치가 적어지는 것이 이점\n",
    ">> - 주제별로 LSTM과 GRU의 성능은 차이가 있음\n",
    "\n",
    "<center><img src='Image/DL_LSTM_GRU.png' width='700'></center>\n",
    "<!-- (https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) -->\n",
    "\n",
    "> - **1) 초기화게이트(Reset Gate):** $R_t = \\sigma_R (U_R H_{t-1} + W_R X_t)$   \n",
    "-> 과거의 정보를 제거할 비율 결정\n",
    "> - **2-1) 업데이트게이트(Update Gate):** $U_t = \\sigma_U (U_U H_{t-1} + W_U X_t)$   \n",
    "-> Input 느낌으로 현재의 정보를 얼마나 반영할지 결정\n",
    "> - **2-2) 업데이트게이트(Update Gate):** $1 - U_t$  \n",
    "-> Forget 느낌으로 과거의 정보를 얼마나 잊을지 결정\n",
    ">> -> Update Gate($U_t$) 한번으로 LSTM의 forget과 input 게이트를 모두 제어하고 0이면 FG가 열리고 IG가 닫힘(1이면 FG닫히고 IG열림). 즉, $t-1$의 기억과 $t$의 기억중 하나만 선택됨\n",
    "> - **3) 새로운입력(Input Candidate):** $h_t = tanh (U_h H_{t-1}*R_t + W_h X_t)$   \n",
    "-> 현 시점의 정보 후보군 계산   \n",
    "-> GRU에는 output gate 없어서 $H_t$가 타임스텝마다 출력되며 $H_{t-1}$의 어느 부분이 출력될지 제어하는 $R_t$ 활용\n",
    "> - **4) 은닉층:** $H_t = (1 - U_t)*H_{t-1} + U_t*h_t$   \n",
    "-> Update와 Candidate 결합하여 현시점 은닉층 계산 후 다음상태의 은닉층으로\n",
    "\n",
    "<center><img src='Image/DL_GRU_Architecture.png' width='400'></center>\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350.571px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
