{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공지능 시대\n",
    "\n",
    "- **인공지능에 대한 2종류의 사람**\n",
    "> **1) \"인공지능은 만능이다\"라는 환상을 가지고 있는 사람**   \n",
    "> **2) 열정적으로 공부하다 현재 수준과 한계에 혼란인 사람**\n",
    ">> **\"보통사람\"들을 위한 가이드로 가능한 쉽게 설명하는 것이 목적**\n",
    "\n",
    "**1) 정의:** 인간지능의 원리를 찾고(연구목표) 이를 프로그래밍으로 인공적 구현하는 것(연구대상)\n",
    "> - 1956년 처음 등장한 이래 아직도 미래기술로써 인식되는 '인공지능'\n",
    "> - 지능의 원리를 발견하고 나면 더이상 지능이 아니게 되는 과학 기반\n",
    "> - **AI Effect:** 발견된 것을 제외하고 아직 발견하지 못한 '지능의 비결'을 꾸준히 찾게 되는 것\n",
    "\n",
    "**2) 방향:** 규칙은 끊임없이 변하니 규칙(정답)찾는 것은 중지하고 기존 경험기반 기계가 스스로 통계적/확률적으로 판단하게 하자\n",
    "> - 기존의 경험은 '데이터를 통한 학습'으로 바뀌었으며 반드시 필요\n",
    ">> - **Supervised Learning:** 기계에게 문제와 답을 알려주며 학습시킨 후 답을 찾는 것으로 단순한 예측과 분류 등에 활용    \n",
    ">> - **Unsupervised Learning:** 기계에게 문제만 알려주며 패턴이라 룰 찾아 답을 스스로 예측하는 것으로 연관규칙이나 군집에 활용   \n",
    ">> - **Reinforcement Learning:** 아무것도 모른채 일단 실전에 뛰어들고 시행착오를 통해 학습   \n",
    ">>> - 답을 찾기위해 시행착오를 발생시킨 후 개선하는 방향\n",
    ">>> - 환경과 상호작용 속에서 답을 찾는 것으로 시간이 오래 소요되나 가장 강력하고 진보적인 방향     \n",
    "\n",
    "**3) 인공신경망과 딥러닝:**\n",
    "\n",
    "<center>\"Evolution of Artificial Intelligence\"<img src='Image/DL_Evolution.png' width='600'></center>\n",
    "\n",
    "> - 인공신경망은 머신러닝 방법론 중 하나로 1950년대 제안되었지만 외면받다 2000대 이후 재부상 (빅데이터 + 컴퓨팅파워 + 알고리즘고도화)\n",
    "> - 인공신경망의 한계는 결국 '최적화'가 어렵고 너무 많은 시간이 걸린다는 점\n",
    "> - 딥러닝이 인공신경망의 알고리즘을 개선하며 '비지도 학습'을 통해 최적화를 수행하는 방식으로 한계 극복 및 가능성 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 기술의 발전방향\n",
    "\n",
    "### 인공신경망(Artificial Neural Network: ANN)\n",
    "\n",
    "> - 딥러닝의 가장 핵심적인 기술로 인간 두뇌의 신경세포를 모방한 컴퓨터 알고리즘 네트워크 구조\n",
    "> - ANN은 어떠한 형태의 함수(Function)도 근사할 수 있는 Universal Function Approximator로 알려져있음\n",
    "> - 우리가 알고싶어 하는 함수를 쉽게 근사하는 것이 목적\n",
    ">> 1) 동그라미: 하나의 노드(뉴런)  \n",
    ">> 2) 실선: 노드와 노드가 이어진 connection  \n",
    "\n",
    "- **뉴런의 입출력**\n",
    "<center><img src='Image/DL_Neuron.png' width='500'></center>\n",
    "\n",
    "- **인공신경망의 입출력**\n",
    "<center><img src='Image/DL_ANN.png' width='500'></center>\n",
    "\n",
    "\n",
    "### 단층퍼셉트론(Single-Layered Perceptron: SLP)\n",
    "\n",
    "**1) 구조:**\n",
    "> - 여러개의 노드(뉴런)들로 이루어진 하나의 인공신경망(단일층)으로 구성된 알고리즘 (Rosenblatt, 1957)\n",
    "> - 데이터가 각 노드의 입력으로 전달되고 각 노드는 입력데이터를 합산하여 하나의 값(분류 or 연속)으로 출력함\n",
    ">> **(1) 입력층(Input Layer):** 입력 데이터가 들어가는 곳  \n",
    ">> **(2) 가중치(Weight):** 입력 데이터 각각이 출력에 주는 영향도를 조절하는 매개변수로 클수록 해당 입력이 중요함  \n",
    ">> **(3) 은닉층(Hidden Layer):** 입력 데이터를 잘 섞어서 변환되는 곳  \n",
    ">> **(4-1) 활성함수(Activation Function):** 층을 쌓아 비선형 공간으로 변환(압축)하고 미분(Gradient)이 작동되게 하는 도구    \n",
    ">>> - 직선이 X의 변화 대비 Y의 변화라면, Gradient는 모든 가중치의 변화 대비 에러의 변화   \n",
    ">>> - 비선형 Scaling과 유사하며 데이터를 필터링하고 미분을 가능하게 함  \n",
    ">>> - 입력의 모든 데이터를 다음으로 보낼 필요 없이 특정 수준(범위)의 데이터만을 보내도록 제한을 걸어둔 것  \n",
    ">>> - 정해진 임계값 기준 은닉층의 출력을 최종적으로 원하는 출력값으로 변환  \n",
    ">>\n",
    ">> **(4-2) 임계값(Bias):** 노드(뉴런)이 얼마나 쉽게 1로 활성화(Activation)되는지를 조정하는 매개변수  \n",
    ">> **(5) 출력층(Output Layer):** 해결하고자 하는 문제의 성격에 맞춘 최종 출력값을 반환되는 곳  \n",
    "\n",
    "<center><img src='Image/DL_SLP_Custom.PNG' width='400'></center>\n",
    "\n",
    "**2) 한계: NN의 첫번째위기\"**  \n",
    "> - 모든 데이터를 정확히 분류시킬때까지 학습이 진행되나 선형분리를 통해서만 데이터 분류가 가능함 (직선으로 나눌수 있는 분류만 가능)  \n",
    ">> - 선형 분류는 가능하지만 비선형 분류는 불가능함   \n",
    ">\n",
    "> - 단층이라 각 노드의 가중치를 변경할 수 없고 초기에 주어진 가중치가 변경되지 않고 유지됨 (가중치 조절 불가)  \n",
    ">> - 시작단계에서 가중치를 부여하여 분류결과를 변경이 가능하지만 학습의 개념이 아닌 단순한 분류문제 해결 (if-else와 동일)  \n",
    "\n",
    "<center><img src='Image/DL_SLP.png' width='500'></center>\n",
    "\n",
    "<center><img src='Image/DL_SLP_limit.gif' width='500'></center>\n",
    "<!-- (http://ecee.colorado.edu/~ecen4831/lectures/NNet3.html) -->\n",
    "\n",
    "\n",
    "### 다층퍼셉트론(Multi-Layered Perceptron: MLP)\n",
    "\n",
    "**1) 방향:** 인공신경망을 여러 계층으로 구성한 다층/심층신경망으로 딥러닝의 출발  \n",
    "> - 단층퍼셉트론이 비선형을 분리할 수 없다는 한계를 다층(여러개의 직선)으로 해결  \n",
    ">> - 위 예에서 2개의 직선 또는 1개의 곡선(여러개의 직선결합)을 이용하여 제대로 분류가 가능할 것? \n",
    ">\n",
    "> - 여전히 학습이란 개념은 적용 불가\n",
    "\n",
    "<center><img src='Image/DL_MLP_Custom.PNG' width='500'></center>\n",
    "\n",
    "> - **비교:**\n",
    "\n",
    "<center><img src='Image/DL_ANN_MLP.bmp' width='500'></center>\n",
    "<!-- (https://4ir.kisti.re.kr/) -->\n",
    "\n",
    "**2) 분석방향:**\n",
    "> - **추정(학습):** 기계학습은 임의 가중치부터 시작하여 업데이트 하며 최종 가중치를 정하는 과정\n",
    "> - **출력:** 0 또는 1이면 선형분류(Linear Classifier)로 볼 수 있음\n",
    "> - **출력수:** 분류문제에 적용시 $n$개의 노드에 대해 $2^n$개 (1개의 노드시 2개 경우의 분류)\n",
    "> - **검증:** 추정된 출력과 실제 출력이 다를 때 에러를 줄이는 방향으로 가중치를 업데이트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파(Back Propagation: BP)\n",
    "\n",
    "> **\"역방향으로 데이터(오차)를 전달한다\"**  \n",
    "> - 학습을 통해 가중치를 업데이트 할 수 있게 됨\n",
    "\n",
    "**1) 학습방향:** 처음 설정된 각 노드별 가중치가 우리가 원하는 결과(검증Metric)를 만들 수 있도록 지속 업데이트\n",
    "\n",
    "<center><img src='Image/DL_MLP_Learning.PNG' width='600'></center>\n",
    "\n",
    "> **(1) 네트워크 초기화:** 가중치의 초깃값이 필요하며 일반적으로 무작위로 초기화 됨   \n",
    "> - **초기값이 모두 같으면 모든 노드들이 같은 값을 받고 같은 값을 출력하고 오차 역전파도 동일하게 전파되므로 다른값 필요**   \n",
    ">\n",
    "> **(2) 전파:** 초기화된 가중치들이 퍼셉트론을 거쳐 출력\n",
    ">> $\\hat{Y}_{init} = f(\\sum_{i}^{k} w_i x_i - \\theta)$  \n",
    ">\n",
    "> **(3) 오차 평가:** \n",
    ">> $Error = MSE = Cost~Function = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$\n",
    ">\n",
    "> **(4) 역전파:** 각 가중치 별 현재 에러에 미치는 영향 계산 \n",
    ">> $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    ">\n",
    "> **(5) 조정:** 에러를 줄이기 위해 가중치를 업데이트(Gradient Descent) \n",
    ">> $w_i^{new} = w_i^{old} - \\alpha \\Delta w_i^{old} = w_i^{old} - \\alpha \\frac{\\delta E}{\\delta w}$  \n",
    "> - **$\\alpha$: 학습률(Learing Rate)로 높은 수치는 최적점을 건너뛰고 오차를 높일 수 있음**  \n",
    ">\n",
    "> **(6) 종료:** 허용오차 범위 내로 들어오면 최종 가중치로 반영\n",
    ">> $w_i^{final} = w_i^{new}$  \n",
    "\n",
    "<center><img src='Image/DL_GD.PNG' width='400'></center>\n",
    "\n",
    "**2) 한계: NN의 두번째위기**  \n",
    "> - **Vanishing Gradients:** BP는 멀리 전파될 때 계산량이 많아지지만 전파 양이 점차 작아지는 문제 존재  \n",
    ">> - **1986년부터 2006년까지 해결하지 못하다(NN의 두번째위기) Hinton교수가 활성함수를 Sigmoid대신 ReLU(Rectified Linear Unit)를 사용하며 해결**\n",
    "\n",
    "<center><img src='Image/DL_ActivationFunction.png' width='500'></center>\n",
    "<!-- (https://t1.daumcdn.net/cfile/tistory/22293C50579F7BBF13) -->\n",
    "\n",
    "<center><img src='Image/DL_ActivationFunction_Type.png' width='700'></center>\n",
    "\n",
    "<!-- <center><img src='Image/DL_ActivationFunction_Type_All.png' width='700'>(https://miro.medium.com/max/814/1*F9-nc6ez5GOJ1mdB3TLlow.png)</center> -->\n",
    "\n",
    "\n",
    "### 최적화(Optimization)\n",
    "\n",
    "> **\"손실함수의 값을 가능한 낮추는 매개변수를 찾는 것\"**  \n",
    "> **\"Prerequisite: Derivatives, Partial Derivatives, Chain Rule\"**\n",
    ">\n",
    "> **(1) 네트워크 초기화:** 가중치의 초깃값이 필요하며 일반적으로 무작위로 초기화 됨   \n",
    "> - **초기값이 모두 같으면 모든 노드들이 같은 값을 받고 같은 값을 출력하고 오차 역전파도 동일하게 전파되므로 다른값 필요**   \n",
    ">\n",
    "> **(2) 전파:** 초기화된 가중치들이 퍼셉트론을 거쳐 출력\n",
    ">> $\\hat{Y}_{init} = f(\\sum_{i}^{k} w_i x_i - \\theta)$  \n",
    ">\n",
    "> **(3) 오차 평가:** \n",
    ">> $Error = MSE = Cost~Function = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$\n",
    ">\n",
    "> **(4) 역전파:** 각 가중치 별 현재 에러에 미치는 영향 계산 \n",
    ">> $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    ">\n",
    "> **(5) 조정:** 에러를 줄이기 위해 가중치를 업데이트<span style=\"color:red\">**(Gradient Descent)**</span>   \n",
    ">> $w_i^{new} = w_i^{old} - \\alpha \\Delta w_i^{old} = w_i^{old} - \\alpha \\frac{\\delta E}{\\delta w}$  \n",
    "> - **신규 가중치 = 기존 가중치 - 학습률 * 그레디언트**\n",
    "> - **$\\alpha$: 학습률(Learing Rate)로 높은 수치는 최적점을 건너뛰고 오차를 높일 수 있음**  \n",
    ">\n",
    "> **(6) 종료:** 허용오차 범위 내로 들어오면 최종 가중치로 반영 \n",
    ">> $w_i^{final} = w_i^{new}$  \n",
    "\n",
    "- **알고리즘 종류 및 방향:**\n",
    "\n",
    "<center><img src='Image/DL_Optimization_Direction.png' width='700'></center>\n",
    "\n",
    "<!-- <center><img src='Image/DL_Optimization_Direction_KR.png' width='700'>(https://www.slideshare.net/yongho/ss-79607172)</center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝(Deep Learning: DL)\n",
    "\n",
    "> **\"인공신경망의 다층구조가 심화(Deep)된 네트워크 구조의 알고리즘\"**\n",
    "\n",
    "<center><img src='Image/DL_What-is-Deep-Learning.png' width='500'></center>\n",
    "<!-- (https://www.michaelchimenti.com/2017/11/deep-neural-nets-software-2-0/) -->\n",
    "\n",
    "| 장점 | 단점 |\n",
    "|:-:|:-:|\n",
    "|- 연속형 및 범주형 변수에 상관없이 분석 가능<br>     - 입력 변수들 간의 비선형 특성 추정 가능<br>     - 예측력이 전통적인 기계학습 알고리즘에 비해 우수한 편<br>     - Feature Engineering이 자동으로 수행(방향)<br>     - 데이터의 양이 많아지면 성능은 계속 상승 | - 복잡한 신경망 일수록 작동 시간이 오래 걸림(고사양   PC필요)<br>     - 데이터 입력이 일정하지 않으므로 결과도 일정하지 않음(Not Robust)<br>     - 가중치의 의미를 정확히 해석할 수 없어서 결과 해석이 불가 |\n",
    "\n",
    "**1) 성능개선 아이디어:** 네트워크 개선   \n",
    "> **(1) Drop Out:** 은닉층의 뉴런을 무작위 확률로 생략하는 방법   \n",
    ">\n",
    "> **\"각 단계의 모델은 약한 학습모델이지만 약한모델들이 합쳐져 강력한 예측력 구현\"**   \n",
    "> - 빠진 뉴런을 포함하여 예측 하기에 여러개의 독립적인 내부표현 학습가능\n",
    "> - 네트워크가 뉴런의 특정 가중치에 덜 민감해짐\n",
    "> - 더욱 일반화에 기여가 가능하고 훈련 데이터를 지나칠 가능성이 적어짐\n",
    "> - 너무 낮은 비율은 영향이 없고 너무 높은 비율은 과소 적합을 하기에 20~50\\% 권장\n",
    "> - Learning Rate과 Momentum을 높여 사용(LR:10 -> 100, Momentum: 0.9 or 0.99)\n",
    "> - LR이 높을 시 너트워크 가중치의 크기를 줄이면 높은 성능 가능  \n",
    "\n",
    "<center><img src='Image/DL_DropOut.png' width='500'></center>\n",
    "<!-- (https://t1.daumcdn.net/cfile/tistory/99324B335D383CBD1B) -->\n",
    "\n",
    "> **(2) Early Stopping:** 테스트 데이터의 성능을 쉽게 끌어낼 수 있는 경우 일찍 종료하는 방법   \n",
    ">\n",
    "> **\"데이터가 Train/Evaluate/Test으로 구분되어 있을때, Evaluate 데이터가 낮은 오차가 나오면 멈추므로 과적합 낮춤\"**   \n",
    "\n",
    "<center><img src='Image/DL_Overfitting_Epoch.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/DL_Overfitting_EarlyStopping.png' width='600'></center>\n",
    "<!-- (https://kevinthegrey.tistory.com/110) -->\n",
    "\n",
    "**2) 과적합 개선 아이디어:** Cost Function 개선   \n",
    "> **\"추정 계수가 커지면 활성함수를 통해 기울기가 급변하게되어 오류 최소화가 어렵고 과적합 여지가 높아짐\"**   \n",
    ">\n",
    "> **(1) L1 Panelty:** LASSO Regression에 사용한 Cost Function 반영    \n",
    "> - 중요도가 적은 가중치는 0이 되기에 과적합이 방지\n",
    "> - 변수선택 효과가 있어 모델 복잡도를 효과적으로 제약\n",
    "> - 변수의 수가 데이터의 수보다 많은 경우도 사용가능하기에 고차원의 데이터도 적용가능(불필요 변수를 0으로 바꿈)\n",
    "> - 페널티의 크기는 Hyperparameter로 사전 결정되며 교차검증이나 유사 방법으로 결정\n",
    "> - 페널티 효과는 가중치 크기에 비례하고 가중치 크기는 데이터 양에 비례\n",
    "> - 모델에 제약을 주며 정확도를 상승시킴   \n",
    ">\n",
    "> **(2) L2 Panelty:** Ridge Regression에 사용한 Cost Function 반영   \n",
    "> - 모든 가중치를 일률적으로 작게 만드는 경향  \n",
    "> - 중요도가 적은 가중치는 0이 아닌 작은 값을 가지게 하므로 변수선택 효과 없음\n",
    "> - 최적 모델 선택에 더 적합하므로 주료 사용됨\n",
    "\n",
    "<center><img src='Image/DL_CF_L1L2.png' width='600'></center>\n",
    "\n",
    "<center><img src='Image/DL_CF_L1L2_Compare.png' width='600'></center>\n",
    "<!-- (https://kevinthegrey.tistory.com/110) -->\n",
    "\n",
    "**3) Others:**\n",
    "\n",
    "> **(1) Batch:** 모델 내부 파라미터를 업데이트 하기 전 샘플의 개수\n",
    "> - **Batch Gradient Descent:** Batch Size = Size of Training Set\n",
    "> - **Stochastic Gradient Descent:** Batch Size = 1\n",
    "> - **Mini-Batch Gradient Descent:** 1 < Batch Size < Size of Training Set\n",
    ">\n",
    "> **(2) Epoch:** 훈련데이터의 알고리즘 수행 횟수 $[1,\\text{Size(Training)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전통적 알고리즘과 딥러닝 알고리즘 비교\n",
    "\n",
    "### (선형)회귀분석과 (비선형)인공신경망의 차이\n",
    "\n",
    "**1) 선형회귀분석 방정식과 구조:** $X$성분이 고유의 가중치 $w$와 곱해져 출력값 $Y$에 독립적이고 직접적인 영향\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{Y} \\approx Y &= f(X_0, X_1, X_2, \\cdots, X_k) = w_0X_0 + w_1X_1 + w_2X_2 + \\cdots + w_kX_k \\\\\n",
    "&= f(X) = \\sum_{i=0}^{k} w_i X_i \\rightarrow \\sigma(\\sum_{i=0}^{k} w_i X_i) \\\\\n",
    "&1)~Y: 출력 노드 \\\\\n",
    "&2)~f: 선형 함수 \\\\\n",
    "&3)~X: 입력 노드 \\\\\n",
    "&4)~w_i: 가중치(입력 노드) \\\\\n",
    "&5)~\\sigma: 활성함수\n",
    "\\end{align*}\n",
    "\n",
    "<center><img src='Image/DL_Comparing1.PNG' width='250'></center>\n",
    "\n",
    "**2) 인공신경망 방정식과 구조:** \n",
    "\n",
    "> (1) $X$성분이 고유의 가중치 $w$와 곱해져 은닉층 $H$에 독립적이고 직접적인 영향  \n",
    "> (2) 은닉층 $H$성분이 고유의 가중치 $v$와 곱해져 출력값 $Y$에 독립적이고 직접적인 영향  \n",
    "\\begin{align*}\n",
    "\\hat{Y} \\approx Y &= f(H_0, H_1, \\cdots, H_{k-2}) = v_0H_0 + v_1H_1 + \\cdots + v_{k-2} H_{k-2} \\\\\n",
    "&= v_0h_0(X_{0}, X_{1}, X_{2}, \\cdots, X_{k}) + v_1h_1(X_{0}, X_{1}, X_{2}, \\cdots, X_{k}) + \\cdots + v_{k-2} h_{k-2}(X_{0}, X_{1}, X_{2}, \\cdots, X_{k}) \\\\\n",
    "&= v_0(w_{00}X_{0} + w_{10}X_{1} + w_{20}X_{2} + \\cdots + w_{k0} X_{k}) + v_1(w_{01}X_{0} + w_{11}X_{1} + w_{21}X_{2} + \\cdots + w_{k1} X_{k}) + \\\\\n",
    "&\\cdots + v_{k-2}(w_{0k-2}X_{0} + w_{1k-2}X_{1} + w_{2k-2}X_{2} + \\cdots + w_{kk-2} X_{k}) \\\\\n",
    "&= f(h_0(X_{0}, X_{1}, X_{2}, \\cdots, X_{k}), h_1(X_{0}, X_{1}, X_{2}, \\cdots, X_{k}), ..., h_{k-2}(X_{0}, X_{1}, X_{2}, \\cdots, X_{k})) \\\\\n",
    "&= f(H) = \\sum_{i=0}^{k-2} v_i H_i = \\sum_{i=0}^{k-2} v_i h_i(X) \\rightarrow \\sigma(\\sum_{i=0}^{k-2} v_i h_i(X)) \\\\\n",
    "&1)~Y: 출력 노드 \\\\\n",
    "&2)~f: 선형 함수 \\\\\n",
    "&3)~H: 히든 노드 \\\\\n",
    "&4)~v_i: 가중치(히든 노드) \\\\\n",
    "&5)~h_i: 비선형 함수 \\\\\n",
    "&6)~X: 입력 노드 \\\\\n",
    "&7)~w_i: 가중치(입력 노드) \\\\\n",
    "&8)~\\sigma: 활성함수\n",
    "\\end{align*}\n",
    "\n",
    "<center><img src='Image/DL_Comparing2.PNG' width='400'></center>\n",
    "\n",
    "**3) 검증방향(Evaluation):** 동일한 편이나 인공신경망이 좀더 자유도가 높고 엄격해야 함\n",
    "\n",
    "**4) 비교 요약:**\n",
    "\n",
    "|  | 선형 회귀분석 | 비선형 인공신경망 |\n",
    "|:-:|:-:|:-:|\n",
    "| 구조 | 은닉층이 없고 입력값과 출력값의 관계가 다이렉트로 연결 | 입력값과 출력값이 직접적으로 연결되지 못하고 복잡한 비선형성을 포함 |\n",
    "| 패턴 반영 개수<br>(모델 개수) | 1개의 회귀분석 | 2개 이상(은닉층과 노드 갯수만큼)의 회귀분석 |\n",
    "| 설명성 | 가능<br>(입력값이 출력값에 어떻게 영향을 주는지 설명) | 불가능<br>(학습된 모델을 사람이 이해할 수 없고 알파고 결과를 설명할 수 없음) |\n",
    "| 예시<br>(부동산가격<br>영향요인) | [결과]<br>- 공원의 개수가 2만큼 영향<br>- 지하철역 유무가 8만큼 영향<br>- 유흥업소 개수가 -3만큼 영향<br><br>[인싸이트]<br>- 지하철 유무가 큰 영향을 주며 <br>공원과 유흥업소는 반대의 영향을 주지만 크기는 비슷<br>- 정확도는 70%이기에 다른 케이스에선 <br>30% 정도 오차 발생 가능 | [결과]<br>- 공원의 개수가 1번 은닉층에서 0.5만큼 영향<br>- 2번 은닉층에선 -1만큼 영향<br>- 3번 은닉층에선 0.1만큼 영향<br>- 집값에는 1번 은닉이 1만큼 영향<br>- 2번 은닉은 3만큼 영향<br>- …<br><br>[인싸이트]<br>- 그냥 모르겠음<br>- 정확도는 95%이기에 다른 케이스에선 <br>동일한 데이터로 정확하게 예측할 수 있음 |\n",
    "| 요약 | 단 하나의 수학적 표현(가설, 1 Layer)으로<br>입력데이터에 제일 알맞은 수학적 표현(Parameters) 추정 | 여러개의 수학적 표현(가설, Several Layer)으로 입력데이터를<br>가장 잘 설명할 수 있는 수학적 표현을 단계별 업데이트 추정<br>-> Layer가 늘어날수록 더 많은 패턴(특징) 찾을 수 있음<br>-> Layer 1개는 1차방정식이지만 Layer 2개는 2차방정식 |\n",
    "| 한계 | 설명은 되지만 정확하지 않은 선형적 패턴만 확인 가능 | - 무작정 Deep하더라도 성능이 낮을 수 있기에(과적합),<br>모델을 간소화 하면서도 성능을 높일 방법을 찾아야함<br>- 얼마나 Deep한 신경망을 사용해야하는지 예측 불가<br>- 사람이 설정해줘야 하는 많은 Hyper-parameter 존재 |\n",
    "\n",
    "\n",
    "### 데이터분석에 딥러닝이 필요한 이유\n",
    "\n",
    "**1) 머신러닝 방향 및 발전**\n",
    "> **\"기존 분석은 인간지능에 의해 분석된 고정 모형의 형태시스템이었다면, 최근 분석은 지속적이고 반복적이며 자동수행되는 학습을 기반으로 한 진화되는 성능개선 방향\"**\n",
    "\n",
    "| 종류 | 통계학습 | 기계학습 | 딥러닝 |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| 설명 | - 정보요약을 위한 기술통계(테이블, 표)<br>- 가설수립을 기반으로 한 수학적 검정<br>- 수치기반 예측통계 | - 데이터에 숨겨진 패턴을 찾기 위한 컴퓨터 알고리즘 기반<br>- 예측, 분류, 군집화, 분류, 시계열 분석 등 | - 인간의 두뇌 구조를 모방한 인공 신경망<br>- 인공지능(AI)의 출발점 |\n",
    "\n",
    "> **(1) 통계분석과 머신러닝의 관계:** 통계적 분석기법 중 회귀분석은 인공신경망과 지도학습 알고리즘으로 발전되어 머신러닝에 포함  \n",
    ">\n",
    "> **(2) 데이터마이닝과 머신러닝의 관계:** 데이터의 패턴과 지식 발견을 위해 고객관계과리, 마케팅 영역의 군집화, 분류, 예측, 시계열 분석 등이 머신러닝의 지도학습/비지도학습으로 발전하여 머신러닝에 포함  \n",
    ">\n",
    "> **(3) 인공신경망과 머신러닝의 관계:** 인간의 두뇌 구조를 모방한 인공지능의 출발점으로 신경망의 계층을 깊게 네트워크화 한 머신러닝의 핵심기술인 딥러닝으로 발전  \n",
    "\n",
    "**2) 딥러닝 적용 필요성:**\n",
    "\n",
    "> **\"(1) 데이터에서 자동으로 Feature를 추출하고 학습할 수 있음\"**\n",
    "> - **기존:** 사람이 이론과 경험을 바탕으로 손수 생성 (Hand-crafted Feature)\n",
    "> - **한계:** 큰 정보를 압축하는 과정에서 많은 손실이나 오류가 반영될 가능성 높음  \n",
    "> (모델이 학습하기 힘든 특징을 찾을 수도 있지만 모델이 더욱 데이터의 특성을 학습하기 어려워 질 수도 있음)\n",
    "> - **방향:** 사람의 영향을 최소화 및 Layer 적용을 통해 새로운 Feature Extraction\n",
    "\n",
    "<center><img src='Image/DL_AutoFE.png' width='600'></center>\n",
    "\n",
    "> **\"(2) Multiple Data 입력과 Multivariate Data 출력을 가능하게 함\"**\n",
    "> - **기존 및 한계:** 기존의 머신러닝 방법도 다양한 입력과 출력을 사용할 순 있지만 알고리즘에 따라 제한적이고 한계가 존재\n",
    "> - **방향:** 입출력에 따른 알고리즘 구분 없이 딥러닝 구조의 처리를 통해 모든 경우를 가능하게 함\n",
    "\n",
    "> **\"(3) 길이가 긴 Sequence 패턴을 추출하는 것을 가능하게 함\"**\n",
    "> - **기존 및 한계:** 입력되는 정보(시간)가 길수록 추정해야 할 파라미터와 시간이 급격히 늘어나고 단 하나의 해가 존재하는 것이 아니기에 결과 신뢰성 의문 존재\n",
    "> - **방향:** 알고리즘 내부에서 이전 시점들의 데이터 학습을 별도로 진행함으로써 가능하게 함\n",
    "\n",
    "**3) 알고리즘의 한계: \"Garbage in, garbage out\"**\n",
    "\n",
    "> (1) 대부분 연구들은 \"알고리즘 설명\"에 상당부분 집중하고 \"어떻게 분석\" 했는지는 없음, 그저 딥러닝 결과가 좋았다!      \n",
    ">\n",
    "> (2) 이에 자극 받아 분석에 활용해보면 결과가 대부분 처참할 정도로 정확성이 낮다 오히려 회귀분석이나 RandomForest가 낫다      \n",
    ">\n",
    "> (3) 실제 실리콘밸리의 많은 회사들은 딥러닝이 아닌 \"회귀분석\"으로 수익을 내고 있다 (Andrew Ng 교수)      \n",
    ">> - **결국 Key는 알고리즘이 아니라 \"데이터!\", 데이터가 Garbage인데 알고리즘이 아무리 좋아봤자 Garbage를 출력한다**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350.562px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
